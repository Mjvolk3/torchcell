---
id: ahp93n9lv1mjp5x5oixwifr
title: '18'
desc: ''
updated: 1718758625084
created: 1718741361644
---
Alright, now time for a little bit of talk I think that it would be interesting if the. random forest Speed up that we see is real Period. It doesn't look like it's real because We have significantly worse performance With the gpu enabled version that looks like it's somewhere around Four times faster, according to the want to be or waste and biases logs. If the speedup is actually significant. We can look at The one EO6 scale which is A billion. Oh my gosh. Not a billion a million. We did 1000 10,000 a 100 thousand. It would be nice to see what happens out of million. Thank you I think in terms of. The base data structure that would be the most universal Is basically each genome Sequence files annotation files. There's an issue in the overall design of the database, and that is We would like to have this general knowledge graph type approach, but there are software limitations so one of them is that our neo for J property. is likely to have some memory limit. Chad G. P. T. Suggest this is around one megabyte, but even if it's up to 10 megabytes, we'll have an issue. The better solution for arbitrary genome size Faster files and corresponding genome annotation files. Is to store them in? Something like a key value store that can be mapped back to the knowledge graph. The knowledge graph captures primarily experiments. Um experiments, genomes, genomes by name. Genome genotype environment, right? So Genome. Is the Beijing on the genotype is a perturbation on top of this And we can easily map the difference between sets. By saying This genome minus that genome is the genomic difference. In terms of which genes belong Now those jeans have a sort of. Piece meal discreet Visualization you can see start and end positions of each gene. And then there's basically a bunch of base pairs in between, sometimes introns, sometimes unannotated d., n., a., that doesn't get modeled This is a coarse graining Effort that step alone can probably eventually be bridged with. machine learning simply of bioinformatic approach that would be maybe more interpretable and robust Period There are multiple approaches. That could be taken here. The only purpose of bringing it up is to emphasize that. This can for a large part likely be audited away, automated away in the future, that is. the transition between Sequence and annotation To the set of genes or sequence elements that are going to be most useful. In downstream prediction Now that's just from the genomic sequence level But if we want to integrate data that we have sort of an encapsulation of historical data or many publications collected over time, data that is richer in its temporal nature, the nature we ultimately. Care about in modeling. How the jeans play out over time This data is often compiled by. computational biologists, bioinformaticians To make larger objects like Networks Now these networks are mostly correlative type descriptions of. biology. They say something like as this gene varies, what happens to this gene? So you can be thinking in terms of Covariance matrices, matrices that are symmetric about the diagonal axis, where each I and gs I throw in GH column define how two things co vary. So as gene A changes, how does gene B change? Now we have many stacks of these style of descriptions. OK. And sometimes they don't have to be symmetric. So in the case of directed interaction So the types of interactions we have are really about layers within the central dogma. Now we group the central dogma into one entity. and consider the central dogma around one gene to be the entity that we are modeling for 2 reasons. 1 of them is that it is worth the richest sanitations come from is the easiest place to begin our annotation. You could do it with very simple search algorithms looking for start and stop codons to get a starting hypothesis about genes. There are rich databases about genes describing their interrelationships Their protein products and those protein products interactions so the common type of interaction network we have is protein protein to interaction networks. We also have regulatory interaction networks that are Learned out of Or approximated out of things like Co expression data. Individual relationships can then be further identified through more targeted mechanistic type studies. So regulatory interactions are really protein dna interactions but since they're correlative, we sort of just fuzz this away. And we say, what's going to happen to these downstream phenotypes that we care about We just view them as being able to be stacked. Maybe one network is more important than the other network. We're actually just relying on the machine learning model to learn this now These interactions can change. There are also signaling networks. Signaling networks. I think this is a super interesting idea. Um. Protein protein interaction type networks and signaling type networks. I don't know how big this signaling networking uses. I imagine it must be pretty small 100 genes And we're expecting basically. Chemical Chemical. acts on protein. which acts on DNA. some sort of chain like that Now you can imagine if one chemical effects approaching protein interaction that somehow disrupts that chain all of a sudden the signaling network is disrupted, the cell doesn't react to say. dangerous chemical, whatever, whatever I chaos in the cell. We're matching to see large changes when those types of things happen Period Right now I'm envisioning the segregation of the torch cell software in 2. different kinds of ways. One of them is that We need. We need a Torch cell database that is. the neophyte database that allows for querying. over the unifying schema language that defines data objects Links to the bio link ontology this should allow us or give us a principled way of building up data objects Adding new ones, making changes etcetera. Imagine this moving a little bit slower in terms of adding data. Since I've already showed that it takes some time to add data sets. We want to really create this **** up man. I need to be able to do this within the next week. That is Getting the expression data sets We need a torch cell. Genome. Berto genome database. All this is Is the? I think it should just be a key value store. It is just the genome name. Its corresponding faster file. And seek file. It's faster and gaffe files gaf. Annotation And we have to have an actually enforced minimal standards on the genome here. That has to be the next project. For now, we're just going to start with the S2 a day focus on S2 a day You can imagine a way to join genomes that does. a joining based off of sequence similarity matching. So if I have an S288CC genome and another genomic variant that looks very similar in fact has say 3000 genes that are named the same gene but are actually a close homolog having 123 snips, maybe some small insertion. I don't know. anywhere in the range I imagine of Three to 30 base pairs. We might want to actually consider this the same gene. We might want to let the user decide that we consider this to be the same gene. And so they can. They should be able to easily set a genome similarity criterion from the wild type. Genomes similarity criteria in my friends. Let's stick it down real slow now. Genome similarity criterion. Genome stimulant criteria. Genome similarity. This is the reason for dealing with genomes next But projects Should be as follows Torch cell first iteration. There's going to be three iterations within the next year I have to have hard launch dates. The first project. Version one. Needs to be delivered By the end of August. That's two and a half months. Two and a half months The deliverables of the paper. I'm thinking how we can make benchmarking easy. And I think you can just be done with. Writing an established loader. I'm not gonna worry right now about how to protect the labels and what not. I think that Adding benchmark data sets would be a very good idea It's just how to do it and. I think once we make the database open **** OK, so the first paper has to be delivered by end of August. This is going to be on motivation of the work genomes. networks, torso database for experiments, systematic language for a corporation of experiments Software that can bind these elements together and pass them into A. Training pipeline. Support of multitask Modeling of the environment. And I think envisioning a future where we can continually to continue to modularly add to these elements, that is the vision of the paper. The demonstration shows that we can use interaction data, fitness data, Protein interaction data. And we can use all of these. To at least Improve. The prediction of the different labels. Then, if we perform traditional machine learning. Tasks Thoroughly testing That each task is better. supported by a multimodal multi-task model. Let's just call it a multi-task model We take a lot of compute to actually prove out, given the size of the data sets As a demonstration We compare two. machine learning methods that have already been published in literature And claim to be make high I don't know if they claim to be state of the art, but you get the idea Papers that that sort of pays the path for these tasks. D. So the mechanistic aware model. We want to do traditional machine learning over. The fitness prediction? No, we don't want to **** repeat their study. We query their data. And we show equivalent performance with the support vector regressor. On gene expression data. Over their data splits We show that One hot on the jeans. Who showed that one hot on the jeans? Combined with The protein interaction network. and the regulatory interaction network What is the difference between this and gene graph? Engine graph. In gene graph I didn't. Have the node vectors. I need to double check gene graph. Instead of using expression. As features, we can use them as training labels. Ok, we have parallel roots here. We just got to keep pushing. Parallel routes are Really five folds. And it might be too many torch cell software, documentation, keeping things robust, testing everything that's in software engineering. #2 torch cell database. That is database builds on Hila Hyper managing builds monitoring builds, adding data for builds. Then we have three, which is the fitness single task interactions single. task. We look at single task learning that can be represented under this framework with traditional machine learning models That includes Random forests support vector machine. We want to show reproduction of work. Look, We can push on the mechanistic models, and then we can do graph neural networks with the smaller data set supervision on The gene expression data.
