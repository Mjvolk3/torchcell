#experiments/006-kuzmin-tmi/conf/cell_graph_transformer.yaml
defaults:
  - default
  - _self_

wandb:
  project: torchcell_006-kuzmin-tmi_equivariant_cell_graph_transformer
  tags: [transformer, graph_regularization, gene_interaction]

cell_dataset:
  graphs: [
      physical,
      regulatory,
      tflink,
      string12_0_neighborhood,
      string12_0_fusion,
      string12_0_cooccurence,
      string12_0_coexpression,
      string12_0_experimental,
      string12_0_database,
    ]

  # Transformer uses learned gene embeddings, not pre-computed
  node_embeddings: []

# Transform configuration
transforms:
  use_transforms: true
  forward_transform:
    normalization:
      gene_interaction:
        strategy: "standard" # Options: standard, minmax, robust

profiler:
  enabled: false  # Toggle profiling on/off
  is_pytorch: true  # Use PyTorch Profiler for detailed CPU/CUDA operator-level profiling

data_module:
  is_perturbation_subset: false
  perturbation_subset_size: 10000  # 2.5e4
  batch_size: 256  # Limited by transformer memory requirements
  num_workers: 16  # 2 > 8...
  pin_memory: true
  prefetch: false
  prefetch_factor: 4
  persistent_workers: true

trainer:
  max_epochs: 1000  # Match DANGO training duration
  strategy: ddp  # ddp_find_unused_parameters_true # auto # ddp
  num_nodes: 1
  accelerator: gpu
  devices: 4
  overfit_batches: 0
  precision: "bf16-mixed"  # "32-true", "16-mixed", "bf16-mixed"

# Compilation settings for PyTorch 2.0+
compile_mode: null  # null, "default", "reduce-overhead", "max-autotune"

# Transformer model configuration
model:
  checkpoint_path: null  # *.ckpt
  gene_num: 6607
  hidden_channels: 180 #90 #96  # d in weekly report
  num_transformer_layers: 8 #8  # L layers
  num_attention_heads: 9  # H heads per layer (must divide hidden_channels evenly)
  dropout: 0.1

  # Loss balancing
  graph_reg_scale: 1.0  # Global scale factor for graph regularization

  # Learnable embedding configuration
  learnable_embedding:
    enabled: true  # Auto-enabled when node_embeddings is empty
    size: ${model.hidden_channels}  # Match larger model size (180)
    preprocessor:  # Only used if total_input_dim != hidden_channels
      num_layers: 2
      dropout: 0.1

  # Graph regularization configuration
  graph_regularization:
    # Global regularization hyperparameters for easy sweeping
    graph_reg_lambda: 0.001
    graph_reg_layer: 1 # python indexed

    row_sampling_rate: 1.0  # Sample 50% of adjacency rows for KL computation

    # Map graph names to (layer, head, lambda) for regularization
    # NOTE: layer can be a single int or list of ints: [4, 5, 6]
    # Each graph MUST have unique head to avoid conflicts
    regularized_heads:
      physical:
        layer: ${model.graph_regularization.graph_reg_layer}  # Can also be [4] or [4, 5, 6]
        head: 0
        lambda: ${model.graph_regularization.graph_reg_lambda}
      regulatory:
        layer: ${model.graph_regularization.graph_reg_layer}
        head: 1
        lambda: ${model.graph_regularization.graph_reg_lambda}
      tflink:
        layer: ${model.graph_regularization.graph_reg_layer}
        head: 2
        lambda: ${model.graph_regularization.graph_reg_lambda}
      string12_0_neighborhood:
        layer: ${model.graph_regularization.graph_reg_layer}
        head: 3
        lambda: ${model.graph_regularization.graph_reg_lambda}
      string12_0_fusion:
        layer: ${model.graph_regularization.graph_reg_layer}
        head: 4
        lambda: ${model.graph_regularization.graph_reg_lambda}
      string12_0_cooccurence:
        layer: ${model.graph_regularization.graph_reg_layer}
        head: 5
        lambda: ${model.graph_regularization.graph_reg_lambda}
      string12_0_coexpression:
        layer: ${model.graph_regularization.graph_reg_layer}
        head: 6
        lambda: ${model.graph_regularization.graph_reg_lambda}
      string12_0_experimental:
        layer: ${model.graph_regularization.graph_reg_layer}
        head: 7
        lambda: ${model.graph_regularization.graph_reg_lambda}
      string12_0_database:
        layer: ${model.graph_regularization.graph_reg_layer}
        head: 8
        lambda: ${model.graph_regularization.graph_reg_lambda} 

  # Perturbation head configuration
  perturbation_head:
    use_cross_attention: true
    num_heads: 9  # Must divide hidden_channels (180/9=20)
    dropout: 0.1

# Loss and optimization configuration
regression_task:
  # Loss configuration
  loss:
    type: "point_dist_graph_reg"  # Modular loss: point + distribution + graph_reg

    # Point estimator configuration
    point_estimator:
      type: "mse"  # "logcosh" or "mse"
      lambda: 1.0

    # Distribution loss configuration
    distribution_loss:
      type: "wasserstein"  # "dist", "wasserstein", or null to disable
      lambda: 0.1

      # Wasserstein-specific parameters (active when type="wasserstein")
      wasserstein_blur: 0.05  # Entropic regularization (smaller = more precise)
      wasserstein_p: 2  # p=2 for W2 (quadratic) distance, p=1 for W1
      wasserstein_scaling: 0.9  # Multi-scale parameter (0.5-1.0)
      min_samples_for_wasserstein: 512  # Minimum samples for Wasserstein distance

      # Dist loss parameters (active when type="dist")
      # dist_bandwidth: 0.5  # Bandwidth for distribution loss
      # min_samples_for_dist: 512  # Minimum samples for dist computation

    # Graph regularization configuration (from model)
    graph_regularization:
      lambda: 1.0

    # Supervised contrastive loss configuration (disabled for now)
    supervised_contrastive:
      lambda: 0.0  # Set to 0.001 or higher to enable
      # supcr_temperature: 0.1  # Temperature for contrastive loss
      # min_samples_for_supcr: 512  # Minimum samples for SupCR
      # embedding_dim is dynamically set from model.hidden_channels

    # Buffer configuration (disabled for large batch)
    buffer:
      use_buffer: false  # Set true for small batches (e.g., batch_size < 64)
      # buffer_size: 8192  # Size of circular buffer for distribution estimation

    # DDP configuration (disabled for large batch)
    ddp:
      use_ddp_gather: false  # Set true to gather samples across GPUs
      # gather_interval: 1  # Gather every N forward passes

    # Adaptive weighting (optional)
    # adaptive_weighting:
    #   enabled: true  # Learn loss weights dynamically
    #   warmup_epochs: null  # None -> will be 10% of max_epochs
    #   stable_epoch: null  # None -> will be 50% of max_epochs

    # Temperature scheduling (for SupCR when enabled)
    # temperature_scheduling:
    #   enabled: true
    #   init_temperature: 1.0
    #   final_temperature: 0.1
    #   schedule: "exponential"  # Options: "exponential", "linear"
    #   max_epochs: 300

  execution_mode: "training"  # "training" or "dataloader_profiling"

  optimizer:
    type: "AdamW"
    lr: 1e-4  # Reduced from 5e-4 to stabilize training with distribution loss
    weight_decay: 1e-8  # Very light regularization - transformer embeddings are sensitive to L2

  # lr_scheduler: null  # No scheduler - constant LR
  lr_scheduler:
    type: "CosineAnnealingWarmupRestarts"
    first_cycle_steps: 200  # First convergence at epoch 50
    cycle_mult: 0.85  # Each cycle is 3x longer: 50 -> 150 -> 450
    max_lr: 1e-4 #5e-4  # Higher peak LR since no overfitting
    min_lr: 1e-6  # Higher floor for faster convergence
    warmup_steps: 1  # Quick warmup to peak
    gamma: 0.75  #0.85 Moderate decay across cycles (reduces peak LR by 50% each restart)

  clip_grad_norm: true
  clip_grad_norm_max_norm: 10.0
  grad_accumulation_schedule: null  # {0: 2} for gradient accumulation
  plot_sample_ceiling: 10000
  plot_every_n_epochs: 1 # Sample plotting (predictions vs truth)
  plot_transformer_diagnostics_every_n_epochs: 10
  plot_edge_recovery_every_n_epochs: 10
