# DO NOT MODIFY THIS FILE - Used for profiling benchmarks
# Created: 2025-10-22
# Purpose: Stable config for measuring SubgraphRepresentation optimizations
# Based on: experiments/006-kuzmin-tmi/conf/hetero_cell_bipartite_dango_gi.yaml

defaults:
  - default
  - _self_

wandb:
  project: torchcell_profiling_optimization
  tags: [profiling, optimization, subgraph_representation]

cell_dataset:
  graphs: [
      physical,
      regulatory,
      tflink,
      string12_0_neighborhood,
      string12_0_fusion,
      string12_0_cooccurence,
      string12_0_coexpression,
      string12_0_experimental,
      string12_0_database,
    ]

  node_embeddings:
    - learnable
  learnable_embedding_input_channels: 64
  incidence_graphs: [metabolism_bipartite]

# Transform configuration moved out of regression_task
transforms:
  use_transforms: true
  forward_transform:
    normalization:
      gene_interaction:
        strategy: "standard" # Options: standard, minmax, robust

profiler:
  is_pytorch: true

data_module:
  is_perturbation_subset: true
  perturbation_subset_size: 1000  # Reasonable subset for profiling
  batch_size: 32 #32 to match cabbi_056
  num_workers: 2 #2 to match cabbi_056
  pin_memory: true
  prefetch: false
  prefetch_factor: 2

trainer:
  max_epochs: 1 # Updated for shorter training
  strategy: auto # Will use single GPU strategy
  num_nodes: 1
  accelerator: gpu
  devices: 1  # Single GPU for profiling
  overfit_batches: 0
  precision: "bf16-mixed"  # "32-true", "16-mixed", "bf16-mixed"

# Compilation settings for PyTorch 2.0+
compile_mode: null #"max-autotune"  # null, "default", "reduce-overhead", "max-autotune"

# ------ Placeholders for hydra optuna
heads: 8 #10 # placeholder
norms: "layer" # placeholder
# ------
model:
  checkpoint_path: null #*.ckpt
  gene_num: 6607
  reaction_num: 7122
  metabolite_num: 2806
  hidden_channels: 64 #64 to match cabbi_056
  num_layers: 3 # Number of hetero conv layers
  # norm: "batch"
  norm: ${norms}
  activation: "gelu"
  dropout: 0.0 # 0.1
  gene_encoder_config:
    encoder_type: "gin"  # Options: "gatv2" or "gin"
    
    # Graph aggregation configuration (NEW)
    # graph_aggregation_method: "cross_attention"  # Default: cross-attention for multiple graphs
    # graph_aggregation_method: "pairwise_interaction"  # Alternative: pairwise graph interactions
    graph_aggregation_method: "sum"  # Simple sum to match cabbi_056
    # graph_aggregation_method: "mean"  # Alternative: simple mean aggregation

    # graph_aggregation_config:
    #   num_heads: 4  # Number of attention heads for cross_attention
    #   dropout: 0.0  # Dropout for aggregation modules (set to 0 as requested)
    
    # GAT-specific parameters (used when encoder_type="gatv2")
    # heads: 5
    heads: ${heads}
    concat: true
    bias: true
    add_self_loops: false
    share_weights: false
    # GIN-specific parameters (used when encoder_type="gin")
    gin_num_layers: 4      # Number of MLP layers in GIN (4 to match cabbi_056)
    gin_hidden_dim: null   # If null, defaults to hidden_channels
  gpr_conv_config:
    encoder_type: "gin"  # Options: "gatv2" or "gin"
    # GAT-specific parameters
    # heads: 2
    heads: ${heads}
    concat: true
    add_self_loops: false
    # GIN-specific parameters
    gin_num_layers: 4  # 4 to match cabbi_056
    gin_hidden_dim: null
  metabolism_config:
    encoder_type: "gin"  # Options: "gatv2" or "gin"
    is_stoich_gated: true
    use_attention: true
    # GAT-specific parameters
    # heads: 2
    heads: ${heads}
    concat: true
    # GIN-specific parameters
    gin_num_layers: 4  # 4 to match cabbi_056
    gin_hidden_dim: null
  prediction_head_config:
    hidden_channels: 64 #64
    head_num_layers: 4
    dropout: 0.0 #0.1
    activation: "gelu"
    # residual: true
    # head_norm: "batch"
    head_norm: ${norms}
  # Dango-like local predictor config
  local_predictor_config:
    num_attention_layers: 2 # Number of self-attention layers (like Dango HyperSAGNN)
    num_heads: 8 # 16 16 # Number of attention heads (matching Dango) - hidden divisible by num HEADS #FLAG
    combination_method: "concat" # Options: "gating" (learned weights) or "concat" (fixed 0.5/0.5 weights)

regression_task:
  loss: mle_wass_supcr # Using Wasserstein to match cabbi_056
  is_weighted_phenotype_loss: false
  # Lambda weights for loss components
  lambda_mse: 1.0
  # lambda_dist: 0.1 # For MleDistSupCR (not used with mle_wass_supcr)
  lambda_wasserstein: 1.0  # For MleWassSupCR (1.0 to match cabbi_056)
  lambda_supcr: 0.0 #0.001 #0.001 #0.001 #0.001

  # MleDistSupCR/MleWassSupCR specific configuration
  loss_config:
    # Component-specific parameters for MleDistSupCR (not used with mle_wass_supcr)
    # dist_bandwidth: 1.0  # For MleDistSupCR
    # min_samples_for_dist: 256  # For MleDistSupCR

    # Wasserstein-specific parameters (for MleWassSupCR)
    wasserstein_blur: 0.05  # Entropic regularization (smaller = more precise)
    wasserstein_p: 2  # p=2 for W2 (quadratic) distance, p=1 for W1
    wasserstein_scaling: 0.9  # Multi-scale parameter (0.5-1.0)
    min_samples_for_wasserstein: 256  # Minimum samples for Wasserstein distance
    supcr_temperature: 0.1
    # embedding_dim is dynamically set from model.hidden_channels
    
    # Buffer configuration
    use_buffer: true
    buffer_size: 256  # 256 to match cabbi_056
    min_samples_for_supcr: 256  # 256 to match cabbi_056
    
    # DDP configuration
    use_ddp_gather: true
    gather_interval: 1  # Gather every forward pass
    
    # Adaptive weighting
    use_adaptive_weighting: true
    # warmup_epochs and stable_epoch now dynamically set based on max_epochs
    # warmup_epochs: None -> will be 10% of max_epochs (30 epochs)
    # stable_epoch: None -> will be 50% of max_epochs (150 epochs)
    
    # Temperature scheduling
    use_temp_scheduling: true
    init_temperature: 1.0
    final_temperature: 0.1
    temp_schedule: "exponential"
    
    # Training duration (for temperature scheduling)
    max_epochs: 500  # 500 to match cabbi_056
  optimizer:
    type: "AdamW"
    lr: 1e-4 #overrode by type (1e-4 to match cabbi_056)
    weight_decay: 1e-12
  lr_scheduler:
    type: "CosineAnnealingWarmupRestarts"
    first_cycle_steps: 300  # Match cabbi_056
    cycle_mult: 1.0  # Keep cycles the same length
    max_lr: 5e-4  # 5e-4 to match cabbi_056
    min_lr: 1e-7  # Higher floor for faster convergence
    warmup_steps: 0  # Quick warmup to peak
    gamma: 0.50  # 0.50 to match cabbi_056
  clip_grad_norm: true
  clip_grad_norm_max_norm: 10.0
  grad_accumulation_schedule: {0: 2}  # Accumulate for 2 steps to match cabbi_056
  plot_sample_ceiling: 10000
  plot_every_n_epochs: 2
