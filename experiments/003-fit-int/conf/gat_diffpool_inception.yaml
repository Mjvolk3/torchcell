defaults:
  - default
  - _self_

wandb:
  # mode: offline # disabled, offline, online
  project: torchcell_test_gat_diffpool_inception
  tags: []

cell_dataset:
  graphs: [physical, regulatory] # [physical], [regulatory], [physical, regulatory], []
  node_embeddings: [codon_frequency]
  # [one_hot_gene], [codon_frequency], [calm], [fudt_downstream], [fudt_upstream], [prot_T5_all], [prot_T5_no_dubious], [nt_window_5979], [nt_window_three_prime_300], [nt_window_five_prime_1003], [esm2_t33_650M_UR50D_all], [esm2_t33_650M_UR50D_no_dubious], [normalized_chrom_pathways], [random_1000], [random_100], [random_10], [random_1]

data_module:
  is_perturbation_subset: true
  perturbation_subset_size: 5e4 #7e3
  batch_size: 4
  num_workers: 4
  pin_memory: true
  prefetch: true

trainer:
  max_epochs: 2
  strategy: auto # ddp, auto
  accelerator: cpu
  devices: auto # auto or int

model:
  num_initial_gat_layers: 1
  initial_gat_hidden_channels: 8
  initial_gat_out_channels: 8
  gat_dropout_prob: 0.2
  gat_skip_connection: true
  num_diffpool_layers: 10
  cluster_size_decay_factor: 2.0
  num_post_pool_gat_layers: 1
  diffpool_hidden_channels: 8
  diffpool_out_channels: 8
  last_layer_dropout_prob: 0.2
  norm: layer # layer, instance, batch, graph, pair, mean_subtraction
  activation: gelu
  pruned_max_average_node_degree: null
  weight_init: xavier_normal
  target_dim: 2

regression_task:
  loss_type: "mse"
  optimizer:
    type: "AdamW"
    lr: 1e-5
    weight_decay: 0
  lr_scheduler:
    type: "ReduceLROnPlateau"
    mode: "min"
    factor: 0.2
    patience: 3
    threshold: 1e-4
    threshold_mode: "rel"
    cooldown: 2
    min_lr: 1e-9
    eps: 1e-10
  boxplot_every_n_epochs: 1
  clip_grad_norm: true
  clip_grad_norm_max_norm: 1.0
  cluster_loss_weight: 1.0 #1.0
  link_pred_loss_weight: 1.0 # 1.0
  entropy_loss_weight: 1.0 # 1.0
  # grad_accumulation_schedule: null
  grad_accumulation_schedule:
    0: 24
    4: 4
    8: 1
