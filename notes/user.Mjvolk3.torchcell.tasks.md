---
id: pt6kzbutl4wmnf8xsg4iurb
title: torchcell.tasks
desc: ''
updated: 1710223097612
created: 1690514887023m
---

![[user.mjvolk3.torchcell.tasks.future#future]]
[[Outline|dendron://torchcell/paper.outline]]

## 2024.03.12

- [x] We were able to build the `DmfKuzmin` nodes, but when we try to get the edges the  program fails. There's a spike in the memory, and so it looks like some sort of out of memory error. Check `Delta` logs to confirm. â†’ logs are unclear, we didn't log the job id, we have have some wandb run link. â†’ Not clear why this run failed.
- [x] Rerun on `Delta` â†’ now times are shockingly low for the datasets â†’ This is likely due to me not restarting with fresh datasets. The extra `preprocess` data is all there for all datasets so I think that everything should be ok.
- [x] #ramble The design choice arises from trying to apply some general constraints to a more open ended data structure. Each experimental instance is denoted by a perturbation, which could be a scalar, vector, matrix, or some high dimensional tensor object. Importantly, we avoid any overlap between the axis of variation and perturbations, to not capture perturbation information in any data axes. This would violate the data model, which we already use to represent perturbations. In fact, perturbation is the first element of the experimental instance object. This is the most natural choice as we want to learn what happens to the cell under different perturbations.
- [x] Change dirs for slurm output and error logging on delta. Belongs to `\scratch`
- [ ] `Lmdb` raw for `dmf` data with docker

## 2024.03.11

- [x] Use previous hacks on the Costanzo Adapter, we've tried without it... Now we need all the help we can get.
- [ ] Compute time approximation for `DmfCostanzo2016` â†’ Since the node compute time should increase roughly linearly. For `1e6` we have, `1003 s * 20 size_increase / 60 (s/min) / 60 (min/hr) = 5.57 hr`. This on on M1 with 10 cores, so hopefully it should get better with more cores. All 10 cores are maxed out in running the adapters. Edges are`623 s`. Previously I was thinking the edges would scale more quadratically but this shouldn't be the case as we don't double loop over all experiments. Edges are `623s` and plugging this into the equation above we get `1039 s * 20 size_increase / 60 (s/min) / 60 (min/hr) = 5.78 hr` ðŸ›‘ I computed for kuzmin need to do for Costanzo..
- [x] Increase cpu cores for Docker to 10
- [x] Run on `Delta` â†’ works very well and is pretty fast. We had an error on bulk import because all of logged output is polluting the final file path.
- [x] Fix the file path issue. By writing a `txt` file with the path information. Revert `stderr` back to `stdout`. Write text file. â†’ had to move the logging inside of hydra because hydra changes `cwd`. â†’ Reverted to just writing a text file instead log file.
- [x] Run small test `Delta`
- [x] Run whole database build on `Delta`
- [ ] Run db build on docker try to fix multiprocessing issue.

## 2024.03.10

- [x] Check on delta run. â†’ Some error, it stopped logging, and the [wandb logs](https://wandb.ai/zhao-group/tcdb/runs/hzjaofbq?nw=nwusermjvolk3) indicate that it finished at the point of processing edges. â†’ We are seeing lower cpu utilization than I expect. Extremely low around 10% it seems.
- [x] Profile `DmfCostanzo2016` dataset creation... Unclear the issue locking is called a lot during threading. We really need to multiprocess the `create_experiment` since we can do true parallelization over rows.
- [x] Make sure that rows being multiprocessed. â†’ using `imap` for this then threading. This cuts the time in half on #M1 for processing the data from `20 min` to `8 min` and hopefully it will to greater improvement once we have more resources to

## 2024.03.09

- [x] Profile `1e6` and the full dataset size. â†’ instead of profiling I just tested out multiprocess and multithreading for a delta launch
- [x] Launch delta build.That's the run page huh I want to see that **** What the **** No The core noodles are good, dude. Dude, he's Cosby Blazin Sushi You like

## 2024.03.08

- [x] Run [[torchcell.knowledge_graphs.create_scerevisiae_kg_small]] locally incase of need for globus transfer
- [x] Run [[torchcell.knowledge_graphs.create_scerevisiae_kg_small]] on `delta` now that I think we have solved the issue with the experimental reference index.
- [x] `Wandb` log database build â†’ created config `torchcell/knowledge_graphs/conf/kg_small.yaml`
- [x] Run local test â†’ [local wandb test](https://wandb.ai/zhao-group/tcdb/runs/qhfaujdq?nw=nwusermjvolk3). Things work find
- [x] Build `DmfCostanzo2016` locally
- [ ] Run large build `delta` â†’ failed, need yamls to be added to package.
- [ ] Implement `NeoDataset`, use the data schema to generate 2-3 common indices.
- [ ] Create `Dmf` dataset and freeze
- [ ] Create `Tmf` dataset and freeze

## 2024.03.07

- [x] `TC_3133452` taking way to long. Something like 100 hr projected finish time on `dmf`. `io_workers = math.ceil(0.5 * num_workers)`. â†’ Adjusting parameters using `io_workers = math.ceil(0.2 * num_workers)` â†’ It appears that the errors were occuring just in the writing of the original dataset. Should probably do this locally as the bottleneck is writing IO on `Delta` and this writing is not optimized batched.
- [ ] Globus transfer of 3133452 output. This file is like 13 MB. We should likely do less logging.

## 2024.03.06

- [x] Add post_porcess decorator on processed for cached `gene_set` and `experimental_reference_index`
- [x] #pro-tip Dataset usage [[2024.03.06 Why the Dataset methods Come after Super|dendron://torchcell/torchcell.datasets.scerevisiae.kuzmin2018#20240306-why-the-dataset-methods-come-after-super]]
- [x] Refactor [[Kuzmin2018|dendron://torchcell/torchcell.datasets.scerevisiae.kuzmin2018]]
- [x] Refactor Kuzmin adapters [[Kuzmin2018_adapter|dendron://torchcell/torchcell.adapters.kuzmin2018_adapter]]
- [x] bump `torchcell` â†’ not recognizing on pypi... bumping `FEAT`
- [x] Run local test of small build [[2024.03.06 - Start and use Neo4j Torchcell Database|dendron://torchcell/cypher-shell#20240306---start-and-use-neo4j-torchcell-database]]
- [x] Run test on `Delta` of small build. â†’ this is going wicked slow... maybe see if there is a difference using processors. Previously this is what I think worked on Delta. â†’ I changed back to `processor` and recovered some speed . Using this for now... I'm not sure of the logic
- [x] Run full database build. â†’ Launched full build on delta even though still using this file [[torchcell.knowledge_graphs.create_scerevisiae_kg_small]]
- [ ] Implement `NeoDataset`, use the data schema to generate 2-3 common indices.
- [ ] Create `Dmf` dataset and freeze
- [ ] Create `Tmf` dataset and freeze

## 2024.03.05

- [x] Methods across all of the datasets is too difficult to maintain. Write a CellDataset Base class that encapsulates the common behavior. â†’ I think the only change to [[torchcell.dataset.dataset]] is the process_files_exists check. We should be able to readily substitute the `pyg` class. â†’ Let's simplify! â†’ We left off with trying to abstract away some of the necessary calls like `gene_set` and `experiment_reference_index`. â†’ Refactored `SmfCostanzo2016` â†’ Refactored `DmfCostanzo2016` and feeling pretty good about everything... for now â›ˆï¸ See [[torchcell.dataset.experiment_dataset]], and [[torchcell.loader.cpu_experiment_loader]]
- [x] [[2024.03.05 Origin - Skip Process Files Exist Check|dendron://torchcell/torchcell.dataset.dataset#20240305-origin---skip-process-files-exist-check]]
- [x] [[Cpu_experiment_loader|dendron://torchcell/torchcell.loader.cpu_experiment_loader]]
- [x] Run with `pyg` `Dataset` class, see if it breaks ðŸ’” â†’ heart is lubadub dub heat is healthy and well â¤ï¸. The class works with `pyg` `Dataset`
- [x] #ramble Check that database exports are key for key the same in `lmdb`. This will make addition of new datasets testable. It is an alternative to trying to reduce duplication. It pushes more work into getting queries correct, or testing queries for certain properties within a class. Theses will be common as they are tied to the data model, and could therefore probably be abstracted, and this is why we love the data model.
- ðŸ”² Verify that rewrite works on `Kuzmin2018`

## 2024.03.04

- [x] #ramble â†’ It seems that this part of the process could be almost completely well-defined. There should only be one rule, if you create a dataset that returns the `lmdb` database, made up of data instances defined by the underlying data model, then, your ticket to play ðŸŽ« is making a such a dataset. We should spare the user of having to actually write the adapters. This also would allow me to separate out the `TorchCell` adapter for contribution to biocypher.
- [x] Work on tables for models â†’ [[dmf-fitness-table.02|dendron://torchcell/paper.outline.dmf-fitness-table.02]]
- [x] #ramble I think that bringing the open random note to my workflow will be a very nice way to bring more unity to the workspace. After my first go at opening random notes I recognized that going into this round of experiments I really need to ride the wave ðŸŒŠ properly this time. ramping up models for testing, small mostly to make sure things are working and that I can run them all at once. Or right after one another I really want to minimize debugging so I can move onto the next thing which must be adding the gene interaction datasets.
- [x] Get `DmfCostanzo2016Adapter` working
- [x] local test on `dmf_costanzo2016_subset_n_1e4`
- [x] Test `SmfCostanzo2016Adapter` on `Delta`
- [x] Make sure the database is queryable.
- [x] #ramble The path to now build the database is getting well established. I think the next thing to do is construct a `NeoCellDataset`. This will be possible in two steps first is to build at the query [[torchcell.neo4j_fitness_query]] so we can construct a raw lmdb where instances are not represented by pytorch geometric data. This raw data should be passed through the data model at some point and then used to write `pyg` data. For this we will want to repurpose [[torchcell.datasets.cell]] it should be nice that we can avoid this notion of a combined dataset, and some other older parts.

## 2024.03.03

- [x] Get things working again. â†’ Had to stash because something is broken.
- [x] Try threading instead of loading on the Custom Loader. â†’ works but we are keeping the process Custom Loader there for now commented out. We will have to move this to a file where it makes more sense.
- [x] Finish `SmfCostanzo2016Adapter`

## 2024.03.01

- [x] Figure on multiprocesssing optimization â†’ ![](./assets/drawio/biocypher-adapter-optimization.drawio.png)

## 2024.02.29

- [ ] Check node write speed on `delta`. â†’ Ran into error when trying to update packages to most recent main commit. [[Apptainer|dendron://torchcell/database.apptainer]]
- [ ] We have multiple `.bashrc` that are competing with apptainer and the base env on delta... fix this

## 2024.02.28

What is going on here ? what could the following text mean?

[x] Rewrite `SmfCostanzo2016Adapter` `_get_experiment_nodes` to use the data loader with chunking from `DmfCostanzo2016Adapter`. â†’ Now we can control the number of workers dedicated to IO and the number dedicated to multiprocessing within the function. This chunking method also reduces memory overhead by controlling chunk size.
The text is describing a programming task or update involving the modification of a function named `_get_experiment_nodes` in a software component referred to as `SmfCostanzo2016Adapter`.

The purpose of this modification is to enable the function to utilize a data loader that incorporates a chunking technique borrowed from another component named `DmfCostanzo2016Adapter`. This adjustment allows for better management of resources when the function is executed. Specifically, it makes it possible to specify and control the number of worker processes allocated for input/output operations (IO) and the number allocated for multiprocessing tasks separately. Additionally, by using chunking to handle data in smaller, more manageable pieces, the update aims to reduce the memory overhead associated with processing large datasets at once.

- [x] Rewrite `SmfCostanzo2016Adapter` `_get_experiment_nodes` to use the data loader with chunking from `DmfCostanzo2016Adapter`. â†’ Now we can control the number of workers dedicated to IO and the number dedicated to multiprocessing within the function. This chunking method also reduces memory overhead by controlling chunk size.
- [x] Remove index from experiment nodes because it will likely get jumbled anyway. â†’ can possibly recover this or find index later through lmdb. sha in key value possibly.
- [x] Abstract the chunking, and data loading logic for nodes in `SmfCostanzo2016Adapter`. â†’ This is general enough where it should be reusable for other classes. Ideally this will tie directly in with the data models so we can reuse across all datasets.

- [ ] Check node write speed on `delta`.

- [ ] Abstract the chunking, and data loading logic for edges in `SmfCostanzo2016Adapter`.

- [ ] change `data.reference.reference_environment` to `data.reference.environment`

- [ ] Local

- [ ] Rewrite `SmfCostanzo2016Adapter` to use data loader on all `getters`.
- [ ] Rewrite `DmfCostanzo2016Adapter` to use data loader on all `getters`. This will be the ultimate test to see if the speed is up to snuff.

- [ ] With chunking I forgot that we can no longer deal with deduplication which voids old tests. Not sure If we can somehow rework this in. ðŸ§  brainstorm a bit.

- [ ] Test `SmfCostanzo2016Adapter` times to write data on `Delta`. Do this from no dataset so we can also squash the confused print statement that gives us the bash path.

- [ ] Yield in `DmfCostanzo2016Adapter` chunking.
- [ ] Dataset index for lmdb

- [ ] Clean up tasks.
- [ ] Docker environment variables.
- [ ] Logging to the slurm out file.

## 2024.02.27

- [x] Check on small **remote_build** â†’ Going shockingly slow. Hasn't finished in 13 hours. On M1 finishes in like 10 min. â†’ on M1 for `SmfCostanzo2016Adapter` we get `20484it [00:01, 12145.92it/s]` and on Delta we are getting `20484it [01:00, 341.24it/s]`
- [x] Investigate **remote_build** speed issue ðŸ¢ â†’ M1 cpu max clock rate `3.2 GHz`, and Delta AMD EPYC 7763 â€œMilanâ€ (PCIe Gen4) clock rate `~2.45`. This shouldn't lead to the nearly 2 orders of magnitude difference in speed. This is a bit baffling to me ðŸ˜§... Maybe there is some networking bottleneck ðŸ¾ although everything should be in memory. â†’ the issue is with slow IO on delta and biocypher writer. â†’ The issue is **NOT** with the Biocypher writer, it is 2 order of magnitude slower just iterating over the `get_` methods, both `get_nodes` and `get_edges`. Must solve this problem before moving on.  â†’ I've at least found the issue, which is IO from the lmdb dataset
- [x] Use pyg data loaders for loading the `lmdb` â†’ This does not work because the pydantic data models can have different schemes, non standard entries, different keys etc, and so they cannot be collated which is a requirement of the `pyg` data loader. I would at least have to write a custom collate to bypass this issue, which would get a bit hacky, when I could just write a queue to load next data on cpu.
- [x] Write a custom data loader for speeding up data writing of adapters on `delta` â†’ we have a working class that seems to create the desired speed up. When running on 128 cpu, we see a speedup from a 1m 30s to 3s. But this was only tested on one function.

## 2024.02.26

- [x] With starting database and maybe not stopping it then the database cannot remain open in future sessions. Not sure what causes this... Rebuilding image to fix. â†’ This isn't so bad the rebuild takes around 5 mins. â†’ Still doesn't work, issues is due to `databases/store_lock` created upon interrupted process [store_lock community neo4j](https://community.neo4j.com/t/var-lib-neo4j-data-databases-graph-db-store-lock-please-ensure-no-other-process-is-using-this-database-and-that-the-directory-is-writable/16863/3) â†’ removing doesn't seem to fix things. â†’ After restarting everything it seems to work so my suspicion is that there was some process that needed to be terminated.

- [x] Try using `.env` for docker file configuration, just add

```yaml
NEO4J_ACCEPT_LICENSE_AGREEMENT=yes
NEO4J_AUTH=neo4j/torchcell
```

```bash
docker run --env-file /path/to/your/.env your_image
```

â†’ This seems to work ok. I verified that it was copied over to `cypher-shell` for apptainer. still need to check for docker.

- [x] **remote_build** of all fitness data with `1e5` on dmf â†’ After 3 hr run I killed the job. There is insufficient logging. For some reason there is no output from the python script to the slurm out file.
- [x] **remote_build** database overnight with `1e5` on dmf â†’ Add slurm environment variables getter function for number of workers.

- [ ] Verify that we were able to read data off of the database on `delta`. Reread notes.
- [ ] Need way to dynamically set `dbms.memory.heap.max_size=10G` and `dbms.threads.worker_count`. It seems that the `heap.max_size` is already set automatically which is recommended. â†’ in debug log heap size is actually recommended to be explicitly set.
- [ ] Looks like I accidentally deleted the `data/scerevisiae` dir. Should be on delta.

## 2024.02.21

- [x] Try to rebuild image with apptainer, then make sure all bind mounts are exact matches and try to start neo4j. If this does not work, build official Neo4j image and make sure we can start this database. â†’ [[2024.02.21 - Building tc-neo4j latest|dendron://torchcell/database.apptainer#20240221---building-tc-neo4j-latest]] â†’ seems just after rebuild it works fine. â†’ [[Fresh Apptainer Build|dendron://torchcell/database.apptainer#fresh-apptainer-build]]
- [x] Compare tc-neo4j with `neo4j_4.4.30-enterprise` [[2024.02.21 - Comparison between tc-neo4j build and neo4j_4.4.30-enterprise official image|dendron://torchcell/database.apptainer#20240221---comparison-between-tc-neo4j-build-and-neo4j_4430-enterprise-official-image]]
- [x] Remove external `jdk-11` from `delta`. â†’ Don't need now that inside image.

- [ ] With starting database and maybe not stopping it then the database cannot remain open in future sessions. Not sure what causes this... Rebuilding image to fix. â†’ This isn't so bad the rebuild takes around 5 mins. â†’ Still doesn't work, issues is due to `databases/store_lock` created upon interrupted process [store_lock community neo4j](https://community.neo4j.com/t/var-lib-neo4j-data-databases-graph-db-store-lock-please-ensure-no-other-process-is-using-this-database-and-that-the-directory-is-writable/16863/3) â†’ removing doesn't seem to fix things.

- [ ] Try using `.env` for docker file configuration, just add

```yaml
NEO4J_ACCEPT_LICENSE_AGREEMENT=yes
NEO4J_AUTH=neo4j/torchcell
```

```bash
docker run --env-file /path/to/your/.env your_image
```

- [ ] Verify that we were able to read data off of the database on `delta`. Reread notes.
- [ ] Need way to dynamically set `dbms.memory.heap.max_size=10G` and `dbms.threads.worker_count`. It seems that the `heap.max_size` is already set automatically which is recommended. â†’ in debug log heap size is actually recommended to be explicitly set.
- [ ] Looks like I accidentally deleted the `data/scerevisiae` dir. Should be on delta.

## 2024.02.20

- [x] Add new scipy semantic versioning formatting.
- [x] Find source of the `Environment` cannot pickle error. â†’ Still cannot find the source of this `Error: Cannot pickle 'Environment'` been about an 1.5 hours now.. â†’ adding `self.experiment_reference_index` at the end of `def process(self):` seems to be causing an issue. â†’ Holy.. 3 hours later I found it. â†’ [[Always Close the lmdb before pickling|dendron://torchcell/torchcell.datasets.scerevisiae.kuzmin2018#always-close-the-lmdb-before-pickling]]
- [x] Inspect all `SmfCostanzo2016`, test that it can be put into database with no errors â†’ [[Useful Commands for Checking Source Code Update|dendron://torchcell/database.docker#useful-commands-for-checking-source-code-update]] Appears to work well. â†’ `cat import.report` is empty.
- [x] Inspect all `DmfCostanzo2016_1e5`, test that it can be put into database with no errors. â†’ `cat import.report` is empty.
- [x] Inspect all `SmfKuzmin2018`, test that it can be put into database with no errors â†’ `cat import.report` is empty â†’ [[Running Docker Interactive Will Merge Stdout and Return Value|dendron://torchcell/torchcell.datasets.scerevisiae.kuzmin2018#running-docker-interactive-will-merge-stdout-and-return-value]]
- [x] Inspect all `DmfKuzmin2018`, test that it can be put into database with no errors â†’ Took an 1 hr 40 min to complete for 400,000 experiments. â†’ `cat import.report` is empty
- [x] Inspect all `TmfKuzmin2018`, test that it can be put into database with no errors â†’ Took 10 mins for around 90,000 experiments. â†’ `cat import.report` is empty.
- [x] **small_build** complete.
- [x] **remote_small_build** getting bash path not consistent in apptainer. â†’ In [[costanzo2016|dendron://torchcell/torchcell.datasets.scerevisiae.costanzo2016]] and [[Kuzmin2018|dendron://torchcell/torchcell.datasets.scerevisiae.kuzmin2018]] switched printing to logging so that we can get bash string. This works well.
- [x] **remote_small_build** try to see if there are the correct number of nodes in the database on delta apptainer. â†’ ðŸ˜  No way of knowing.. you cannot use `cypher-shell` in apptainer. [[2024.02.21 - Cannot use cypher-shell in Apptainer|dendron://torchcell/database.apptainer#20240221---cannot-use-cypher-shell-in-apptainer]] â†’ Turns out the database is immediately shutting down so hope remains.

## 2024.02.17

- [x] UIUC containerization workflow â†’ ![](./assets/drawio/database-containerization-uiuc.drawio.png)

## 2024.02.16

- [x] **small build** - fix path matching for neo4j. â†’ Issue was that we need to uninstall biocypher before installing my forked branch.
- [x] Summarize why we have separated certain files from docker. For instance, I have made the `"biocypher/config"` shared between the docker container and the local project for writing source. Then I have `"biocypher-out"` in both `"/database"` and in the local workspace. â†’ It is easier to keep the config as part of the local workspace and it should possibly be put into the source so we can generate config dirs with the user repo. `"biocypher-out"` maintains database versioning for local, docker local, and apptainer remote. In theory if we use relative paths we could globus transfer `"biocypher-out"` for easy database transfer. Can give @Junyu-Chen credit for this as he asked about database transfer last machine learning subgroup.
- [x] Ego net drawio ![](./assets/drawio/ego-net-fitness-model.drawio.png)
- [x] Update containerization drawing drawio ![](./assets/drawio/database-containerization.drawio.png)

## 2024.02.15

- [x] **small build** - Does the data disappear if the container is removed? â†’ When we map use volumes to bind the `"database/data"` dir, we do not lose the data upon stopping and removing the container, `docker stop tc-neo4j`, `docker rm tc-neo4j.`
- [x] **small build** - Document using `cypher-shell` from within the tc-neo4j container. â†’ [[Using cypher-shell to Access torchcell Database|dendron://torchcell/database.docker#using-cypher-shell-to-access-torchcell-database]]
- [x] **small build** try small bulk import. â†’ Did with subset of `DmfCostanzo2016` and this works well. We just have a ton of debug duplicates messages since we are importing multiple datasets. The container only runs with 4 cpus. I believe I set this in docker desktop settings, but a bit unsure.
- [x] **small build** document bulk import errors. â†’ [[Docker Common Build Error - Invalid value for option '--nodes'|dendron://torchcell/database.docker#docker-common-build-error---invalid-value-for-option---nodes]]

- [ ] **docker volumes** - map local `"data/torchcell"` to docker `"data/torchcell"`â†’ Adding torchcell environment environment variables to clean things up some. â†’ Added `BIOCYPHER_OUT_PATH` since the bash script is specific to the `.env`

```bash
pip install git+https://github.com/Mjvolk3/torchcell.git@main
```

â†’ `"docker-entrypoint.sh"` runs every time a docker container spins up, but the docker image needs to rebuild to use this entrypoint... might want to have two separate docker files, one for latest stable that downloads the latest stable build, and the other for latest commit that pip installs from source. This second one will be most useful to me immediately. I will try another build overnight with the pip install. â†’ Added the install to the entrypoint.

- [ ] Path issue in my biocypher overwrite... maybe try to git pull my version to test.

- [ ] **query index** - After building lmdb, also create experiment and graph label indices.

- [ ] **small build** fix issue with neo4j desktop import on `SmfKuzmin2018` issue data.. Forget what the actual error is. Investigated report and found `PhenotypeMemberOf` had couldn't make link due to missing phenotype node. The `reference_phenotype` was missing. Added for loop block for `experiment_reference_index` to add data to other other than `SmfKuzmin2018`.

- [ ] **small build** We should have three workflows... bash script, VsCode tasks. One is `tcdb_build` always copying script from local since this should be fast, `tcdb_build_image_fresh` which rebuilds the image, pulls the image then runs build., and `tcdb_build_stable` which uses the the latest pypi package for building. This give a nice checkpoint for different datasets.
- [ ] **local lmdb** query should be used to write an `lmdb` to raw along with some txt description possibly. This separates the query raw `lmdb` key-value store writing nicely with the `CellDataset`. I want to keep index creation on the side of the query. There are some that are dead obvious. Like label in phenotype, and experiment origin. Additional indices can always be created later, but I think these two are essentially for now.

- [ ] **small build** - Check other fitness adapters.

- [ ] **remote build** apptainer build image
- [ ] **remote build** try small db bulk import
- [ ] **remote build** try db query

- [ ] **small build** - check nan import case.

- [ ] **small build**
- [ ] **local lmdb**
- [ ] **remote build**

## 2024.02.14

- [x] Spin up tc-neo4j container. `chmod +x  database/local-package/docker-entrypoint.sh`. â†’ Forgot this line `COPY --chmod=755 ./local-package/* /startup/` so need to rebuild ðŸ˜¡
- [x] Rebuild tc-neo4j image â†’ started 16:32 and finished 18:17... almost 2 hours for the build! ðŸ¢
- [x] Pull tc-neo4j image.
- [x] Document **python publish** â†’ [[Pypi Publish|dendron://torchcell/pypi-publish]], [[Versioning|dendron://torchcell/versioning]]
- [x] Add  loop block for `experiment_reference_index` to add data `reference_phenotype` data for `get_phenotype_nodes`
- [x] Run [[Test_kuzmin2018_adapter|dendron://torchcell/tests.torchcell.adapters.test_kuzmin2018_adapter]] with all pass.
- [x] **small build** Fix issue with `SmfCostanzo2016`, cannot pickle Environment. â†’ Rewrote `get_nodes` to find error now cannot download file. waiting... â†’ Very strange that issue is not showing up anymore. Keep eyes ðŸ‘€ out .
- [x] Run [[test_no_duplicate_warnings|dendron://torchcell/tests.torchcell.adapters.test_costanzo2016_adapter#test_no_duplicate_warnings]] with all pass.
- [x] **small build** try local query â†’ `cypher-shell` works

- [ ] **small build** - currently only the `SmfKuzmin2018` was being used for testing. Check other fitness adapters.

- [ ] **small build** fix issue with neo4j desktop import on `SmfKuzmin2018` issue data.. Forget what the actual error is. Investigated report and found `PhenotypeMemberOf` had couldn't make link due to missing phenotype node. The `reference_phenotype` was missing. Added for loop block for `experiment_reference_index` to add data to other other than `SmfKuzmin2018`.

- [ ] **small build** try small bulk import.
- [ ] **small build** We should have three workflows... bash script, VsCode tasks. One is `tcdb_build` always copying script from local since this should be fast, `tcdb_build_image_fresh` which rebuilds the image, pulls the image then runs build., and `tcdb_build_stable` which uses the the latest pypi package for building. This give a nice checkpoint for different datasets.
- [ ] **local lmdb** query should be used to write an `lmdb` to raw along with some txt description possibly. This separates the query raw `lmdb` key-value store writing nicely with the `CellDataset`. I want to keep index creation on the side of the query. There are some that are dead obvious. Like label in phenotype, and experiment origin. Additional indices can always be created later, but I think these two are essentialy for now.

- [ ] **remote build** apptainer build image
- [ ] **remote build** try small db bulk import
- [ ] **remote build** try db query

- [ ] **small build** - check nan import case.

- [ ] **small build**
- [ ] **local lmdb**
- [ ] **remote build**

## 2024.02.13

- [x] **Neo4j Enterprise**, switch to enterprise since free for academics according to @Sebastian-Lobentanzer â†’ changed everything over to enterprise. For this you must download `/local-package`, and the `entrypoint.sh`. Can later automate this. â†’ Verified [Neo4j Licensing](https://neo4j.com/open-core-and-neo4j/), "Universities for teaching and learning" then can be transitioned to altruistic projects or something else like it, but we should still be able to use Enterprise. We want to do this for the multiple db problem, start and stop neo4j issues.
- [x] **Neo4j Enterprise** test image build to see if it works â†’ [[2024.02.13 - Docker image Startup vs Apptainer image startup|dendron://torchcell/database.apptainer#20240213---docker-image-startup-vs-apptainer-image-startup]] â†’ Build was exited due to vscode lag. Restarted. ðŸ¢ This image buid seems even slower than the community one. â†’ pulled image.
- [x] **semantic versioning** â†’ updates to files locally now work as expected, similar to `bumpver` [python-semantic-release](https://python-semantic-release.readthedocs.io/en/latest/configuration.html#config-build-command). Dropping bumpver from dependencies.
- [x] **python publish** add publish github action â†’ `"workflows/python-publish.yaml"` â†’ Everything works. We are using `semantic-release` for versioning and using a bash script with vscode tasks to push to pypi. This works pretty well.

## 2024.02.12

- [x] **automate local build**.. Should have called small build, local build, but we will just keep it the same for time tracking.
- [x] Get **local query** to work with some bash and python scripting. â†’ [[Docker Image and Container Life Cycle - Local Source with Env Source|dendron://torchcell/database.docker#docker-image-and-container-life-cycle---local-source-with-env-source]] â†’ This works but I am thinking I should start building up a command line tool to make things more systematized instead of having a bunch of floating scripts.
- [x] **cliff cli** for building db. â†’ Wrote some files for this but could not get the outputs to stream properly. Things kept getting stuck. â†’ Alternative solution is to use a vscode task that can spawn an external process.
- [x] `tcdb: build linux-arm` used to building the local db. â†’ This works well!
- [x] Find way to get lmdb from database â†’ This is now possible but it is a bit ugly. The crux of the matter is that we can build the db, but we cannot get some of the standard tools to interact with it such as `cypher-shell`. You cannot even add the db with `cypher-shell`. I'm actually not sure if this is a good or bad thing because we need the database to be down for us to bulk import to it.

## 2024.02.10

- ðŸ”² Properly submit pull request related to `import_call_file_prefix` â†’ moved to [[user.mjvolk3.torchcell.tasks.future]]
- ðŸ”² Check nan import case.

## 2024.02.09

- [x] #pr.biocypher.import_call_file_prefix, path mapping `import_call_file_prefix` â†’ Found a solution! it works reasonably well and I don't think it will change the src too much. Using locally forked `Biocypher`. â†’ Need to follow pr instructions properly first.
- [x] **small build** try small bulk import. â†’ Was able to import `SmfKuzmin2018`. ðŸŽ‰
- ðŸ”² **small build** try a standard path without time on `SmfKuzmin2018` â†’ Not necessary with Biocypher overwrite.

## 2024.02.08

- ðŸ”² **small build** try a standard path without time on `SmfKuzmin2018`
- ðŸ”² Add  loop block for `experiment_reference_index` to add data `reference_phenotype` data. All but `SmfKuzmin2018` should need this.

## 2024.02.07

- [x] For managing data will wand use [wandb sync](https://docs.wandb.ai/ref/cli/wandb-sync)
- [x] **small build**, added `./notes/assets/scripts/open-external-terminal.sh` which opens an external terminal to run commands which will be good for run database builds to prevent risk of crashing vscode.
- [x] **small build** import `torchcell` in container.
- [x] **small build** `torch-scatter` install issue. We have two options, break dependency chain with careful importing, fix install image to allow install. Second option is more  general. Investigating. â†’ fixed `gcc` and `g++` availability for `torch-scatter` install by commenting out Dockerfile purge.
- [x] **small build** verify that the current build method works for quickly updating src. â†’ `rm -rf dist/*`, `bumpver update -p`, `python -m build`, `twine upload dist/*` â†’ This is fast enough.
- [x] **small build** the issue for docker import is likely related to paths. Inspect paths in config. â†’ The issue lies with config `import_call_file_prefix` which prevents the dynamic versioning of the db. The biocypher source should probably be updated [[user.mjvolk3.torchcell.tasks.future]]
- ðŸ”² **small build** Fix issue with `SmfCostanzo2016`, cannot pickle Environment. â†’ Rewrote `get_nodes` to find error now cannot download file. waiting...
- ðŸ”² **small build** fix issue with neo4j desktop import on `SmfKuzmin2018` issue data.. Forget what the actual error is. Investigated report and found `PhenotypeMemberOf` had couldn't make link due to missing phenotype node. The `reference_phenotype` was missing. Added for loop block for `experiment_reference_index` to add data
- ðŸ”² **small build** For now separating `biocypher-out` from container. Produce `biocypher-out` via local lib. Need to bind `biocypher-out`
- ðŸ”² **small build** try small bulk import.
- [x] **small build** update query script, `twine`, pip install in container for updating source quickly.build â†’ abandoned this idea, as even it is slower for development than the copying of source method.
- ðŸ”² **small build** try local query
- ðŸ”² **small build** pip install inside env for quick src update. We should have three workflows... thinking just bash scripts. One is `tcdb_build` always pip installing `torchcell -U` since this should be fast, `tcdb_build_origin` which rebuilds the image, pulls the image then runs build.  
- ðŸ”² **local lmdb** query should be used to write an `lmdb` to raw along with some txt description possibly. This separates the query raw key value store writing nicely with the `CellDataset`.
- ðŸ”² **remote build** apptainer build image
- ðŸ”² **remote build** try small db bulk import
- ðŸ”² **remote build** try db query

## 2024.02.06

- [x] Setup **small build** with these 5 datasets on local neo4j-4.4.30. Not sure if there is a distinction between this and community version. â†’ Looks like we need to rebuild the image for both `amd` and `arm` [[2024.02.08 - Troubleshooting Docker Build Local|dendron://torchcell/database.docker#20240208---troubleshooting-docker-build-local]]. We are avoiding using docker compose because this prevents us from transferring this workflow easily to Delta slurm cluster. â†’ Considering github actions as a simpler solution [[GitHub Action Docker Build and Push Template|dendron://torchcell/database.docker#github-action-docker-build-and-push-template]] â†’ We need to save time building images... Right now it takes nearly 45 min to both build and push image ðŸ¢ [[Docker Update TorchCell Source Without Image Rebuild|dendron://torchcell/database.docker#docker-update-torchcell-source-without-image-rebuild]] â†’ Nearly the entire time is dominated by the python environment. â†’ ðŸ˜²ðŸ˜‘ðŸ« [[Docker Image and Container Life Cycle|dendron://torchcell/database.docker#docker-image-and-container-life-cycle]] â†’ things working now with checking correct container and updates to allow executable permission. Running container build overnight.

- [ ] Setup **remote build**, try to get to full size datasets to get a time estimate of the entire build.
- [ ] Test **local query** to get `Dmf` data for `DCell` benchmark. Take only deletions for benchmark dataset. Verify with `Dcell` publication.
- [ ] Run query locally and write **local lmdb**.
- [ ] Write class for **`get_indices`** on lmdb and write indices to `processed`. lmdb should be a cell dataset and should be written to `/scratch`. Indices for fitness dataset can include: `deletion_number`, `experiment_name`, `p_value_threshold` (check supplementary for this one.)  
- [ ] **Document DB** steps for neo4j db continue on [[Docker|dendron://torchcell/database.docker]] â†’ [[Docker|dendron://torchcell/database.docker]], [[Delta Build Database from Fresh|dendron://torchcell/database.apptainer#delta-build-database-from-fresh]] â†’
- [ ] Double **check adapters** for Fitness data.
- [ ] **remote build**

- [ ] Save list of `sha256` duplicates somewhere. Can query db based on these and construct an lmdb to investigate or do additional checks on query.

- I have a suspicion that the that the docker build importing `None` doesn't work with the biocypher provided docker compose because of the mismatched neo4j version. Unsure..

## 2024.02.05

- [x] Move configs â†’ Used .env for `biocypher_config` and `schema_config` this way we can easily move between local and Delta. â†’ [[TypeError NoneType object is not iterable|dendron://torchcell/torchcell.adapters.costanzo2016_adapter#typeerror-nonetype-object-is-not-iterable]], [[Path Issue Indicated by KeyError 'experiment reference'|dendron://torchcell/torchcell.adapters.costanzo2016_adapter#path-issue-indicated-by-keyerror-experiment-reference]]
- [x] Rewrite adapter for `DmfCostanzo2016` â†’ Since rewriting we have some double logging. This can be silenced with setting the `biocypher._logger` level at the beginning of the script. We have some error that the ontology has multiple inheritance. â†’ Runs but needs to be double checked.
- [x] Ontology with multiple inheritance might cause some issues. Investigate. â†’ [[Warning - The Ontology Contains Multiple Inheritance|dendron://torchcell/torchcell.adapters.costanzo2016_adapter#warning---the-ontology-contains-multiple-inheritance]]
- [x] Fix the use of datamodels on `Kuzmin2018` this will involve rewriting the `extract_systematic_gene_names`, just follow example from `Costanzo2016`. Also make all genotype perturbations list.
- [x] Rewrite adapter for `SmfKuzmin2018`
- [x] Make perturbations list for `DmfKuzmin2018`
- [x] Make perturbations list for `TmfKuzmin2018`
- [x] Overwrite get to disallow torch indexing... â†’ Unnecessary and makes get more complicated can add back later if necessary. In [[torchcell.datamodels.schema]] I would like to try to keep python native types for now. We can cast to tensor in `CellDataset`.
- [x] Rewrite adapter for `DmfKuzmin2018`
- [x] Look into log to see what "no missing label" means â†’ with multiprocessing the "No duplicate nodes" and "No duplicate edges" don't make it to the log. This will break the previous tests of these adapters. â†’ Not sure what to do about this... â†’ I see now we just need to find the right handler... I already dealt with this ðŸ˜… moving on.  â†’ "no missing label" is the label of either `BioCypherNode` or `BioCypherEdge`.
- [x] Rewrite adapter for `TmfKuzmin2018` â†’ All now work but I fear there are lurking mistakes with all of the copying.
- [x] Look into running apptainer locally so we can have some consistency between `local` and `Delta`. â†’ This cannot be done because apptainer relies on linux kernel #ChatGPT
- ðŸ”² Setup small build with these 5 datasets on local neo4j-4.4.30. Not sure if there is a distinction between this and community version.
- ðŸ”² Test query to get `Dmf` data for `DCell` benchmark. Take only deletions for benchmark dataset. Verify with `Dcell` publication.
- ðŸ”² Run query locally and write lmdb.
- ðŸ”² Write class for getting indices on lmdb and write indices to `processed`. lmdb should be a cell dataset and should be written to `/scratch`. Indices for fitness dataset can include: `deletion_number`, `experiment_name`, `p_value_threshold` (check supplementary for this one.)  
- ðŸ”² Document download steps for neo4j db continue on [[Docker|dendron://torchcell/database.docker]]
- ðŸ”² Get rid of the `preprocess_config.json â†’ moved to [[user.mjvolk3.torchcell.tasks.future]]
- ðŸ”² Change `reference_environment` to `environment`â†’ moved to [[user.mjvolk3.torchcell.tasks.future]]
- ðŸ”² Double check fitness adapters.

## 2024.02.04

- [x] Check that we can still write to the database.
- [x] Get querying working.

## 2024.02.03

- [x] Dockerfile python.
- ðŸ”² Document download steps
- ðŸ”² CI/CD for docker image builds queued on changes in Dockerfile or version of `torchcell` lib. â†’ moved [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² Check that we can still write to the database
- ðŸ”² Test a query from the database.
- ðŸ”² Write a query to produce an lmdb in some output dir. `/scratch` on delta.

## 2024.02.02

- [x] Paper outline state of things â†’ [[Outline|dendron://torchcell/paper.outline]]
- [x] Correct the data schema â†’ guessed around when this completed.
- ðŸ”² Rewrite the adapter for `DmfCostanzo`.
- ðŸ”² Document the process for downloading a dbms.
- ðŸ”² Test query script on delta

## 2024.02.01

- [x] Test the dbms on Delta with an interactive terminal.
- [x] Build db with sbatch script. â†’  `apptainer build neo4j_4.4.30_community.sif docker://neo4j:4.4.30-community` â†’ [[Delta Build Database from Fresh|dendron://torchcell/database.apptainer#delta-build-database-from-fresh]]
- ðŸ”² Document the process for downloading a dbms.

## 2024.01.31

- [x] Replace `Sgd` with the correct `Sga` for synthetic genetic array.
- [x] We can simplify the data model [[Issues with the Current Data Scheme that Uses Different Named Phenotypes|dendron://torchcell/torchcell.datamodels.ontology_pydantic#issues-with-the-current-data-scheme-that-uses-different-named-phenotypes]]. Do this first for `Costanzo`.
- ðŸ”² Rewrite the adapter for `SmfCostanzo` and `DmfCostanzo`.
- ðŸ”² Get rid of the `preprocess_config.json
- ðŸ”² Fix the use of datamodels on `Kuzmin2018` this will involve rewriting the `extract_systematic_gene_names`, just follow example from `Costanzo2016`. Also make all genotype perturbations list.
- [x] Check the run with modified edges. â†’ Crashed due to memory 60GB tied up in memory and another 40 GB in something called Fig and Media. Not sure what that is. â†’ Going to commit first to see if we can run this db build on Delta. â†’ Gave up on rebuild locally.
- ðŸ”² Revert get_edges to old version.
- [x] Write Cron Job for Db build â†’ instead moving to delta and a locally. Move to [[user.mjvolk3.torchcell.tasks.future#future]]

## 2024.01.30

- [x] Read through the adapter and find where we are making errors with `PhenotypeMemberOf`. Since there are only two the first place to look is probably reference. â†’ This is the case, `f4a674caf85adc0800397536db8ed9d7a941e70d8ecf86a3843c25f28516b4a7` this is the phenotype. â†’ The issue is that the `experiment_reference_index` differs from the indices associate with each experiment. Needs to be fixed within the property. â†’ some dumping issue, likely with serialization. â†’ The issue was that we did not properly type hint the child class `FitnessExperimentReference` and instead were only type hinting `ExperimentReference` in `ExperimentReferenceIndex` [[Data|dendron://torchcell/torchcell.data.data]]. I am a bit worried about this because as the list of supported classes under the `ExperimentReferenceIndex` expand we will have to make sure we maintain the typing it would be better to have and intermediary defined type where we can add a list of these objects. â†’ I have prototyped a `ExperimentReferenceType` and this works well. I think this will be more maintainable in the future making the classes more streamlined centralizing typing to one location.
- [x] Do a local query test on the `DmfCostanzo2016` to get an idea of speed. â†’ Some of the data does not get validated properly, every genotype looks like a `BaseGenotype`. I think this is ok since the genotype definitions as we have them are more functional when they should be imperative. Perturbations serve as the imperative equivalent of the Genotype types. I think we might want to get rid of these genotype types. We could just ignore them for now, but they aren't totally clear. For example the Suppressor genotype takes a possible list of suppressor alleles when there really only needs to be one suppressor allele. This is confusing and would cause the number of possible genotypes to balloon.
- [x] Is it possible to start the Neo4j db from the command line? â†’ yes! `cd /Users/michaelvolk/Library/Application Support/Neo4j Desktop/Application/relate-data/dbmss/dbms-010e9a89-2083-4d8f-8f08-4770f372f858`, `./bin/neo4j start`,  `./bin/neo4j stop`
- [x] Investigate heap error in config â†’ `/Users/michaelvolk/Library/Application Support/Neo4j Desktop/Application/relate-data/dbmss/dbms-010e9a89-2083-4d8f-8f08-4770f372f858/conf/neo4j.conf` dbms.memory.heap.max_size=10G previously 1G
- [x] We are still only getting around `1e3 it/s` try to optimize. â†’ Maxed out thread workers in config. `dbms.threads.worker_count=10` now achieving > `1e4 it/s` which should be sufficient for now.
- [x] Summary of neo4j times. â†’ Bulk import is very fast on order of minutes, this means that the db can be easily scrapped and made new. Querying is fast after optimizing the heap and num workers for threading, on order of mins. The slowest portion is adapters which is on order of hours or days in the worst case. I think around 14 hours right now for `DmfCostanzo2016`.
- [x] Try alternative async parallelization. â†’ After looking into this more it won't provide much benefit unless we use queues for memory management.
- [x] Many of `perturbed_gene_names` and `systematic_gene_names` are `null` in `_get_genotype_nodes`. Correct this. â†’ Fixed in `DmfCostanzo2016` but the others need modifying
- ðŸ”² Try bulk import of smf data to Neo4j db on Delta.
- ðŸ”² Try to query deletion data from Delta Neo4j.
- ðŸ”² Change `reference_environment` to `environment`
- [x] Make a duplicate adapter for `async` â†’ abandoned this idea.

## 2024.01.29

- [x] Send NCSA a message about getting help with port forwarding to be able to interact with the neo4j browser. â†’ Asked about port forwarding for using the Neo4j browser.
- [x] We are getting low iterations `11838875/20705612 [2:09:53<1:22:32, 1790.52it/s` on looping through the entire dataset. I believe this might be because we have no parallelization directly in the `get_edges` method. â†’ There are some async options to help speed this up. â†’ Finished in about 14 hours. That slow iteration shown was the last one before writing all edge csv. â†’ No duplicates.
- [x] Check how we can get the dir for bulk import. â†’ `bc_output_directory:
'/Users/michaelvolk/Documents/projects/torchcell/biocypher-out/20240129211021'` then `"neo4j-admin-import-call.sh"` always sits in this dir. We can just cd to the neo4j terminal and and run this command after returning this information from the writer.
- [x] Try bulk import of `DmfCostanzo2016` into Neo4j database. â†’ Import took 4m 19s but there are reported bad entries
- [x] Investigate bad entries. â†’ I think these just need to be corrected after reading though the adapter again.

```bash
cat import.report
f4a674caf85adc0800397536db8ed9d7a941e70d8ecf86a3843c25f28516b4a7 (global id space)-[PhenotypeMemberOf]->29d4c34a4008c66eb05e3a34e7366cd05e8fb3ca5b58fd7dae4a215bb201f3ec (global id space) referring to missing node f4a674caf85adc0800397536db8ed9d7a941e70d8ecf86a3843c25f28516b4a7
f4a674caf85adc0800397536db8ed9d7a941e70d8ecf86a3843c25f28516b4a7 (global id space)-[PhenotypeMemberOf]->73ca7d4694085d74c54e04db8956f128262c0ef4cf2bbe46775b97cb170a7813 (global id space) referring to missing node f4a674caf85adc0800397536db8ed9d7a941e70d8ecf86a3843c25f28516b4a7
```

- [x] Check number of nodes on `DmfCostanso2016` import and number of edges â†’ `Nodes: 27,507,626, Edges: 124,244,54`0

## 2024.01.28

- [x] Revise `if genotype.perturbation:`, check WT. There is some bug..
- [x] Chunk what can be chunked in nodes and edges for `DmfCostanzo2016` â†’ Some of these we had to be careful to not chunk so we could remove obvious duplicates. We could revert if this continues to cause memory issues, but it provides more clarity when reviewing the data processing.
- [x] Do I need a `_get_media_environment_ref_edges`? â†’ For `DmfCostanzo2016` we an just get the media from the ref.
- [x] Get neo4j docker image working on delta with apptainer. â†’ It seems that there are some java issues still but I have tried both using `jdk-11` and and `jkd-17` and keep getting back the same errors. Google searches in including some stack overflow suggest some issue with permissions. â†’ Got things to work. [[Starting Neo4j Database with Apptainer on Delta Interactive CPU|dendron://torchcell/neo4j.delta#starting-neo4j-database-with-apptainer-on-delta-interactive-cpu]]. It doesn't seem there is anyway to forward ports for access the database on local host. Can send NCSA help a message about this.
- [x] Run `1e6` and and `1e7` tests
- ðŸ”² Change `reference_environment` to `environment`

## 2024.01.25

- [x] Check `torchcell env` run of [[torchcell.knowledge_graphs.create_scerevisiae_kg]]. Did not finish in 10 hrs of running. â†’ Used 4 workers and got through 3 total iterations on the `2e7` data instances. â†’ Killed job since need computer for development around neo4j db on `Delta`.
- [x] Look into `ERROR -- Edge generation method generated an exception: 'ExperimentReference'  object has no attribute 'environment'`, which appeared when running [[Create_pypy_scerevisiae_kg|dendron://torchcell/torchcell.knowledge_graphs.create_pypy_scerevisiae_kg]] â†’ Think this is fixed. Had some typos related to dict key vs attr
- [x] [Push docker image](https://hub.docker.com/repository/docker/michaelvolk/torchcell_biocypher/general) â†’ Doing this so we can potentilly pursue the singularity option on delta. This way we could avoid the neo4j install directly to `delta`.
- [x] Transfer `Jdk17` to Delta with Globus `x64 Compressed Archive 174.03 MB
https://download.oracle.com/java/17/latest/jdk-17_linux-x64_bin.tar.gz ( sha256)`

## 2024.01.24

- [x] Result of building db with 6 workers. â†’ Ran out of memory and crashed computer when running `DmfCostanzo2016`
- [x] Update python envs for use with python â†’ Using a virtual env instead of conda env.
- [x] There is no `pypy torch`. This looks like the last necessary lib. Investigate chain of files to make Datasets. â†’ no `torch` in [[torchcell.data.data]], [[torchcell.datamodels.pydantic]], [[Ontology_pydantic|dendron://torchcell/torchcell.datamodels.ontology_pydantic]] â†’ Separated out all `torch`. Used [[Reader|dendron://torchcell/torchcell.dataset_readers.reader]] to be able to do this. Now we can run the env with pypy.
- [x] Rebuild `tc` with pypy 3.9. It seems with conda env I can only use 3.9 but mac m1 allows for 3.10 install. I am unsure how to use this pypy3.10 with my conda env libs. â†’ Have a working version that uses local `pypy3.10` since I couldn't get `pypy3.9` to install in a conda env. I use a virtual env with pypy3.10 called `pypy`. This is now stored in the env dir. It can be activated  and used with the pypy_adapter modules.
- [x] pypy run `Kuzmin2018`
- [x] Update `add_frontmatter` because it was replacing the `.pypy` with `py`. This is because I previously thought no other `.py` would show. â†’ Now we just replace the file extension. â†’ split out file extension for this.
- [x] Test speed difference on `Kuzmin2018`, `mp` vs. `pypy` â†’ `pypy` time: 15m 30s , `mp` time 40s â†’ Yikes. We can see why this is a major issue. Lets test regular python with generators â†’ `regular` time 7m... â†’ Pretty awful news considering my last few hours of effort...ðŸ˜¡
- [x] Write `Costanzo2016` `pypy_adapter`
- [ ] Property `experiment_reference_index` should be run in `process`
- [ ] Look into alternatives. Chunking the dataset... this will cause issues with the reference index... Would need to consider chunks of 1 million. Then each Chunked dataset would get a new reference index for that chunk.
- [ ] Hybrid `mp` for nodes and with yield for edges for memory purposes... gees ðŸŒŠ. I hate the hacks.

- [ ] pypy run `Costanzo2016`
- [ ] Run a `create_graph` using pypy
- [ ] Write a `requirements.txt` to be able to easily recreate the pypy env

## 2024.01.23

- [x] Optimize `DmfCostanzo2016` [[Costanzo2016_adapter|dendron://torchcell/torchcell.adapters.costanzo2016_adapter]] â†’ Changed edge functions that can should rely on reference.
- [x] Update `_get_phenotype_experiment_ref_edges` in adapters to use reference.
- [x] Note on classes using Liskov substitution [[Using Static Methods like in get_perturbation|dendron://torchcell/torchcell.adapters.costanzo2016_adapter#using-static-methods-like-in-get_perturbation]]
- [x] Test `DmfCostanzo2016` [[Costanzo2016_adapter|dendron://torchcell/torchcell.adapters.costanzo2016_adapter]] â†’ Small works
- [x] Correct dataset names in adapters â†’ Fixed names. Could take class name for this.
- [x] Test small [[Create_scerevisiae_kg|dendron://torchcell/torchcell.knowledge_graphs.create_scerevisiae_kg]] works.
- [x] Look into hosting Neo4j â†’ Should be possible. Have slurm script start db, python script to connect to db via local host. Might have to change some of the configs to connect. Run python script to build the db. close db. New slurm script for query and lmdb db construction.
- [ ] Run db build overnight. I am running with few workers since this could lead to memory overhead.
- [ ] Look at `_get_perturbation` using some index probably which I don't think is necessary.

## 2024.01.22

- [x] Database Pipeline figure ![](./assets/drawio/torchcell-neo4j-databae-pipeline.drawio.png)
- [ ] Reduce repetitive code for fitness datasets. After query for @Junyu-Chen, first remove comments from `Kuzmin2018` and copy clipboard compare. Maybe need a experiment type parser. Whether or not to do this needs some careful consideration. It'll be regretted either way.

## 2024.01.21

- [x] Parallelize the other adapters. At least `TmfKuzmin2018` and `DmfKuzmin2018`.
- [ ] Reorganize [[Create_scerevisiae_kg|dendron://torchcell/torchcell.knowledge_graphs.create_scerevisiae_kg]]
- [ ] Run [[Create_scerevisiae_kg|dendron://torchcell/torchcell.knowledge_graphs.create_scerevisiae_kg]]
- [ ] Remove `preferred_id` in `_get_perturbation`

## 2024.01.20

- [x] The `db` is taking a long time with the full datasets due to `DmfCostanzo`. Start two runs one small and one normal. Hopefully can trouble shoot with the small
- [x] Message @Sebastian-Lobentanzer about the slow `db` build. â†’ Let's hope I'm not bothering him. After finding solution to `concurrent.futures`, I let him know this solves my issue.
- [x] Investigate parallelism in `get_nodes` â†’ totally possible using `concurrent.futures`.
- [x] Use `concurrent.futures` to write a second version of `DmfCostanzoAdapter` and run this version in [[Create_scerevisiae_kg|dendron://torchcell/torchcell.knowledge_graphs.create_scerevisiae_kg]]
- [x] Launch [[Create_scerevisiae_kg|dendron://torchcell/torchcell.knowledge_graphs.create_scerevisiae_kg]] â†’ Some of the other adapters should be parallelized.

## 2024.01.19

- [x] Markdown all in one issue... â†’ extension still makes things way to slow, only using linter.
- [ ] Plans for db and class update .... add note
- [x] Review `neo4j` commands to get db in. â†’ I am making a mistake not using docker. Without docker and cannot do the smart querying.
- [x] Create scripts for iteratively adding to database. â†’ Cannot doing this, we need to update every time there is a new db. We rely on remove duplicates from multiple datasets.
- [x] Write src build instructions [[Source Build Instructions|dendron://torchcell/src.build#source-build-instructions]]
- [x] Rebuild `src` so db has the correct functions.
- [ ] Get ChatGSE working â†’ Trying to build small db again. â†’ [[Create_scerevisiae_kg|dendron://torchcell/torchcell.knowledge_graphs.create_scerevisiae_kg]] created graph. â†’ [[SmfKuzmin2018 Docker Import Issues with None and Special Characters|dendron://torchcell/torchcell.datasets.scerevisiae.kuzmin2018#smfkuzmin2018-docker-import-issues-with-none-and-special-characters]] â†’ There also seems to be something strange with the order in which I run `docker compose`, I reran once after just resetting everything and got a different error. Have to be careful to check that the proper version of `torchcell` is being used. I also think it helps to `--force-recreate` to recreate containers even if configuration and image haven't changed... maybe it is changing somewhere? I'm not sure. Shouldn't really need this flag. To avoid the versioning issue it is best to just set it in `env/requirements-tc-graph.txt`.
- [x] torchcell version add to bumpver in toml for `env/requirements-tc-graph.txt`
- [x] Message @Sebastian-Lobentanzer about `None` getting turned into `nan` when using Docker... For now giving up on Docker.
- [ ] Try to build db locally.

- [ ] Read [testmon github actions](https://testmon.org/blog/better-github-actions-caching/)
- [ ] Try [testmon library](https://testmon.org/)
- [ ] @Sebastian-Lobentanzer informed me that we could set the config in the class instantiation, not the config. Change the location of config and corresponding paths.
- [ ] Change `reference_genome` to `reference.genome`
- [ ] Take the

### Features For the User

I want to be able to give @Junyu-Chen the following protocol which should be easy.

1. `python -m pip install torchcell==0.0.?`. I recommend you used a virtual environment for this.
2. Unzip the data I sent you and put it in any `data/*` folder your have.
3. Import the dataset
4. Load dataset
5. Iterate in dataset, and access data

```python
from torchcell.dataset import Yeast9FitnessDataset
yeast9_fitness_dataset = Yeast9FitnessDataset(root:str="data/*")
for data in yeast9_fitness_dataset:
  print(data['experiment'].genotype)
  print(data['experiment'].phenotype)
  print(data['reference'])
```

### Features I want

1. `Yeast9FitnessDataset` calls query in the lmdb.
2. Basic indexing features. For instance it should hold an index for each dataset that a given data point comes from. Pydantic class for index using pydantic to pass standardized data. Do `isinstance` check on on say genotype, environment, or phenotype, then could do another layer of `isinstance.`
3. `indices` property that returns a list of index objects. These index objects need to be general enough to apply to a lot different types of data. They can be based off the current pydantic data model.
4. This will be the zipped up process, preprocess, and raw dirs. So it will contain the lmdb. preprocess can capture the index or indices property.

```python
class BaseCellDataset(Dataset):
  pass

class Yeast9FitnessDataset(BaseCellDataset):
  pass
```

## 2024.01.18

- [x] Add perturbations to the knowledge graph. â†’ Did this by writing `staticmethod` I think this is more intuitive and will save looping over the dataset multiple times
- [x] Build out testing for individual datasets. The tests should not allow graphs with duplicates to pass. Need to intercept the `Biocypher` logging for this. Consider just copying something like `tc_create_knowledge_graph.py` for this. â†’ Added test but I think it is unclear if this should be handled upon the writing of the class instead of in tests.
- [x] Make sure files generated from tests are being properly removed so we aren't generating superfluous data. â†’ Little on the current design choice. [[test_no_duplicate_warnings|dendron://torchcell/tests.torchcell.adapters.test_costanzo2016_adapter#test_no_duplicate_warnings]]
- [x] Optimize costanzo adapter so we can save on copying. Seems maybe premature but I think it should pay off. First look to `staticmethod` ... â†’ Mocked up a function and realized that the juice ðŸ§ƒ is not worth the squeeze ðŸª—.  [[Using Static Methods like in get_perturbation|dendron://torchcell/torchcell.adapters.costanzo2016_adapter#using-static-methods-like-in-get_perturbation]].
- [x] Inspect ontology visualization with biocypher. â†’ [[Useful Functions for Debugging Adapter and Printing Ontology|dendron://torchcell/torchcell.adapters.costanzo2016_adapter#useful-functions-for-debugging-adapter-and-printing-ontology]]
- [x] Sort `list[GenePerturbation]` in `BaseGenotype`, these are now sorted
- [x] Add `BaseGenotype` as a type of node. â†’ Maybe what we want is whether it is an interference or deletion genotype... â†’ `BaseGenotype` deserialized the children class so this doesn't make much sense. I think it is best to hold off for now, but there is a good idea here which is that data can be easily add to the graph for querying, but this starts to get a bit hacky because you move away from the pydantic models.
- [x] Read Biocypher docs on set `_set_types_and_fields_` â†’ not in docs just in the collectri example. Don't think this is necessary to use since we have the pydantic models. â†’ Removed the `node_type` and `edge_type` args.
- [x] Write the `Smf Kuzmin` Adapter. Consider optimizations to not loop over the dataset multiple times. â†’ Tests pass for `SmfKuzmin2018Dataset`
- [x] Write the `Dmf Kuzmin` Adapter â†’ Doing some damage control ðŸ¥Š since there is an issue with the `DmfKuzmin2018Dataset` [[Processing Kuzmin Double Mutants in Trigenic Rows|dendron://torchcell/torchcell.datasets.scerevisiae.kuzmin2018#processing-kuzmin-double-mutants-in-trigenic-rows]]. This solved the issues with duplicate node and edge types in the adapter â†’ Works well but takes some time to complete.
- [x] Fix `_get_genome_nodes` to work on reference
- [x] Try to used pytest cache â†’ We don't want to cache the result as the data is big, we really just want to compare last passing test date with date modified on the `src`.
- [x] See if we can modify the tests on the data so they only run when the `src` has been updated or the last test failed, otherwise skip to "success". â†’ This relies on both git history and the pytest history which makes things a bit complicated. I could get the git date but only using full paths and I couldn't get the pytest data. â†’ Seems that [testmon library](https://testmon.org/) can help with this.
- [x] Write the `Tmf Kuzmin` Adapter
- [x] Update `DmfKuzmin` and `TmfKuzmin` tests for [[Test_kuzmin2018_adapter|dendron://torchcell/tests.torchcell.adapters.test_kuzmin2018_adapter]]
- [x] Write the `DmfCostanzo2016Adapter` â†’ This is essentially a copy of the `DmfKuzmin`... also `TmfKuzmin`is a copy of `DmfKuzmin`. These adapters could be united under a common class but I am hesitant to do this yet.
- [x] Test for `DmfCostanzo2016Adapter` â†’ [[Test_costanzo2016_adapter|dendron://torchcell/tests.torchcell.adapters.test_costanzo2016_adapter]]
- [x] Start writing [[Create_scerevisiae_kg|dendron://torchcell/torchcell.knowledge_graphs.create_scerevisiae_kg]] with a small example using `SmfCostanzo` and `SmfKuzmin` to see if it works. â†’ Works well.
- [x] Rerun the `create_kg` with all datasets. Delete current data and run overnight.

## 2024.01.17

- [x] Write the `Smf Costanzo` Adapter â†’ First pass and it looks good, haven't done thorough check but did a lot of renaming.
- [x] Look into integrating the `Biocypher` functionality more directly with `TC`. â†’ Set @Sebastian-Lobentanzer ... for now will keep the config in the config dir as I think the software looks for `yaml` files in root and config.
- [x] Run `DmfCostanzo` to update the dataset
- ðŸ”² Add perturbations to the knowledge graph.
- ðŸ”² Build out testing for individual datasets. The tests should not allow graphs with duplicates to pass. Need to intercept the `Biocypher` logging for this. Consider just copying something like `tc_create_knowledge_graph.py` for this.

## 2024.01.15

- [x] Add `ExperimentReferenceIndex` property to `DmfCostanzo`
- [x] Add  `ExperimentReferenceIndex` property to all of Kuzmin. I know this shouts of standardization of the Dataclass, but I want to hold out on this until I complete the data upload cycle for multiple datasets. We can try to keep the consistency in mind for later unification.
- [x] Reorganize datasets for preparation of adapters â†’ [[Costanzo2016 Notes on Design|dendron://torchcell/torchcell.datasets.scerevisiae.costanzo2016#costanzo2016-notes-on-design]]
- ðŸ”² Write the `Smf Costanzo` Adapter
- ðŸ”² Write the `Smf Kuzmin` Adapter
- ðŸ”² Write the `Dmf Kuzmin` Adapter
- ðŸ”² Write the `Tmf Kuzmin` Adapter
- ðŸ”² Write the `Dmf Costanzo` Adapter

## 2024.01.14

- [x] Take notes on duplicate array-query alleles â†’ [[We Did Away with the Notion of Duplicate Query-Array Genes|dendron://torchcell/torchcell.datasets.scerevisiae.neo_costanzo2016#we-did-away-with-the-notion-of-duplicate-query-array-genes]]
- [x] Consider adding `ExperimentReferenceIndex` - might help in Biocypher
- [x] Add `ExperimentReferenceIndex` property to `SmfCostanzo`, this helps with the uploading data to the knowledge graph

## 2024.01.13

- [x] `Dmf` Kuzmin
- [x] `Smf` Kuzmin â†’ Processing is a bit tricky since the table is a bit wack. We have no standard deviation for measured mutant fitness so I put nan.
- [x] `Dmf` revise fitness adding double mutant fitness from the `trigenic` rows
- [x] `Tmf` Kuzmin â†’ `Dmf` not done yet, there are double mutants within the trigenic rows too. This brings up the larger issue of recording the fitness values of lower order combinations. I looked to see if the the single mutant fitness was the same as recorded in Costanzo to see if they just used the same values but it appears they remeasured it... Implementing a `Smf`.
- ðŸ”² `Costanzo ggi` double mutant interaction
- ðŸ”² `Kuzmin ggi` double mutant interaction
- ðŸ”² `Kuzmin ggi` triple mutant interaction
- ðŸ”² Consider adding `ExperimentReferenceIndex` - might help in Biocypher

## 2024.01.12

- [x] `tmf` Kuzmin â†’ Data cannot be easily downloaded from science, moving data to zipped hosted version on my github. This works well, there is always the trick of getting the proper path. Got through download, going to split between `smf` and `dmf` as the other datasets have focused on a single phenotype.
- [x]  Rewrite `TsAllele` to be `Allele` with a `allele_class` attribute. â†’ No this probably a bad idea I think this starts to verge on the idea that everything is an `allele` you just needs its sequence specification. For now we respect the helpful terminology related to different biological classes and types. In this case I am just going to added an additional class. â†’ Added `AllelePerturbation` and `SgdAllelePerturbation`. I think this solves the problem nicely without having to rewrite ðŸ“.
- [x] `tmf` Kuzmin. â†’ We need to see allele as a more general type of perturbation and temperature sensitive should be a type. This should help some in de cluttering classes. We need the right amount of clutter. â†’ We find there are alternative alleles not well documented... ðŸ“ rewrite time. â†’ It will exist on the same level as a sort of undifferentiated type, and be associated with the `BaseGenotype`. â†’ [[Unsure of Origin of Alleles|dendron://torchcell/torchcell.datasets.scerevisiae.kuzmin2018#unsure-of-origin-of-alleles]] â†’ [[All Double Mutants Contain the ho deletion|dendron://torchcell/torchcell.datasets.scerevisiae.kuzmin2018#all-double-mutants-contain-the-ho-deletion]] â†’

## 2024.01.11

- [x] Add `Strain ID` so we can be sure alignment between `smf` and `dmf` the data. â†’ Not doing this. Instead just adding all available data, had to build out more refined pydantic models to do this. [[Costanzo Smf and Dmf Whiteboard Recap|dendron://torchcell/torchcell.datamodels.ontology_pydantic#costanzo-smf-and-dmf-whiteboard-recap]]. This will allow for the alignment of data in the database.
- [x] `Dmf`add all data

## 2024.01.09

- [x] Fix up the `Dmf` class adding `gene_set`, no `Data()`, fixing `preprocess`
- [x] Compute the means on `Dmf` for different temperatures.
- [x] Add a clean up dir method after the download happens to remove superfluous files
- ðŸ”² Convert the `Smf` class to look like the `Dmf` class. â†’ `Array allele name` in `Dmf` has a `-supp1` suffix that seems â†’ `-supp1` is for strains that have a suppression mutation. We should drop these because we have no way of tracking them. â†’ I've hit on some troubling issues in harmonizing the data. I had an inkling before, but now it is obvious that sometimes you need the gestalt before you process individual datasets. Now that I see how all of the mutant fitness data must be processed to come together in the db, it is obvious that I wouldn't have been able to arrive at such a solution by atomistically processing each dataset on its own.
- [ ] Correct the `Smf` adapter
- [ ] Write `Dmf` adapter
- [ ] Run query on Yeast9 genes and save json `(list[genes], fitness)`

## 2024.01.08

- [x] Graphic for comparing key value querying vs graph db querying â†’ ![](./assets/drawio/kv-iteration-vs-graph-db.drawio.png)

## 2024.01.06

- [x] Add `dmf` data, try to keep datasets separate to reduce memory footprint â†’ I now have a combination of the new method with pydantic data models and the old method using pytorch geometric `Dataset` with lmdb. This is a good solution and should allow for the usage of individual datasets and their addition to the neo4j database for more complicated multiplex datasets.

## 2024.01.04

- [x] Add processed dir of `smf`. â†’ There will definitely be some issues with this process and querying from neo4j since we are using the reference index. I guess we could just pull the index from the previously downloaded data, or we could also add the indices to the neo4j data. This would allow for later just hosting the db remotely.
- [x] Add loading to dataset. â†’ works for now.
- ðŸ”² Add `dmf` data. â†’ We should really only be saving the `reference_index` and not reference since this creates a lot of redundant information. â†’ We don't explicitly add the single mutant fitness to the data because this should naturally appear in the graph structure when the data is added. â†’ Using multiprocessing to speed things up for creating experiments and now looks time is estimated to be around 20 mins, which is more reasonable â†’ Failed at 37% do to a memory error.

## 2023.12.31

- [x] Add `Gif` example of a query â†’ [[SMF Costanzo 2016 Query Example|dendron://torchcell/ChatGSE.smf-costanzo2016-demo#smf-costanzo-2016-query-example]]

## 2023.12.30

- [x] Docker and test `ChatGSE`. â†’ Had to get rid of apostrophe for `perturbed_gene_name` with pydantic data validation.

## 2023.12.19

- [x] Finish adding data for `Costanzo SMF` â†’ Using sets with MD5 has to avoid forcing Biocypher to take care of deduplication.
- [x] View data in Bloom. â†’ This works fine, nothing special yet.
- [x] Make temperature a property of environment. â†’ Not doing this. Mostly for making the modelling process consistent. If everything at bottom in a pydantic class I think it will be easier conceptually.
- [x] Publish package to PyPi so we can used docker, and `ChatGSE` â†’ build from src steps
- ðŸ”² Docker and test `ChatGSE`.

## 2023.12.18

- [x] Discuss Biocypher [[Sebastian-Lobentanzer.2023.12.18|dendron://torchcell/meet.Sebastian-Lobentanzer.2023.12.18]]

## 2023.12.17

- [x] Add children as nodes with their data

## 2023.12.16

- [x] Write `FitnessExperimentReference` for `FitnessExperiment` to get around the issue of only recovering the base data upon deserialization. This was dropping the `fitness` and `fitness_std`. I think as a rule of thumb we should probably try to limit typing to single types if possible. â†’ I am puzzled in terms of where the reference state should go. I like it with data instances because at the time of modeling, it is available. In general reference states are typically singular with any given experiment, in some cases there could be a few. For memory they can be kept separate and also in the context of graphs it makes more sense if they are kept together. This way we can link to references and use they compatibility to do selection.
- [x] Separate out the reference from the data itself. â†’ my current solution to this is to have the index as a boolean array, paired with the reference data. I made some pydantic models for this. This will be very useful data for creating nodes within biocypher. The 1st node, publication or study, then 2nd we have reference states (study), and data (study).
- [x] Add publication, reference states, with boolean indices as data. Add studies. â†’ Create an outline of how we can do this in [[notes/assets/notability/notability.pdf]] â†’ Didn't add the boolean indices as data as these should primarily be used for link creation between reference and data. Waiting on publication, which should be linked to dataset.
- [x] Create the necessary relations with biolink predicate, likely in information entity. These include concept links and instance links. â†’ had to change change from the predicates to ther relationship types. These are not capable of being visualized.
Concept links:
  - `experiment` - `member_of` -> `dataset`  
  - `dataset` - `has_member` -> `experiment`
  - `experiment reference` - `member_of` -> `dataset`
  - `dataset` - `has_member` -> `experiment reference`
Instance links:
  - `experiment` - `associated_with` ->  `experiment reference`

- [ ] Children of study into nodes and information content predicates to link them.
- [ ] ChatGSE to get query for extracting only nodes that are temperature 30 and gene deletions.
- [ ] Check that pydantic model names are being propagated as properties so they can be used as selection. Maybe we call this `TcType`.

- [ ] Add immediate children of experiment to graph.
- [ ] Create a ChatGSE demonstration.
- [ ] Add Costanzo doubles to graph. Start with small subset.

## 2023.12.15

- [x] Add `isort` and `black` tasks, since it takes so damn long with the normal vscode extensions.
- [x] TS alleles inspection. â†’ can find some [nature ts alleles SI Table 1](https://www.nature.com/articles/nbt.1832#MOESM5) around 40 % seem to have enough information to reconstruct the allele sequence. For now will avoid sequence specificaiton.
- [x] Add `costanzo` single deletions to graph â†’ Did it! Yay ðŸŽ‰ but there are some issus, which ontologies to use, data serialization and deserialization

## 2023.12.14

- [x] Rename `src` to `torchcell`, refactor notes.
- [x] Test if `Dcell` slim works.
- [x] Check python utility functions â†’ fixed move file, now works well.
- [x] [[Pyreverse|dendron://torchcell/python.lib.pyreverse]]
- ðŸ”² Add costanzo single deletions to graph

- [evotorch](https://evotorch.ai/)

## 2023.12.13

- [x] Clean up miscellaneous files within the workspace in preparation for adding a database to the root. â†’ Moved [metabolic_graph](./assets/images/metabolic_graph.png) to assets, [smf blox plots gene expression](./assets/images/SMF_gene_expression_box_plot_6170_genes_1143_sm.png) to assets
- [x] Test docker build with conda env in [Biocypher collectri](https://github.com/biocypher/collectri) â†’ This works but needed to write a custom `Dockerfile`
- [x] Move collectri example into `tc`. I think one large repo is better for now because there will be some interaction between the adapter, input data etc. Mostly due to my indecision about the right time to do data cleansing, etc. after or before cypher query. â†’ most difficult part is the need to explicitly copy in the `scripts/build.sh`, the default `cp` seemed to be copying everything.

## 2023.12.12

- [x] [Biocypher collectri](https://github.com/biocypher/collectri) real world example tutorial â†’ can use ChatGSE for writing cypher queries which is a great benefit for complex queries over a complicated knowledge graph. It would be nice to know if ChatGSE has access to node properties within the graph.

## 2023.12.08

- [x] Run biocypher tutorial

"This allows maintenance of property lists for many classes at once. If the child class has properties already, they will be kept (if they are not present in the parent class) or replaced by the parent class properties (if they are present)."

- Confusing, how does inheritance work?
  <https://biocypher.org/tutorial.html#section-3-handling-properties>

## 2023.12.06

- [x] Update pydantic for schema viz â†’ Looks like custom functions are needed.
- [x] Pydantic schemas â†’ â›”ï¸ [[Strategy For Writing Pydantic Schema|dendron://torchcell/torchcell.graph.validation.locus_related.locus#strategy-for-writing-pydantic-schema]]
- [x] You cannot inherit both from `owlready2` and pydantic models.
- [x] Investigate serialization and deserialization of data â†’ This works well and give a robust way for reconstructing data in python.
- [ ] Add media components
- [ ] Does high level meta data belong to ontology?

- [ ] Add `Costanzo` singles to neo4j
- [ ] Add `Costanzo` double to neo4j
- [ ] Add morphology to neo4j
- [ ] Check ontology vs SGD ontology GAF.

- Make sure on `Delta` that lightly is uninstalled. It had dependencies on an earlier version of pydantic.

## 2023.12.05

- [x] Figure out linking to ontology with toy data â†’ A lot of care will have to go into this. There are a thousand different ways to flay this cat ðŸˆâ€â¬›. I think the most important thing is to remember that the fundamental data structure is the data instance, which will be the object of all queries for the foreseeable future. Edges from data instance on to ontology are really to assist in this query.
- [x] Process `Costanzo` singles with data validation that matches ontology. Then output list of experiments â†’ We should not include DAmP alleles or ts alleles are they are not true deletions. DAmP is a KanMX at the 3' UTR, and the ts alleles have mutations that make their protein product unstable at high temperatures. In both of these cases there is still some functional protein around. Not inclusing them for this.

- [ ] Add `Costanzo` singles to neo4j
- [ ] Add `Costanzo` double to neo4j
- [ ] Add morphology to neo4j
- [ ] Check ontology vs SGD ontology GAF.

- [The localization and quantitation atlas of the yeast proteome](https://www.weizmann.ac.il/molgen/loqate/)

## 2023.12.04

- [x] Revise ontology into only subclasses. â†’ more works still needed
- [x] Investigate why property keys don't delete â†’ [Why don't property keys get deleted](https://neo4j.com/developer/kb/why-do-my-deleted-property-keys-appear/) Have to create a new DB for this. This shouldn't be an issue since we will only need to rebuild the DB once locally when things are stable.
- [ ] Check ontology vs SGD ontology GAF.

## 2023.12.02

- [x] Look into adding json serialized data. â†’ works well

## 2023.11.30

- [x] Make example `my_ontology.rdf` with `OwlReady2` â†’ Cannot save to owl directly
- [x] Convert `my_ontology.rdf` with `Robot` â†’ Use docker image to run commands, this is easiest to make sure we have all the correct Java versioning. Docker desktop needs to be running for this.
- [x] Try to get the ontology into `Neo4J` â†’ We can get things via the browser. Also nice to use Bloom to view hierarchical representation of the graph.
- [x] Delete all nodes and import ontology via python. This will likely take config then upload.
- [ ] Make the modification to the ontology so environment is on the same level as phenotype and genotype.
- [ ] Depending on this modification act...

## 2023.11.29

- [x] Play with ontology, still unclear to me how I am going to do this.

## 2023.11.28

- [x] Study using Neo4j and with ontologies. â†’ Got to [Going Meta - episode 7](https://www.youtube.com/watch?v=Y_IygO4MOqc&list=PL9Hl4pk2FsvX-5QPvwChB-ni_mFF97rCE&index=7)
- [x] Study tools for building ontology though OBO training tools â†’ UO, the Units of Measurement Ontology. â†’ I have enough now to write a report for getting help with ontology development.

## 2023.11.25

- [x] Check on `Dcell` training â†’ Looks like fitting is better, but it would be nice to speed up training by at least 2x.
- [x] Look at lightning profilers â†’ Using `PytorchProfiler` is the way to go. Found how to schedule its activation so the trace file is not so big.
- [x] Try to run a lightning profiler on `Delta` â†’ This works but instead I have just started using the slimmed version.

## 2023.11.24

- [x] Parameterized `alpha` and `lambda_reg` to increase `lambda_reg` from `0.01` to `1`.
- [x] Launch on `delta`
- [x] Read The Definitive Guide to Graph Databases for the RDBMS Developer

## 2023.11.22

- [x] `Dcell` keeps failing typically after 2 epochs in, makes me think there is some accumulation of data â†’ [[experiments.costanzo_smd_dmf_dcell.dmf_dcell_slim]] Slimmed to stop recording predictions and true values, this is the solution to keep the prrocess memory constant during training. ![](./assets/images/user.Mjvolk3.torchcell.tasks.md.dcell-constant-process-memory-b32-wrks4.png)

## 2023.11.20

- [x] Investigate launched models â†’ There are to many workers `pid` fail
- [x] Model metrics needs to be fixed â†’ logging root predictions and subsystem predictions

## 2023.11.19

- [x] Fix device issues with `Dcell`
- [x] Launch on `DCell`

## 2023.11.18

- [x] Fix [[torchcell.models.dcell]] the modules are not being passed correctly â†’ I think these are fixed but should go back and annotate and test properly.
- [x] Quick run model locally â†’ works on `cpu`
- ðŸ”² Launch on `Delta` â†’ device issues

## 2023.11.17

- [x] Correct [[./assets/drawio/Dcell.drawio.png]] â†’ Corrected including subsystem representation.
- [x] Fix [[torchcell.models.dcell]] the modules are not being passed correctly
- [ ] Check the Yeast OBO for annotations. If it contains annotations check to see if they align with the `DCell` reported annotations counts. Is this the same thing as `GAF`?

## 2023.11.16

- [x] Review GEARS paper [[./assets/drawio/GEARS.drawio.png]]

## 2023.11.14

- [x] Switching plot font to default `DejaVu Sans` since this is platform agnotic.
- [x] Train [experiments.costanzo_smd_dmf_dcell.dmf_dcell] on Delta. â†’ Training both a fitness model an interaction model.
- [x] Recording work with pronto [[torchcell.datasets.pronto_ontology]]
- [ ] Build an ontology that can be used from `smf` and `dmf` [[Ontology|dendron://torchcell/torchcell.datasets.ontology]].

- [ ] Combine `smf` and `dmf` into one dataset. Some work has already been done on this. [[Experiment|dendron://torchcell/torchcell.datasets.experiment]]

## 2023.11.13

- [x] Get [experiments.costanzo_smd_dmf_dcell.dmf_dcell] running.
- [x] Globus transfer of all data â†’ Took 4-5 hours.
- ðŸ”² Train [experiments.costanzo_smd_dmf_dcell.dmf_dcell] on Delta.
- ðŸ”² Combine `smf` and `dmf` into one dataset.

## 2023.11.04

- [x] Get the [[Cell|dendron://torchcell/torchcell.datasets.cell]] working since [[Graph|dendron://torchcell/torchcell.graph.graph]] was recently updated. It is now obvious that we need a well defined interface between `Graphs` and `CellDataset` need unit tests for these. I think we should probably use pydantic data validation. For instance I think that we need to ensure that any graphs specify a graph dictionary. I think this is a better option compared to the multigraph, because this allows us to process only individual graphs, and the current functions for conversion.
- [ ] Build out [[dataset.dcell|dendron://torchcell/torchcell.datasets.dcell]]

## 2023.11.03

- [x] Run some example data through [[Dcell|dendron://torchcell/torchcell.models.dcell]]. This turned out to be much more involved than I previoulsy thought since we have to do so much data transformation prior to passing the data to the model. [[Model Implementation|dendron://torchcell/torchcell.models.dcell#model-implementation]]

- [ ] Build out [[dataset.dcell|dendron://torchcell/torchcell.datasets.dcell]]
- [ ] Build out [[datamodules.dcell|dendron://torchcell/torchcell.datamodules.DCell]] data module. Test loading and a way to vary sampling frequency from different data.
- [ ] Trial bringing together DCell data for the `<6` gene in dataset criterion

## 2023.11.02

- [x] Double check GO annotations with GO file `.gaf` file. â†’ It is terribly annoying to find that they don't overlap and they don't overlap in mysterious ways. Some of the terms from the `gaf` file are not on the GO annotations for the genes.
- [x] Allow for taking the Union between the two sets, and adding the GO information prior to joining. â†’ In [[Adding GO Union from GAF|dendron://torchcell/torchcell.graph.graph#adding-go-union-from-gaf]] I deem this is as currently unnecessary as the contributurion from these terms would be small.
- [x] Build [[Dcell|dendron://torchcell/torchcell.models.dcell]] â†’ prototype

## 2023.11.01

- [x] Since we aren't getting matching graphs using GO annotations processed from the SGD graphs. Double check another source. â†’ GO has a `.gaf` file.

## 2023.10.24

- [x] Double check deletion of `IGI` and forwarding of edges. â†’ [[Test_graph|dendron://torchcell/tests.torchcell.graph.test_graph]]
- [x] Plot histogram of GO dates with minimal possible filtering while maintaining DAG and no isolated nodes. â†’ [[Histogram of DCell GO Gene Annotations Binned in Time|dendron://torchcell/torchcell.datasets.dcell#histogram-of-dcell-go-gene-annotations-binned-in-time]]
- [x] Plot of `DCell` DAG without any isolted nodes, and before filtering and with super node â†’ [[DCell DAG No Isolated Nodes No Filtering with Super Node|dendron://torchcell/torchcell.datasets.dcell#dcell-dag-no-isolated-nodes-no-filtering-with-super-node]]
- [x] Allow for gene removal of annotation according to date with a date arg. â†’ [[DCell Dataset GO Gene Annotations Time Cutoff|dendron://torchcell/torchcell.datasets.dcell#dcell-dataset-go-gene-annotations-time-cutoff]], added tests [[Test_graph|dendron://torchcell/tests.torchcell.graph.test_graph]]
- [x] Check total number of ontology terms with no date args, and with date args. Look for date in `Dcell` paper. â†’ Can only get a rough estimate based on commits and paper submission dates. Even with this we cannot get the number of GO terms to match properly.

- [ ] Build out [[dataset.dcell|dendron://torchcell/torchcell.datasets.dcell]]
- [ ] Build out [[datamodules.dcell|dendron://torchcell/torchcell.datamodules.DCell]] data module. Test loading and a way to vary sampling frequency from different data.
- [ ] Trial bringing together DCell data for the `<6` gene in dataset criterion

- [ ] In any of the graphs Met, GO, Regulatory, Metabolic , PPI, GGI. What is the average degree. For the special graph graphs what is the average number of nodes perturbed. Plot histograms for these.

- [ ] I previously thought that we removed all redundant edges by removing parents with no annotations. If the number of nodes is not the same, check for nodes whose parent node has the exact same gene annotations. I think we consider these nodes redundant. Double check.
- [ ] Build DCell network takes `nx.DiGraph` as arg. Simulate some data

- [ ] Build out DCell Regression task, experiment script, and configuration.
- [ ] Run DCell locally

## 2023.10.23

- [x] Explore metabolism â†’ `/Users/michaelvolk/Documents/projects/yeast-GEM/torchcell_test.py`. This was done in another `yeast-GEM`. It shows that we can represent metabolism with a heterogenous graph of both metabolites and reactions. I will wait off on adding metabolism right now. Need to look into hetero graph data more.
- [x] Graph object, get other relevant networks including GO
- [x] GO graph add DCell options. â†’ Need to double check `IGI`, and the 6 genes criterion nees to be handled after [[DCell|dendron://torchcell/torchcell.datamodules.DCell]]

- [ ] Figure out multigraph with `pyg`. We do not need a custom `Data` object yet for batching data, but we will eventually if we have some heterogenous graph structures. Graph matching case, or bipartite case.
- [ ] Verify graph subsetting. I am thinking there has to be a series of objects that can pass around the standard data. Not sure if this would involve writing a new `Data` type object.
- [ ] Add in single fitness

- [ ] Change `self.genome.go` to `self.genome.go_set` and make it an ordered set.
- [ ] Add method for `available_graphs` that tell you which graph constructor classes have been implemented, and which graphs have already been saved.
- [ ] Implement option to train on fitness and interaction simultaneously. Both work for now, but individuals need to work.

- [ ] Solve issue with `genome.gene_set` vs `genome` had to use `gene_set`. Had to use `gene_set`, but we will likely want to use `genome` for data validation.[[Graph|dendron://torchcell/torchcell.graph.graph]]

- [ ] Embedding overlay plot for `FungalCIS` and `mean expression`
- [ ] switch to `torch_geometric.utils.scatter`

## 2023.10.22

- [x] Check if PPIs do overlap with GGI. â†’ They do to some extent.
- [x] Read on data batching [PyG data batch](https://pytorch-geometric.readthedocs.io/en/latest/advanced/batching.html)
- [x] Regroup
- [x] Workout saving graphs objects, this will greatly reduce dev time â†’ Saving as `json` for now because this avoids some serialization issues. `nx` has removed their `gpickle` methods.
- [x] Graph object add regulatory graph. â†’ `locus1` regulates `locus2` [SGD YER125W](https://www.yeastgenome.org/locus/S000000927/regulation)
- [x] Add GGI
- ðŸ”² Graph object, get other relevant networks including GO, Met.
- ðŸ”² Figure out multigraph with `pyg`. We do not need a custom `Data` object yet for batching data, but we will eventually if we have some heterogenous graph structures. Graph matching case, or bipartite case.
- ðŸ”² Verify graph subsetting. I am thinking there has to be a series of objects that can pass around the standard data. Not sure if this would involve writing a new `Data` type object.
- ðŸ”² Add in single fitness
- ðŸ”² Implement option to train on fitness and interaction simultaneously. Both work for now, but individuals need to work.
- ðŸ”² Solve issue with `genome.gene_set` vs `genome` had to use `gene_set`. Had to use `gene_set`, but we will likely want to use `genome` for data validation.[[Graph|dendron://torchcell/torchcell.graph.graph]]
- ðŸ”² Embedding overlay plot for `FungalCIS` and `mean expression`
- ðŸ”² switch to `torch_geometric.utils.scatter`

## 2023.10.20

- ðŸ”² Read on data batching [PyG data batch](https://pytorch-geometric.readthedocs.io/en/latest/advanced/batching.html)
- ðŸ”² Regroup
- ðŸ”² Graph object, get other relevant networks including PPI, Reg, GGI, GO, Met.
- ðŸ”² Workout saving graphs objects, this will greatly reduce dev time
- ðŸ”² Verify graph subsetting. I am thinking there has to be a series of objects that can pass around the standard data. Not sure if this would involve writing a new `Data` type object. The subset graph procedure that interacts with all data is probably
- ðŸ”² Implement option to train on fitness and interaction simultaneously. Find more general dataset structures for box plots etc... This might be an issue with num_workers instead.

## 2023.10.19

- [x] Models didn't work with one hop neighborhood. Try to zero out the embeddings. â†’ `zero_pert` bool, this doesn't seem to work either. I suspect that the issue is that we are using the embedding of 1 hop neighborhood which can be quite large. We could possibly pull out the embedding of the perturbed nodes themselves, but this would require keeping indices in the data object. After now going through 3-4 modelling designs, I think I the subset of data is the key part. It would be nice to specify a very specific transformations like the `pert_graph`, while building out a key to track `edge_index` and ed,
- [x] Prepare presentation

## 2023.10.18

`funal_up` and `fungal_down` work. `one_hot_gene` and the `prot_T5` datasets don't work

- [x] Something looks strange with the interaction distribution. â†’ 0.04 is about one std away so the genetic interaction score plots show the range of one std.
- [x] In `nx.compose` the second graph will overwrite data if there are duplicate keys. â†’
- [x] Hack to using graphs on perturbation graph â†’ one hop for removed nodes, not yet zeroing out node features. Not sure if it'll be necessary. [[Cell|dendron://torchcell/torchcell.datasets.cell]]
- [x] Hack for e issue with `genome.gene_set` vs `genome` â†’ using `gene_set` as arg [[Graph|dendron://torchcell/torchcell.graph.graph]], when using `genome` things get hung up on `Sanity Checking:`.
- [x] Implement a trainer for graph convolution network â†’ [[Graph_convolution_regression|dendron://torchcell/torchcell.trainers.graph_convolution_regression]]
- [x] Implement option to train on fitness and interaction simultaneously â†’ set up the beginnings of this, but it is going to be a lot of work.
- [x] Issue with `genome.gene_set` vs `genome` might instead be an issue with `num_workers`. When I set `num_workers=0` things run. `num_workers=4` things don't run. Gets hung up at `Sanity Checking:` â†’ `num_workers=2` works but it takes a very long time to get going. `num_workers=4` probably also works
- [x] Launch on `Delta`.
- ðŸ”² Add in single fitness
- ðŸ”² Solve issue with `genome.gene_set` vs `genome` had to use `gene_set`. Had to use `gene_set`, but we will likely want to use `genome` for data validation.[[Graph|dendron://torchcell/torchcell.graph.graph]]
- ðŸ”² Plan graph merge
- ðŸ”² Add single fitness data
- ðŸ”² Embedding overlay plot for `FungalCis` and `mean expression`
- ðŸ”² switch to `torch_geometric.utils.scatter`

## 2023.10.17

- [x] [[455 Proteins in s288c that are geq 1022 Amino Acids|dendron://torchcell/torchcell.models.esm2#455-proteins-in-s288c-that-are-geq-1022-amino-acids]]
- [x] See if T5 Model can handle max protein size. "Here, we trained models that were affected by this limitations (ProtBERT, ProtAlbert, ProtElectra) first on proteins of length 512, then on proteins 1024. Only setting the length of the positional encoding to 40k after pre-training allowed the models to process protein sequences up to a length of 40k. ([Elnaggar et al., 2022, p. 7114](zotero://select/library/items/6FEQEKR6)) ([pdf](zotero://open-pdf/library/items/HMD9SDQ2?page=3&annotation=PTEJIYQC))" â†’ We can embed all proteins with this.
- [x] Figure out how to handle pseudogenes [[6 Pseudogenes From cmd F orf_trans_all_R64-4-1_20230830.fasta|dendron://torchcell/torchcell.sequence.genome.scerevisiae.s288c#6-pseudogenes-from-cmd-f-orf_trans_all_r64-4-1_20230830fasta]]
- [x] Figure out how to identify noncoding â†’ [[Dubious ORFs are Unlikley to Encode an Expressed Protein|dendron://torchcell/torchcell.sequence.genome.scerevisiae.s288c#dubious-orfs-are-unlikley-to-encode-an-expressed-protein]]
- [x] Compute protT5 embeddings â†’ This had to be done on `Delta` because the model is not working for me on cpu currently. [Github Issue](https://github.com/agemagician/ProtTrans/issues/130). More on embeddings - [[Embed all Proteins Can Like 0 Non Expressed Protein|dendron://torchcell/torchcell.datasets.protT5#embed-all-proteins-can-like-0-non-expressed-protein]]
- [x] Check if we can add in protein embeddings. â†’ We used `protein_data` as an attribute, which prevents us from adding embeddings datasets.
- [x] Close `genetic_interaction_score`,`dmf_costanzo_linear` [Wandb_Log](https://wandb.ai/zhao-group/torchcell/groups/2514760_3e78b6beb5b847285ef128c72f402d96b88fbb592e9ecb98a745b43db0a857c1/workspace?workspace=user-mjvolk3). You cannot learn the `genetic_interaction_score` by simply `one_hot` encoding the genes.
- [x] Recompute `ProtT5Dataset` â†’ Had to debug `self.initialize`. There are not some inconsistencies with the embeddings models. I can sort this out later. Added to [[user.mjvolk3.torchcell.tasks.future]]
- [x] Check we can add the `ProtT5Dataset` to the other datasets. â†’ This is works since we are using the `dna_windows` hack. [[ProtT5 Embedding Input and Output|dendron://torchcell/torchcell.models.protT5#prott5-embedding-input-and-output]]
- [x] Fix indexing [[Embedding|dendron://torchcell/torchcell.datasets.embedding]] â†’ now `dataset[0]` and `dataset["Q0010"]` match as they should.
- [x] Compute `"prot_t5_xl_uniref50_all"` and `"prot_t5_xl_uniref50_no_dubious"` and globus transfer. â†’ Changed my mind on making these separate datasets [[Embed all Proteins Can Like 0 Non Expressed Protein|dendron://torchcell/torchcell.datasets.protT5#embed-all-proteins-can-like-0-non-expressed-protein]]
- [x] Add option to combine zero-like out non-expressing proteins denoted as `'dubious'`. â†’ Separate datasets [[Embed all Proteins Can Like 0 Non Expressed Protein|dendron://torchcell/torchcell.datasets.protT5#embed-all-proteins-can-like-0-non-expressed-protein]]
- [x] Fix bug "dubious" to "Dubious" and "uncharacterized" to "Uncharacterized" and recompute and transfer embeddings.
- [x] Test `dmf_costanzo_deepset` with different protein embeddings.
- [x] Launch `dmf_costanzo_deepset` with `no_dubious` protein embeddings.
- [x] Check [[dmf Genetic Interaction Score Mean|dendron://torchcell/torchcell.datasets.scerevisiae.costanzo2016#dmf-genetic-interaction-score-mean]]
- [x] Check `PyG` â†’ [[Graph|dendron://torchcell/torchcell.graph.graph]]
- [x] Create graph object starting with `PPI` and `mean_expression`. Note that since we aren't using the data validation right now, that the structure of each gene is not consistent. No `None` for empty data. We are doing this for two reasons. One we want it for visualization overlay, and the other is for PPI message passing.
- [x] Launch `prot_T5_no_dubious`, `costanzo_1e6

- [ ] Add multigraph to prediction

- [ ] Add double prediction of `fitness` and `gene_interaction_score`
- [ ] Embedding overlay plot for `FungalCis` and `mean expression`
- [ ] Set default font to `Helvetica`

## 2023.10.16

- [x] Make a simple SGD linear model. We use a linear model because the dataset won't fit into memory otherwise.
- [x] Launch linear `fitness` model on Delta. â†’ Probably could use cpu for this.
- [x] Launch linear `genetic_interaction_score` model on Delta.
- [x] Bring in ESM embeddings â†’ Brought in models but not sure if we should bring in embeddings yet since they can't handle 455 protines in that are > 1022 amino acids [[Esm2|dendron://torchcell/torchcell.models.esm2]]

## 2023.10.15

- [x] Launch linear on delta. â†’ Failed due to memory error.
- [x] Make a simple SGD linear model. â†’ started... the issue is with the dataloader, so we could probably append to some file instead of keeping every in memory. Try SGD first.

## 2023.10.14

Big results to report. one hot is best on fitness, can't learn epistatic interactions. Linear  models for fitness on one hot. Add to report

- [x] Set up linear models â†’ [[Dmf_costanzo_linear|dendron://torchcell/experiments.costanzo_smf_dmf_supervised.dmf_costanzo_linear]]
- [ ] DCell model
- [ ] Run full linear model.
- [ ] Add full dataset with `smf` and `dmf`.
- [ ] Joint prediction of fitness and genetic interaction

## 2023.10.13

- [x] Add codon frequency dataset.
- [x] Add one-hot positional encoding dataset, this will make task transductive (check).
- [ ] Train one hot on `1e6` â†’ wandb log we get much better correlation compared to any of the previous models
- [ ] Add cell morphology dataset
- [ ] Add ESM2 dataset

## 2023.10.12

- [x] Rewrite `SmfCostanzoDataset` [[Costanzo2016|dendron://torchcell/torchcell.datasets.scerevisiae.costanzo2016]] to use `Dataset`instead of in memory dataset. This might be unnecessary overkill, but it should allow for inheritance from a generalized `Dataset` class and will make `+` and merges easier. [[Summing vs. Merging Experiment Datasets|dendron://torchcell/torchcell#summing-vs-merging-experiment-datasets]] â†’ converted over, but haven't done any testing.
- [x] Check for duplicates in `SmfCostanzoDataset` â†’ Add many duplicate methods. I thing the `both_` methods might abuse the `duplicate` notation.
- [x] `dmf + smf` dataset. â†’ This creates some issues. I've pursued it some, but the main issue is that another database should be constructed. I think this is the most straightforward way, although it will create duplicate data. This is the easiest solution for now.
- [x] Implement `MergeExperiments` â†’ We had to move away from Dunder add. [[Dunder Adding of Experiments is a Bad Idea|dendron://torchcell/torchcell.datasets.experiment#dunder-adding-of-experiments-is-a-bad-idea]] I took from the individual experiment design and modified it to do simple merges, where are really concatentations. [[Use Dataset Logic but Use Process for Merger Operations|dendron://torchcell/torchcell.datasets.experiment#use-dataset-logic-but-use-process-for-merger-operations]]
- [x] Fix dmf data_list iteration.
- [x] Make box plot for [[Genetic_interaction_score|dendron://torchcell/torchcell.viz.genetic_interaction_score]] â†’ If we want a closer match we could make a few more adjustments.
- [x] Train `1e6` on interaction. â†’ queued priority.

- [ ] Add codon frequency dataset.

- [ ] Add assertion in the addition to make sure that at least the sizes are the same.
- [ ] Read about `pyg` collate to see if all data objects must be same

- [ ] Create new ModelStrict for CodonSelection Result, or use multiple `DnaSelectionResults`. Try multiple `DnaSelectionResults`first

- [ ] Add additional only CDS dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]

- [ ] Add additional `five_prime + partial_CDS + three_prime` dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]

- [ ] Plot Umap overlays with new datasets
- [ ] Optional dimensionality reduction of embeddings

- [ ] Bring the the `Culley` data in properly and correct [[experiments/fitness_expr_data_exploration/smf_ge_box_plot.py]]
- [ ] Need to bring in `SGD` data in properly and correct [[experiments/protein_concentration_nt_projection.py]]

- [ ] Give str `__repr__` to `DnaSelectionResult` like `DnaWindowResult`

- [ ] Change the `FungalUpDown` to `FungalCRE` for fungal cis regulatory element

- [ ] Try dbfn=`â€œ:memory:"` in genome to solve the sqlite remove deprectaed GO issue.
- [ ] Need tex for plotting settings on delta â†’ [conda install texlive-core](https://anaconda.org/conda-forge/texlive-core)

## 2023.10.11

- [x] Cancel run on `upstream + downstream` â†’ [wandb upstream and downstream](https://wandb.ai/zhao-group/torchcell/groups/2498195_6e7d3de812c0282976a4afcf00b099283e3b054397d8d3b967a99dc89cba9fea/workspace?workspace=user-mjvolk3)
- [x] Start run on all `costanzo`
- [x] Fix plotting of box plots to match `Dcell` â†’ ![](./assets/drawio/dcell-boxplot.drawio.png) used template to adjust figures [[dendron://torchcell/torchcell.viz.fitness]] should be publication qualtiy now.
- [x] log wandb artifacts in training.
- [x] Check if 'Arial' is on `Delta`. â†’ `conda install -c conda-forge mscorefonts`
- [x] Start run on all `costanzo`, with artifact logging and new box plots.
- ðŸ”² Add assertion in the addition to make sure that at least the sizes are the same.

## 2023.10.10

- [x] Launch experiment with all `FungalCRE` features.
- [x] Check if pytorch collate works if the Data Objects are different. This is important for eventually combining datasets. â†’ It looks like add only works if all objects have the same data types and sizes, but need to look into more
- ðŸ”² Add assertion in the addition to make sure that at least the sizes are the same.
- ðŸ”² Create new ModelStrict for CodonSelection Result, or use multiple `DnaSelectionResults`. Try multiple `DnaSelectionResults`first
- ðŸ”² Add codon frequency dataset.
- ðŸ”² Add additional only CDS dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]
- ðŸ”² Add additional `five_prime + partial_CDS + three_prime` dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]
- ðŸ”² Summarize the setting under which models can be successfully trained, or rather where training can at least be started. Create table.
- ðŸ”² Plot Umap overlays with new datasets
- ðŸ”² Optional dimensionality reduction of embeddings
- ðŸ”² Bring the the `Culley` data in properly and correct [[experiments/fitness_expr_data_exploration/smf_ge_box_plot.py]]
- ðŸ”² Need to bring in `SGD` data in properly and correct [[experiments/protein_concentration_nt_projection.py]]
- ðŸ”² Give str `__repr__` to `DnaSelectionResult` like `DnaWindowResult`
- ðŸ”² Test speed of nucleotide transformer speed up.
- ðŸ”² Change the `FungalUpDown` to `FungalCRE` for fungal cis regulatory element
- ðŸ”² Try dbfn=`â€œ:memory:"` in genome to solve the sqlite remove deprectaed GO issue.
- ðŸ”² Need tex for plotting settings on delta â†’ [conda install texlive-core](https://anaconda.org/conda-forge/texlive-core)

## 2023.10.09

- [x] Launch experiment with just upstream features and add to [[dmf_costanzo_deepset.results.01|dendron://torchcell/experiments.dmf_costanzo_deepset.results.01]] â†’ Delta login is currently down.
- [x] Write report for @Yunan-Luo â†’ [[results.01.report|dendron://torchcell/experiments.dmf_costanzo_deepset.results.01.report]]
- [x] Send report to @Yunan-Luo
- [x] Rerun embeddings on full `s288c`genome. This allows for the proper collation when combining datasets with add. â†’ This can lead to some pernicious bugs, where you don't recognize that the dataset is smaller and if the designed net always adapts to feature size you never catch this bug. [[Perturbation Nodes Benchmark Model - Adding Bug|dendron://torchcell/experiments.dmf_costanzo_deepset.results.01.report#perturbation-nodes-benchmark-model---adding-bug]]. Might want to consider adding some sort of assertion in the addition to make sure that at least the sizes are the same.
- [x] Configure type of dataset to train on. â†’ got paritally there but it is difficult to add datasets with `sum` â†’ [[Difficult to Add Datasets with Sum operator|dendron://torchcell/torchcell.datasets.embedding#difficult-to-add-datasets-with-sum-operator]]

## 2023.10.08

- [x] On train wt try to overfit the wt before moving to train batch. â†’ Tried this but doesn't seem to work very well. There are a lot of instabilities.
- ðŸ”² Create new ModelStrict for CodonSelection Result, or use multiple `DnaSelectionResults`. Try multiple `DnaSelectionResults`first
- ðŸ”² Add codon frequency dataset.
- ðŸ”² Add additional only CDS dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]
- ðŸ”² Add additional `five_prime + partial_CDS + three_prime` dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]
- ðŸ”² Summarize the setting under which models can be successfully trained, or rather where training can at least be started. Create table.
- ðŸ”² Plot Umap overlays with new datasets
- ðŸ”² Optional dimensionality reduction of embeddings
- ðŸ”² Bring the the `Culley` data in properly and correct [[experiments/fitness_expr_data_exploration/smf_ge_box_plot.py]]
- ðŸ”² Need to bring in `SGD` data in properly and correct [[experiments/protein_concentration_nt_projection.py]]
- ðŸ”² Give str `__repr__` to `DnaSelectionResult` like `DnaWindowResult`
- ðŸ”² Test speed of nucleotide transformer speed up.
- ðŸ”² Change the `FungalUpDown` to `FungalCRE` for fungal cis regulatory element
- ðŸ”² Try dbfn=`â€œ:memory:"` in genome to solve the sqlite remove deprectaed GO issue.
- ðŸ”² Need tex for plotting settings on delta â†’ [conda install texlive-core](https://anaconda.org/conda-forge/texlive-core)

## 2023.10.07

- [x] Summarize some of the successful run. [[dmf_costanzo_deepset.results.01|dendron://torchcell/experiments.dmf_costanzo_deepset.results.01]]
- [x] Add codon frequency dataset â†’ This is a bit more difficult than I anticipated since we have codon frequency of gene with intron and codon frequency of mRNA. â†’ paritally finished.
- ðŸ”² Add additional only CDS dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]
- ðŸ”² Add additional `five_prime + partial_CDS + three_prime` dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]
- ðŸ”² Summarize the setting under which models can be successfully trained, or rather where training can at least be started. Create table.
- ðŸ”² Plot Umap overlays with new datasets
- ðŸ”² Optional dimensionality reduction of embeddings
- ðŸ”² Bring the the `Culley` data in properly and correct [[experiments/fitness_expr_data_exploration/smf_ge_box_plot.py]]
- ðŸ”² Need to bring in `SGD` data in properly and correct [[experiments/protein_concentration_nt_projection.py]]
- ðŸ”² Give str `__repr__` to `DnaSelectionResult` like `DnaWindowResult`
- ðŸ”² Test speed of nucleotide transformer speed up.
- ðŸ”² Change the `FungalUpDown` to `FungalCRE` for fungal cis regulatory element
- ðŸ”² Try dbfn=`â€œ:memory:"` in genome to solve the sqlite remove deprectaed GO issue.
- ðŸ”² Need tex for plotting settings on delta â†’ [conda install texlive-core](https://anaconda.org/conda-forge/texlive-core)

## 2023.10.06

- [x] Try training on `x_pert`. Allow for easy switching between `x` and `x_pert`.
- [x] Launch experiment on Delta for `x_pert`

## 2023.10.02

- [x] Implement `LayerNorm` normalization and activation parameterization
- [x] Implement gradient clipping. â†’ No implementation needed. This is easy on pytorch lightning as it can be done by simply passing args to the `Trainer`
- [x] Add option to include skip connections in instance layers and set layers of [[torchcell.models.deep_set]]. Of course skip connections can only be applied to layers of the same dimension so the boolean corresponds to layers with repeat of the same dimension.
- [x] Automatic clipping only works if we don't use manual backward... [[Manual Backward with Forcing Node Wt to Ones|dendron://torchcell/torchcell.trainers.regression#manual-backward-with-forcing-node-wt-to-ones]] â†’ This along with layer norm has appeared to smooth out the loss.
- [x] Implement set transformer â†’ [[Deep_set_transformer|dendron://torchcell/torchcell.models.deep_set_transformer]]
- [x] Add weighted MSE in case the models don't learn distribution but instead just predict the mean. â†’ [[Weighted_mse|dendron://torchcell/torchcell.losses.weighted_mse]]
- ðŸ”² Create a sweep locally for `1e4` data.

## 2023.09.29

- [x] Send query to @Yunan-Luo about [[Training Instability with Wildtype Embedding Difference|dendron://torchcell/torchcell.trainers.regression#training-instability-with-wildtype-embedding-difference]]
- [x] Kill run â†’ [Wandb Run](https://wandb.ai/zhao-group/torchcell/groups/2459252_ad9b6cf8e9b4acd6438053d0ff7a6d814888f8e2931913741695b28cdffa1030/workspace?workspace=user-mjvolk3), some sides notes on this run [[Training Instability with Wildtype Embedding Difference|dendron://torchcell/torchcell.trainers.regression#training-instability-with-wildtype-embedding-difference]]
- [x] [[Fixing Padding on Upstream Model to Match GitHub Issue Recommendation|dendron://torchcell/torchcell.models.fungal_up_down_transformer#fixing-padding-on-upstream-model-to-match-github-issue-recommendation]]
- [x] Delete old upstream embeddings and rerun [[Fungal_up_down_transformer|dendron://torchcell/torchcell.datasets.fungal_up_down_transformer]]. â†’ Tried running but there is an issue [[Parse Genome and Return None For Dunder Add - Need Genome in Process|dendron://torchcell/torchcell.datasets.fungal_up_down_transformer#parse-genome-and-return-none-for-dunder-add---need-genome-in-process]]
- [x] Try to regularize by forcing `wt` embedding to 1, can also try 0 which should be fine because you can get to 1 with bias from linear.

## 2023.09.28

- [x] Unify `wandb` when training on multiple gpus previous is slurm job id and date. Don't think this will work across sweeps. Add agent id. â†’ Agent id is not accessible. Instead I chose to use a hashing of the config. I also got rid of date because if the launch on different gpus happens right around midnight this could create some error. The Hashing method should be unique to the model config.
- [x] Track value of `WT` embedding.
- [x] Launch experiments on [[Delta|dendron://Kbase/computer.delta]]
- [x] `DDP` is with genome `sqlite` â†’ First tracked this error [[2023.09.09|dendron://torchcell/user.Mjvolk3.torchcell.tasks#20230909]], made some proper documentation [[DDP sqlite gff error|dendron://torchcell/torchcell.sequence.genome.scerevisiae.s288c#ddp-sqlite-gff-error]] â†’ I still have not confirmed if this is the issue because there was another issue related to combining data
- [x] Fix issue related to combining data. â†’ There is some dark magic ðŸª„ that I am unfamiliar with here, but I found a workable solution for now. [[Parse Genome and Return None For Dunder Add|dendron://torchcell/torchcell.datasets.fungal_up_down_transformer#parse-genome-and-return-none-for-dunder-add]]
- [x] Configure `weight_decay` and `learning_rate`
- [x] Launch [Wandb - 1e5 Deep Set DDP Find Unused](https://wandb.ai/zhao-group/torchcell/groups/2459252_ad9b6cf8e9b4acd6438053d0ff7a6d814888f8e2931913741695b28cdffa1030/workspace?workspace=user-mjvolk3)
- [x] Try to fix forced ddp find unused by passing two `wt` instances and only computing the loss for one. We lose significant speedups according to [GitHub Issue](https://github.com/Lightning-AI/lightning/issues/17212) [[Deep Set Model Only Works with DDP Find Unused|dendron://torchcell/experiments.costanzo_smf_dmf_supervised.dmf_costanzo_deepset_1e5#deep-set-model-only-works-with-ddp-find-unused]] â†’ This works, using a batch of `wt` [[Deep Set Model Only Works with DDP Find Unused - Solution|dendron://torchcell/experiments.costanzo_smf_dmf_supervised.dmf_costanzo_deepset_1e5#deep-set-model-only-works-with-ddp-find-unused---solution]]
- [x] Write set transformer model â†’ Threw something quick together [[Regression_deep_set_transformer|dendron://torchcell/torchcell.trainers.regression_deep_set_transformer]]
- ðŸ”² Add codon frequency dataset â†’ This is a bit more difficult than I anticipated since we have codon frequency of gene with intron and codon frequency of mRNA.
- ðŸ”² Need tex for plotting settings on delta â†’ [conda install texlive-core](https://anaconda.org/conda-forge/texlive-core)
- ðŸ”² Add additional only CDS dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]
- ðŸ”² Add additional `five_prime + partial_CDS + three_prime` dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]
- ðŸ”² Summarize the setting under which models can be successfully trained, or rather where training can at least be started. Create table.
- ðŸ”² Plot Umap overlays with new datasets
- ðŸ”² Optional dimensionality reduction of embeddings
- ðŸ”² Bring the the `Culley` data in properly and correct [[experiments/fitness_expr_data_exploration/smf_ge_box_plot.py]]
- ðŸ”² Need to bring in `SGD` data in properly and correct [[experiments/protein_concentration_nt_projection.py]]
- ðŸ”² Give str `__repr__` to `DnaSelectionResult` like `DnaWindowResult`
- ðŸ”² Test speed of nucleotide transformer speed up.

## 2023.09.27

- [x] Respose to [Github Issue Selecting Upstream Sequence](https://github.com/gagneurlab/SpeciesLM/issues/2) â†’ [[How input_ids_len Changes with Different Sequences|dendron://torchcell/torchcell.models.fungal_up_down_transformer#how-input_ids_len-changes-with-different-sequences]], [[Window Five Prime S288c Genes Less Than 1003 bp|dendron://torchcell/torchcell.sequence.genome.scerevisiae.s288c#window-five-prime-s288c-genes-less-than-1003-bp]], [[Window Three Prime S288c Genes Less Than 300 bp|dendron://torchcell/torchcell.sequence.genome.scerevisiae.s288c#window-three-prime-s288c-genes-less-than-300-bp]]
- [x] [[Test_fungal_up_down_transformer|dendron://torchcell/tests.torchcell.models.test_fungal_up_down_transformer]]
- [x] Document [[Fungal_up_down_transformer|dendron://torchcell/torchcell.models.fungal_up_down_transformer]] â†’ Added some documentation and made docs. Still difficult for me to tell how the raw strings will render using sphinx.
- [x] Generate new embeddings using upstream model change.
- [x] Think more on label adding â†’ [[Adding Fitness Labels|dendron://torchcell/torchcell.datasets.scerevisiae.costanzo2016#adding-fitness-labels]] using a temporary solution for now to handle multiple different cases `dmf`, 'fitness', and mapping them all to `fitness`.
- [x] Implement wt difference embedding â†’ This has gotten a bit complicated, I am often running into this error `Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed)` when trying to embed the wild type and instances from the batch. I overcame this with explicityly handling the optimization. â†’ [[Explicitly Optimising the training_step and train_wt|dendron://torchcell/torchcell.trainers.regression#explicitly-optimising-the-training_step-and-train_wt]]
- [x] Change [[torchcell.models.deep_set]] to have only dropout on last layer - [[Dropout on Last Layer Only|dendron://torchcell/torchcell.models.deep_set#dropout-on-last-layer-only]]
- [x] Glbous transfer `cell_1e4`, `cell_1e5`, `costanzo2016_1e4`, `costanzo2016_1e5`. â†’  running `cell` and `costanzo` for later transfer.
- [x] Review [Github issue](https://github.com/gagneurlab/SpeciesLM/issues/2#issuecomment-1737756856) â†’ It appears there is nothing to do about sequences like this. ðŸš‚ Moving on .
- [x] Prepare trainer and run models locally for `FungalUpDownTransformer`
- [x] Run experiments locally
- ðŸ”² Unify `wandb` when training on multiple gpus previous is slurm job id and date. Don't this will work across sweeps. Add period delimited time or something else.
- ðŸ”² Launch experiments on [[Delta|dendron://Kbase/computer.delta]]
- ðŸ”² Add additional only CDS dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]
- ðŸ”² Add additional `five_prime + partial_CDS + three_prime` dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]
- ðŸ”² Summarize the setting under which models can be successfully trained, or rather where training can at least be started. Create table.
- ðŸ”² Write set transformer model
- ðŸ”² Plot Umap overlays with new datasets
- ðŸ”² Optional dimensionality reduction of embeddings
- ðŸ”² Bring the the `Culley` data in properly and correct [[experiments/fitness_expr_data_exploration/smf_ge_box_plot.py]]
- ðŸ”² Need to bring in `SGD` data in properly and correct [[experiments/protein_concentration_nt_projection.py]]
- ðŸ”² Give str `__repr__` to `DnaSelectionResult` like `DnaWindowResult`
- ðŸ”² Test speed of nucleotide transformer speed up.

## 2023.09.26

- [x] Downselect by `costanzo` gene interaction scores or `1e5` â†’ Tabling this for now. This would bias training. Unseen data would then be expected to have interactions, when they were specificially selected to not have an interaction. Also this doesn't make much sense. It would make more sense to takes the `abs` then select upper quantiles of high interaction scores, dropping low interaction. â†’ [[DmfCostanzo2016Dataset Genetic Interaction Score Histogram|dendron://torchcell/torchcell.datasets.scerevisiae.costanzo2016#dmfcostanzo2016dataset-genetic-interaction-score-histogram]]
- [x] Check how many genes now are outside of the [[nucleotide_transformer|torchcell.datasets.nucleotide_transformer]] window. â†’ [[Genes Larger than Nucleotide Transformer Window|dendron://torchcell/torchcell.datasets.nucleotide_transformer#genes-larger-than-nucleotide-transformer-window]]
- [x] Plot `Dmf` fitness â†’ [[DmfCostanzo2016Dataset Double Mutant Fitness Score Histogram|dendron://torchcell/torchcell.datasets.scerevisiae.costanzo2016#dmfcostanzo2016dataset-double-mutant-fitness-score-histogram]]
- [x] Check dna transformers are using `SortedSet`
- [x] Check we can index on gene name in torch datasets. `dataset[0]`, `dataset["YDR210W"]` â†’  This only makes sense for more generic types of datasets like embeddings datasets, [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]. We cannot do this for things like `DmfCostanzo2016Dataset` becuase there are two genes being removed and we cannot index on both.
- [x] Implement wildtype property â†’ [[Wildtype Property|dendron://torchcell/torchcell.datasets.scerevisiae.costanzo2016#wildtype-property]]
- [x] Implement wt difference embedding â†’ Got decently far writing a [[torchcell.models.mlp]] so this can be used on top of aggegated embeddings from `wt` and instance.
- ðŸ”² Prepare trainer and run models locally for `FungalUpDownTransformer`
- ðŸ”² Change [[Dcell|dendron://torchcell/torchcell.models.dcell]] to have only dropout on last layer - `zendron_citation`
- ðŸ”² Run experiments locally
- ðŸ”² Unify `wandb` when training on multiple gpus previous is slurm job id and date. Don't this will work across sweeps. Add period delimited time or something else.
- ðŸ”² Add additional only CDS dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]
- ðŸ”² Add additional `five_prime + partial_CDS + three_prime` dataset [[Nucleotide_transformer|dendron://torchcell/torchcell.datasets.nucleotide_transformer]]
- ðŸ”² Summarize the setting under which models can be successfully trained, or rather where training can at least be started. Create table.
- ðŸ”² Launch experiments on [[Delta|dendron://Kbase/computer.delta]]
- ðŸ”² Write set transformer model
- ðŸ”² Plot Umap overlays with new datasets
- ðŸ”² Optional dimensionality reduction of embeddings
- ðŸ”² Bring the the `Culley` data in properly and correct [[experiments/fitness_expr_data_exploration/smf_ge_box_plot.py]]
- ðŸ”² Need to bring in `SGD` data in properly and correct [[experiments/protein_concentration_nt_projection.py]]
- ðŸ”² Give str `__repr__` to `DnaSelectionResult` like `DnaWindowResult`
- ðŸ”² Test speed of nucleotide transformer speed up.

## 2023.09.25

- [x] GitHub issue response since used incorrect query `dataset_five_prime.loc[dataset[dataset['gene_id'] == "YDL061C"].index.tolist()]` â†’ [Github Issue Selecting Upstream Sequence](https://github.com/gagneurlab/SpeciesLM/issues/2)
- [x] GitHub issue response. There are genes that don't have 1003 `bp` upstream because they are against a chromosome. â†’ [Github Issue Upstream less than 1003 bp](https://github.com/gagneurlab/SpeciesLM/issues/1)
- [x] In plotting we have some `dmf` data that has only one perturbation on the gene set. fix. â†’ changed `any()` to `all()`
- ðŸ”² Check dna transformers are using `SortedSet`
- ðŸ”² Check we can index on gene name in torch datasets. `dataset[0]`, `dataset["YDR210W"]`
- ðŸ”² Summarize the setting under which models can be successfully trained, or rather where training can at least be started. Create table.
- ðŸ”² Downselect by `costanzo` gene interaction scores or `1e5`
- ðŸ”² Prepare trainer and run models locally for `FungalUtrTransformer`
- ðŸ”² Change [[Dcell|dendron://torchcell/torchcell.models.dcell]] to have only dropout on last layer - `zendron_citation`
- ðŸ”² Implement wt difference embedding
- ðŸ”² Run experiments locally
- ðŸ”² Unify `wandb` when training on multiple gpus previous is slurm job id and date. Don't this will work across sweeps. Add period delimited time or something else.
- ðŸ”² Launch experiments on [[Delta|dendron://Kbase/computer.delta]]
- ðŸ”² Write set transformer model
- ðŸ”² Plot Umap overlays with new datasets
- ðŸ”² Optional dimensionality reduction of embeddings
- ðŸ”² Bring the the `Culley` data in properly and correct [[experiments/fitness_expr_data_exploration/smf_ge_box_plot.py]]
- ðŸ”² Need to bring in `SGD` data in properly and correct [[experiments/protein_concentration_nt_projection.py]]
- ðŸ”² Give str `__repr__` to `DnaSelectionResult` like `DnaWindowResult`
- ðŸ”² Test speed of nucleotide transformer speed up.

## 2023.09.23

- [x] Test genome [[torchcell.sequence.genome.scerevisiae.s288c]] â†’ [[Gene class looks more like ORF|dendron://torchcell/torchcell.sequence.genome.scerevisiae.s288c#gene-class-looks-more-like-orf]], forgot about 1 bp shift, this was messing up all windows [[Selecting Gene Sequence - Adjust -1 on Start for Negative Sequence|dendron://torchcell/torchcell.sequence.genome.scerevisiae.s288c#selecting-gene-sequence---adjust--1-on-start-for-negative-sequence]]. We still report the start and end as they are reported in `gff` so the length of seq is `1bp` longer than `end-start`.
- [x] Write fungal utr model â†’ done but there are still some issues with deciding how to pad the upstream sequence. [[ModelUsage.py Padding for Upstream Models|dendron://torchcell/torchcell.models.fungal_up_down_transformer#modelusagepy-padding-for-upstream-models]]
- [x] Email `SpeciesLM` Model authors about this [[ModelUsage.py Padding for Upstream Models|dendron://torchcell/torchcell.models.fungal_up_down_transformer#modelusagepy-padding-for-upstream-models]]
- [x] Write fungal utr dataset â†’ [[Model Variants Support|dendron://torchcell/torchcell.datasets.fungal_up_down_transformer#model-variants-support]]
- [x] Recompute `nt dataset` with SortedSet and fixed windows
- [x] Compute `FungalUtrTransformerDataset`
- ðŸ”² In plotting we have some `dmf` data that has only one perturbation on the gene set. fix.
- ðŸ”² Make sure dna transformers are using `SortedSet`
- ðŸ”² Check we can index on gene name in torch datasets. `dataset[0]`, `dataset["YDR210W"]`
- ðŸ”² Summarize the setting under which models can be successfully trained, or rather where training can at least be started. Create table.
- ðŸ”² Downselect by `costanzo` gene interaction scores or `1e5`
- ðŸ”² Prepare trainer and run models locally for `FungalUtrTransformer`
- ðŸ”² Change [[Dcell|dendron://torchcell/torchcell.models.dcell]] to have only dropout on last layer - `zendron_citation`
- ðŸ”² Implement wt difference embedding
- ðŸ”² Run experiments locally
- ðŸ”² Unify `wandb` when training on multiple gpus previous is slurm job id and date. Don't this will work across sweeps. Add period delimited time or something else.
- ðŸ”² Launch experiments on [[Delta|dendron://Kbase/computer.delta]]
- ðŸ”² Write set transformer model
- ðŸ”² Plot Umap overlays with new datasets
- ðŸ”² Optional dimensionality reduction of embeddings
- ðŸ”² Bring the the `Culley` data in properly and correct [[experiments/fitness_expr_data_exploration/smf_ge_box_plot.py]]
- ðŸ”² Need to bring in `SGD` data in properly and correct [[experiments/protein_concentration_nt_projection.py]]

## 2023.09.22

- [x] Make docs â†’ Changed to `pyg` template as it is simpler
- [x] Make  `pytest --cov=torchcell --cov-report html tests/` task â†’ have to use full paths to executables.
- [x] Check non-coding exons `noncoding_exon` features to see if they can imply `3'utr`. There are no `exons` in the sgd `.gff` â†’ [[Using the NCBI s288c we cannot compute UTR lengths|dendron://torchcell/torchcell.sequence.genome.scerevisiae.s288c_ncbi#using-the-ncbi-s288c-we-cannot-compute-utr-lengths]]
- ðŸ”² Test genome [[torchcell.sequence.genome.scerevisiae.s288c]]

## 2023.09.21

- [x] Complete coverage on [[Data|dendron://torchcell/torchcell.sequence.data]]
- ðŸ”² Check non-coding exons `noncoding_exon` features to see if they can imply `3'utr`

## 2023.09.20

- [x] Email Fungal UTR authors to clarify some of the 5 utr selections
- [x] The genbank files `.gbff` contain all information in one file, which could be streamline for creating the genome class. See if it is worth converting to genbank files. â†’ I explored [[GeneBank for Constructing Genome|dendron://torchcell/torchcell.sequence.genome.scerevisiae.s288c_gb#genebank-for-constructing-genome]] and decided against it for now. We show that any missing information can be recovered from various APIs [[GFF File Does not Contain EC Number|dendron://torchcell/torchcell.multidigraph.uniprot_api_ec#gff-file-does-not-contain-ec-number]]
- [x] Complete coverage on data â†’ [[Data|dendron://torchcell/torchcell.sequence.data]] made some progress
- [x] Previous task was to organize Umap visualization overlays. Now delete since these are now invalid.
- ðŸ”² Make  `pytest --cov=torchcell --cov-report html tests/` task
- ðŸ”² Test genome
- ðŸ”² Write fungal utr model
- ðŸ”² Write fungal utr dataset
- ðŸ”² In plotting we have some `dmf` data that has only one perturbation on the gene set. fix.
- ðŸ”² Make sure dna transformers are using `SortedSet`
- ðŸ”² Recompute `nt dataset` with SortedSet and fixed windows
- ðŸ”² Compute `f-utr-t dataset`
- ðŸ”² Check we can index on gene name in torch datasets. `dataset[0]`, `dataset["YDR210W"]`
- ðŸ”² Summarize the setting under which models can be successfully trained, or rather where training can at least be started. Create table.
- ðŸ”² Plot Umap overlays with new datasets
- ðŸ”² Implement wt difference embedding
- ðŸ”² Optional dimensionality reduction of embeddings
- ðŸ”² Downselect by `costanzo` gene interaction scores or `1e5`
- ðŸ”² Unify `wandb` when training on multiple gpus previous is slurm job id and date. Don't this will work across sweeps. Add period delimited time or something else.
- ðŸ”² Bring the the `Culley` data in properly and correct [[experiments/fitness_expr_data_exploration/smf_ge_box_plot.py]]
- ðŸ”² Need to bring in `SGD` data in properly and correct [[experiments/protein_concentration_nt_projection.py]]
- ðŸ”² Change [[Dcell|dendron://torchcell/torchcell.models.dcell]] to have only dropout on last layer - `zendron_citation`

## 2023.09.19

- [x] Clarify notes on [[Selecting Gene Sequence|dendron://torchcell/torchcell.sequence.genome.scerevisiae.s288c#selecting-gene-sequence]]
- [x] Add protein to `Genome` â†’ [[Adding Protein to Genome|dendron://torchcell/torchcell.sequence.genome.scerevisiae.s288c#adding-protein-to-genome]]
- [x] Resolve start and stop codon issues â†’ [[Selecting Gene Sequence|dendron://torchcell/torchcell.sequence.genome.scerevisiae.s288c#selecting-gene-sequence]]
- [x] There is a very annoying issue where the gff annoation  `self.seq = str(self.fasta_dna[chr].seq[self.start - 1 : self.end])`. The issue is that gff files are genearlly 1-indexed. â†’ I think we are able to mostly bypass having to rewrite the `calculate_window` functions in [[Data|dendron://torchcell/torchcell.sequence.data]] since we can just pass in `self.start-1`

## 2023.09.18

- [x] Looks like we are a base pair short when selecting DNA sequences. This is why we don't have stop codons in the last `3bp`. Fix. â†’ fixed with `1bp` shift when selecting from fasta sequences but will need to change this for window selection.
- [x] Change `window_5utr`, and `window_3utr` to `window_upstream` and `window_downstream` since the sequences in question are not isomorophic to `utr`, `upstream` and `downstream` are more accurate. â†’ changed instead to `window_five_prime`, and `window_three_prime`, since downstream sounds like it the absolute coordinates of genome ignoring `+` or `-` strand. This naming scheme tells us relevant information relative to `CDS`.
- [x] Adjust `1bp` for window selection, and 5utr and 3utr selection. â†’ `1bp` shift
- [x] I've found a bug in the previous window selection where we were not taking the reverse complement of the strand for the `window()` method. This invalidates the visualizations and models that have been used `nt_embeddings`. â†’ I think it's fixed
- [x] Write fungal utr model â†’ There is a discrepancy between fungal utr model instructions and the model itself. They say to select the stop codon and the `297bp` after but they never actually check for any stop codon. [ModelUsage.py GitHub](https://github.com/gagneurlab/SpeciesLM/blob/main/ModelUsage.ipynb) â†’ I think I have a decent grasp now on using the model after playing with the notebook.
- ðŸ”² Write fungal utr dataset
- ðŸ”² Fungal UTR authors to clarify some of the 5 utr

## 2023.09.17

- [x] Download updated fungal UTR transformer. â†’ Got things to work in their jupyter notebooks, but it is not as straight forward as the `nt_transformer`. [[Tokenizing Data Procedure Taken from ModelUsage.py|dendron://torchcell/torchcell.models.fungal_up_down_transformer#tokenizing-data-procedure-taken-from-modelusagepy]]
- ðŸ”² In plotting we have some `dmf` data that has only one perturbation on the gene set. fix.
- ðŸ”² Recompute `nt dataset` with SortedSet

## 2023.09.15

- [x] We have an issue where where if `drop_mt` isn't changing data. fix this. I found that `drop_mt` and `drop_empty_go` aren't reducing `gene_set` size. This might have to do with the reading and writing of the `db`. This is indeed the case. I am not sure if this is the behavior we want. We should try to go back to always writing the `db`, since I think the `sqlite` errors were due to not removing the `db` then double check `ddp`. I think better behavior is to start with the fresh genome each time. â†’ changed back but haven't tested.
- [x] Make sqlite db removal less hacky and change the `CellDataset` arg to take `genome` again. â†’  [[Genome Sqlite DB Removal For Dataset Pickling|dendron://torchcell/torchcell.datasets.cell#genome-sqlite-db-removal-for-dataset-pickling]]. I've also added a `GeneSet` object to enfoce `SortedSet[str]` for gene sets.
- ðŸ”² In plotting we have some `dmf` data that has only one perturbation on the gene set. fix.
- ðŸ”² Recompute `nt dataset` with SortedSet
- ðŸ”² Organize Umap visualization overlays
- ðŸ”² Summarize the setting under which models can be successfully trained, or rather where training can at least be started. Create table.
- ðŸ”² Make sure dna transformers are using `SortedSet`
- ðŸ”² Change [[torchcell/models/dcell.py]] to have only dropout on last layer - `zendron_citation`
- ðŸ”² Rerun [[torchcell/datasets/nucleotide_transformer.py]] to comply with `SortedSet`
- ðŸ”² wt difference embedding
- ðŸ”² optional dimensionality reduction
- ðŸ”² Downselect by gene interaction scores or `1e5`...
- ðŸ”² Unify `wandb` when training on multiple gpus previous is slurm job id and date. Don't this will work across sweeps. Add period delimited time or something else.
- ðŸ”² Scale up model training
- ðŸ”² Bring the the `Culley` data in properly and correct [[ experiments/fitness_expr_data_exploration/smf_ge_box_plot.py]]
- ðŸ”² Need to bring in `SGD` data in properly and correct [[experiments/protein_concentration_nt_projection.py]]

## 2023.09.14

- ðŸ”² Change [[torchcell/models/dcell.py]] to have only dropout on last layer - `zendron_citation`
- ðŸ”² Unify `wandb` when training on multiple gpus previous is slurm job id and date. Don't this will work across sweeps. Add period delimited time or something else.
- ðŸ”² Scale up model training
- ðŸ”² Launch job.

## 2023.09.13

- ðŸ”² In plotting we have some `dmf` data that has only one perturbation on the gene set. fix.

## 2023.09.12

- [x] Figure out why `Delta` batch job fails â†’ [Jira - Delta Batch Job Failing](https://jira.ncsa.illinois.edu/browse/DELTA-2412) â†’ `Delta` should be Thursday 10 pm.
- [x] git clean up across computers
- [x] See if `Dcell` number of layers idea checks out (50 min) â†’
- [x] Add removed vectors to data object
- [x] Add dataframe cached property to datasets â†’ [[DMF stats dmf and dmf_std with low_dmf_std preprocess|dendron://torchcell/experiments.dmf_costanzo_deepset#dmf-stats-dmf-and-dmf_std-with-low_dmf_std-preprocess]]
- ðŸ”² visualize the dependency of the library (10 min) â†’ moved to [[user.mjvolk3.torchcell.tasks.future]]
- [x] reorganize task list around
- [x] UMAP visualization with `dmf` overlay â†’ Organize results.
- ðŸ”² Summarize the setting under which models can be successfully trained, or rather where training can at least be started.
- ðŸ”² Make sqlite db removal less hacky
- ðŸ”² wt difference embedding
- ðŸ”² optional dimensionality reduction
- ðŸ”² Downselect by gene interaction scores or `1e5`...

## 2023.09.11

- [x] Investigate why the previous `dmf` `1e6` failed. â†’ Ran out of gpu memory. Memory increased gpu process memory increased epochs... my suspicion is that tracking model gradients leads to this, but I am unsure. Trying again without gradient tracking. â†’ Now that I think this doesn't make much sense since I was able to track weights for large models. Maybe it has something to do with size of data?
- [x] Review system metrics â†’ Still unsure why some models fail with large gpu memory allocation still available. We get a gpu shut down message, but I think it has to do with running out of cpu memory on the node. Also it looks like the amount of cpu memory used for `num_workers` is much higher than the amount of allocated memory on gpu. Clearly there are gaps in understanding the allocation. Some anecdotal evidence [[2 gpus A40x4-sbatch - Training Speedup with 1e5 CellDataset cont. 1|dendron://torchcell/experiments.dmf_costanzo_deepset#2-gpus-a40x4-sbatch---training-speedup-with-1e5-celldataset-cont-1]]
- ðŸ”² git clean up across computers
- ðŸ”² wt difference embedding
- ðŸ”² optional dimensionality reduction
- ðŸ”² Downselect by gene interaction scores or `1e5`...
- ðŸ”² Unify `wandb` when training on multiple gpus previous is slurm job id and date. Don't this will work across sweeps.
- ðŸ”² add period delimited time

## 2023.09.10

- [x] Add job id and date stamp to the grouped runs â†’ cannot easily add date.
- [x] Find smaller model that can avoid overfitting â†’ Training smaller model, but haven't found one that can avoid overfitting.
- [x] Unify `wandb` when training on multiple gpus. â†’ slurm job id and date
- ðŸ”² Try dimensionality reduction
- ðŸ”² Downselect by gene interaction score for `1e5`

## 2023.09.09

- [x] Fix genome sqlite database issue â†’ checking if database already exists and reading in fixes issue with `ddp` over multiple gpus
- [x] Find max slurm `--mem` for A40x4 â†’ `SBATCH --mem=243g`... this must be because there is some overhead somewhere. [ncsa delta A40x4](https://wiki.ncsa.illinois.edu/display/DSC/Delta+User+Guide#DeltaUserGuide-Table.4-wayNVIDIAA40GPUComputeNodeSpecifications)
- [x] Adjust number of GPUS on tasks. This is good for interactive, and also serves as documentation.
- ðŸ”² Unify `wandb` when training on multiple gpus.
- ðŸ”² Launch 100 epochs on `1e4`.

## 2023.09.08

- [x] Recreate the `1e5` dataset, only was able to complete 2e4 data in 10 hrs on 1 A40.
- [x] Globus transfer data
- [x] Run `1e5` training loop speed tests. â†’ [[Training Speedup with 1e5 CellDataset|dendron://torchcell/experiments.dmf_costanzo_deepset#training-speedup-with-1e5-celldataset]]
- [x] Since `1e5` dataset is taking some time to run through in interactive node, make `1e4` dataset.
- [x] Globus `1e4` datset to `Delta`.
- [x] Move notes in tasks to proper note
- [x] Try MI100 interactive â†’ created new task for launch, MI100 is discounted on Delta. â†’ `>>> torch.cuda.is_available(); False`
- ðŸ”² Profile `1e5`
- ðŸ”² We need reason to believe that using llm should work. Collect `1e5` dataset, `add`, `mean`, vectors of missing data, umap visualize, with dmf overlay â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² Do same `umap` for `smf` alone. â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² If both of `smf` and `dmf` umap look to work, do a combined umap, with `smf` as a different shape. â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² Gene ontology for `DCell` â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² `DCell` model â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² Write `DCell` network as perturbation to GO graph â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² WT difference for loss function... thinking dataset should have a reference object at highest level. â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² WL-Lehman for fitness prediction â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² Add in gene essentiality dataset `smf` â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² Add in synthetic lethality dataset `dmf` [synthetic lethality db](https://synlethdb.sist.shanghaitech.edu.cn/v2/#/) this doesn't look like it has media conditions. â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² Rewrite single cell fitness for `lmdb` â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² Work on merge single cell fitness data â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² Add triple mutant fitness dataset `tmf` â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² Add gene expression for `smf` data â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² Add gene expression data for `dmf` data â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² Add morphology dataset â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]
- ðŸ”² Add plotting functionality on genomes â†’ Moved to [[user.mjvolk3.torchcell.tasks.future#future]]

## 2023.09.07

- [x] Run test run on `cell.py` on interactive cpu. â†’ `20it/s` very slow, M1 is nearly `2000 it/s`
- [x] Filter in cell dataset talking a terribly long time. Can Globus transfer for now but should figure out how we can speed up filtering. â†’ Started transfer
- [x] Write on dataset merge issues â†’ [[Merging Dataset Issues|dendron://torchcell/torchcell.datasets.cell#merging-dataset-issues]]
- [x] The limitation again looks like IO from reading data from `lmdb`. We should be able to take advantage of multithreading for this. Try multithreading filtering delta interactive cpu. â†’ There does look to be a speed up to `120it/s` on 16 cpu. With this the job with finish in 30 hrs... For now just going to run things locally and tranfer with Globus, since it takes around 2 hours to transfer the data... This isn't a great solution for the library.
- [x] Try a cpu slurm job with 32 cpu. â†’ This fails due to some `sqlite3` error. To use `num_workers > 0` we need to be to pickle the dataset for multiprocessing, this cannot be done if there is a database open. `self.genome` is using a `sqlite3` database.
- [x] Fix `dmf` dataset so it can work with `lmdb` and `num_workers > 0`  â†’ [[Using LMDB with Dataloader num_workers ge 0|dendron://torchcell/torchcell.datasets.scerevisiae.costanzo2016#using-lmdb-with-dataloader-num_workers-ge-0]]
- [x] Fix `cell` dataset so it can work with `lmdb` and `num_workers > 0`, we will also need to handle removing the genome sql db from the init. â†’ This is a bit Hacky for now. Also had to make sure we removed `seq_embedding` datasets. [[Data Loader Speed with Number of Workers for CellDataset|dendron://torchcell/experiments.dmf_costanzo_deepset#data-loader-speed-with-number-of-workers-for-celldataset]]
- [x] Launch experiment on whole dataset `dmf` â†’ We have speed issues[[Training Speed with Number of Workers for CellDataset|dendron://torchcell/experiments.dmf_costanzo_deepset#training-speed-with-number-of-workers-for-celldataset]]
- ðŸ”² WT difference for loss function... thinking dataset should have a reference object at highest level.
- ðŸ”² Gene ontology for `DCell`
- ðŸ”² Consider making an `smf` dataset that comes from the `dmf` data. â†’ moving to [[user.mjvolk3.torchcell.tasks.future]]
- ðŸ”² Write `DCell` network as perturbation to GO graph
- ðŸ”² Add in gene essentiality dataset `smf`
- ðŸ”² Add in synthetic lethality dataset `dmf` [synthetic lethality db](https://synlethdb.sist.shanghaitech.edu.cn/v2/#/) this doesn't look like it has media conditions.
- ðŸ”² Rewrite single cell fitness for `lmdb`
- ðŸ”² Work on merge single cell fitness data
- ðŸ”² Add triple mutant fitness dataset `tmf`
- ðŸ”² Add gene expression for `smf` data
- ðŸ”² Add gene expression data for `dmf` data
- ðŸ”² Add morphology dataset
- ðŸ”² Add plotting functionality on genomes

## 2023.09.06

- [x] Try to archive files in `5e5` at a time. â†’ I was able to write the entire `lmdb` before this finished.
- [x] Follow up on Jira issue â†’ met with @Craig-Steffen â†’ Suggestion was to write files on Delta to `/tmp` which is essentially an `ssd`, tar on node, then copy to parallel file system `/scrath`. First trying to write database on `delta`.
- [x] Write database on `delta` â†’ This took 24 mins. Very fast!
- [x] Remove mitochondria genes. â†’ added methods to [[torchcell/sequence/genome/scerevisiae/s288c.py]] `drop_chrmt()` and `drop_empty_go`
- [x] Move sequence download into scerevisiae genome
- [x] Correct `go` methods [[torchcell/sequence/genome/scerevisiae/s288c.py]]
- [x] Create cell dataset locally. â†’ Added `lmdb`
- [x] Review run and launch another exp. â†’ Reviewed but didn't launch
- ðŸ”² WT difference for loss function... thinking dataset should have a reference object at highest level.
- ðŸ”² Gene ontology for `DCell`
- ðŸ”² Add in gene essentiality dataset.
- ðŸ”² Add plotting functionality on genomes [[Rough Plots of Gene Ontology Terms per Gene|dendron://torchcell/torchcell.datasets.scerevisiae.costanzo2016#rough-plots-of-gene-ontology-terms-per-gene]]
- ðŸ”² Filtering cell takes a long time on `Delta` filter is `123.78it/s` on `M1` filter is `2000it/s`. Try to speed up. â†’ [[Cell|dendron://torchcell/torchcell.datasets.cell]]

## 2023.09.05

- [x] Summarize major takeaways from meeting â†’ [[Adam Stewart Meeting|meet.Adam-Stewart.2023.09.01]]
- ðŸ”² Try to archive files in one million at a time.
- [x] 10 min reserach on HDF and LMDB â†’ LMDB looks like a good option for quick reads and HDF look like a good option for hierarchy, but will be slower reading. If I am already using hierearcy within the Torch Data object it might be best to try LMDB first.
- ðŸ”² Jira issue followup, include HDF, and LMDB as mentioned by @Yunan-Luo .
- ðŸ”² Review run and launch another exp.
- ðŸ”² WT difference for loss function... thinking dataset should have a reference object at highest level.
- ðŸ”² Gene ontology for `DCell`
- ðŸ”² Add in gene essentiality dataset.

## 2023.09.01

- [x] Respond to Jira request. â†’ Gave enough info to get the conversation started but need to add a lot more details.
- u Prepare markdown for Adam
- ðŸ”² Cluster nt embeddings of genes and look at smf
- ðŸ”² Add in gene essentiality dataset.

## 2023.08.31

- [x] Work on deleting files â†’ [[torchcell/delete_subset.py]] tried to see if array and mutlithreading could help. Doesn't seem to help that much.
- [x] Submit a Jira Ticket for data transfer issues â†’ [jira-ticket-2023.09.01](https://jira.ncsa.illinois.edu/browse/DELTA-2385)
- [x] Removing the Costanzo folder with  `~2e6` million files. â†’ Done with Globus. I think this is the most robust method moving forward for deleting large set of files.
- [x] Try to send files with Globus again. â†’ This is still very slow, and warning give no progress message. ![](./assets/images/user.Mjvolk3.torchcell.tasks.md.Globus-warning-file-transfer.png)
- [x] Cancel Globus Job, no zip. â†’ After cancelling get ![](./assets/images/user.Mjvolk3.torchcell.tasks.md.Globus-transfer-canceled-90p-completed.png) this has happened almost every time I've cancelled even after only cancelling an hour or so after starting the transfer, so it must not be representative of the number of files transferred. We see that no process files were even transferred.
- [x] Try to Zip dir. â†’ This ran for 20 minutes...
- [x] Globus subset dataset of `1e5` `.pt` â†’  Globus transfer completed. We see here that the number of files was clear. For large transfer attempts we only see 1 or two files. I think these are the `.json` files from other dirs. We also see a `MB/s` rate, whereas the other large runs get stuck and just have a `B/s` rate.
![](./assets/images/user.Mjvolk3.torchcell.tasks.md.globus-transfer-completed-1e5-2023.08.31.png)
- [x] Launch Experiment on A40. â†’ [wandb experiment](https://wandb.ai/zhao-group/torchcell/table?workspace=user-mjvolk3) #wandb.tags.poc, #wandb.tags.subset, poc for proof of concept and subset, since for subset of all data.
- ðŸ”² Respond to Jira request.

## 2023.08.30

- [x] Run test run with dmf â†’ [[M1 Test Run Batch 16 Very Slow|experiments.dmf_costanzo_deepset#m1-test-run-batch-16-very-slow]]
- [x] hydra config experiments â†’ [[experiments/conf/dmf_costanzo_deepset.yaml]]
- [x] Start transfer data to Delta with Globus â†’ This is a bit slow
- [x] Install lua. â†’ Not compatible with Delta Redhat [[Lua Not Compatible with Delta Redhat|dendron://torchcell/torchcell.models.dcell#lua-not-compatible-with-delta-redhat]]
- [x] Build ontology... We will build DCell in `torchcell` since DCell requires, both the curation of data and the building of the ontology which couuld be error prone. There model is clear enough to build. â†’ Building ontology requires getting multigraph data.
- [x] Get all multigraph data from SGD API â†’ Delaying this for `gff`solution
- [x] Build base multidigraph with no edges and data classes stored in network graph. In future transformations on top of multidigraph, will be used to move node data to edges. Then transformations on edged filled graph are used to get pytorch graphs. All along we only need to be check for not compatibility. â†’ Delaying this for `gff`solution
- [x] GO and other useful information also exists in the `gff` file annotating the genome. It will be easier for now to GO out of this. This is probably also more extensible to other organisms too since it is more likely they will have a `gff` than some sort of manually constructed multidigraph. Pull out useful db in an aesthetically appleasing way ðŸ¦‹. â†’ Easier than I thought, all needed info stored in `db.attributes`, added attrs for these.
- [x] Download GO [GO website](http://geneontology.org/docs/download-ontology/) â†’ We could just download the slim set for yeast, but for now this seems like a more general solution. We can do this this if things are slow.
- [x] Look into how we can subset GO by date. â†’ From the looks of this is not possible with the `gff`, but this data does exists in SGD. Just showing one term... We would have to cross reference with this data to get the GO subset. â†’ [[Subsetting GO by Date|dendron://torchcell/torchcell.models.dcell#subsetting-go-by-date]]

## 2023.08.29

- [x] Launch experiment on [delta](https://wiki.ncsa.illinois.edu/display/DSC/Delta+User+Guide). â†’ Just to load data takes over 1 hr [[File Existence Check on Dataset|torchcell.datasets.scerevisiae.costanzo2016#file-existence-check-on-dataset]]
- [x] Rename [[profiles/DMFCostanzo2016LargeDataset_threads_write_data-2023.08.25-22.29.48.prof]] to write since it looks like we will need to use threads in the file check.
- [x] Remedy long file checks with multithreading. â†’ Looks like it works well. Now we have diverged from `PyG`... Will need to add our own datasets module
- [x] AB pointed out some major issues with [[torchcell/models/fungal_utr_transformer.py]]. First embeddings need to be generated, double check. â†’ I've confirmed issues and am diving in deeper.
- [x] Look into fungal transformer weight initialization â†’ Looks like all [upstream models wights not initialized](https://github.com/DennisGankin/species-aware-DNA-LM/issues/4). We should not be using these models in our current experiments.
- [x] Try to recreate experiments to determine how `S. cerevisiae` embeddings are created. We need to know which tokens are used for biasing the species aware model. â†’ There are issues with the env, and the code looks like it is out of data. I've submitted an [env issue](https://github.com/DennisGankin/species-aware-DNA-LM/issues/5).
- [x] Create `dataset.py` for quick reading of data. â†’ [[Skip File Exist Check on Process|torchcell.datasets.scerevisiae.costanzo2016#skip-file-exist-check-on-process]]
- [x] printing an instance is extremely slow. Speed up. â†’ [[Speeding up Data Getting with Len Cache|torchcell.datasets.scerevisiae.costanzo2016#speeding-up-data-getting-with-len-cache]]
- [x] Since moving to `Dataset` `gene_set` is no longer being computed properly. fix. â†’ Fixed by writing to file, during process. Essentially caching the `gene_set`. This saves us from having to compute it many times, and should save us ðŸ›Ÿ on merges.
- [x] Refactor `CellDataset` so it can handle experiments of type `CellDatset`. â†’ copied over the gene set property. I think writing to file is a decent solution, and will save compute. To do this had to make a small dataset `DMFCostanzo2016SmallDataset`
- [x] Transfer `DMFCostanzo2016SmallDataset` to `DMFCostanzo2016LargeDataset`
- [x] Rerun `DMFCostanzo2016LargeDataset` to get with new `gene_set`
- [x] Rerun [[torchcell/datasets/cell.py]] to get a new cell dataset â†’ [[torchcell.datasets.cell]]

## 2023.08.28

- [x] Figure how to preprocess data. â†’ This was a bit of a long exploration. I looked into using HD5 but it is relatively difficult to due vectorized operations with HD5. I started to used `polars` thinking it could solve some of my speed issues, but I still don't fully understand the API how to deal with lazy dataframes. My final solution was to add preprocess methods to the dataset. These differ from transforms in that they can operate over the entire dataset, removing duplicates, filtering etc.
- [x] Add a dataframe property to the dataset that converts the Data object to a dataframe. This could allow for the joining of experimental datasets. â†’ I looked into this and it seems like a bad idea since we will need to populate arbitrary columns, I need to be joining data objects instead
- ðŸ”² Run dataset locally
- ðŸ”² Transfer datset with Globus

## 2023.08.22

- [x] Rerun scripts for dmf fitness â†’ 2023.08.23 3 billion done in a day, when we have 20 billion data points. This won't scale.
- [x] Sync notability notes.
- ðŸ”² Restructure data with metadata. â†’ Holding out on these. While they look like good directions they are a bit difficult ot implement.
- ðŸ”² Restructure data with ontology. â†’ Holding out on these. While they look like good directions they are a bit difficult ot implement.

## 2023.08.19

- [x] Check dmf data on `Delta`. â†’ Exceeded storage limit, ordered 10 Tb.

## 2023.08.17

- [x] Save `DmfCosanzoDataset` small. â†’ 1000 samples
- [x] Fix pre-commit [[.pre-commit-config.yaml]] â†’ Needed to add configuration to [[pyproject.toml]] and deleted `mypy.ini`, since the mypy config can be directly recognized from the `toml`. Needed to make sure that `isort` and `black` were compatible. For now ignoring `flake8` and `mypy` settings.
- [x] Look at `torchgeo` pre-commit â†’ taking a lot of tips from this.o
- [x] In `cell.py` [[Cell|torchcell.datasets.cell]] construct base sequence graph. â†’ It is a set but we add a dummy edge index to make it look more like the standard `pyg` graph. We know there are no edges since it is size `(2,0)`.
- [x] In `cell.py` [[Cell|torchcell.datasets.cell]] allow for getting diff items. â†’ We implement this with a series of methods. We have to overwrite the `get` method of `InMemoryDatset`, then we add a few methods `_subset_graph` and `_add_label` to construct set to be trained on. This is the key part of the library will need careful consideration.
- [x] Create a lightning data module for `DmfCosanzoDataset`, using small. â†’ [[Cell|torchcell.datamodules.cell]]
- [x] Build trainer for fitness prediction. â†’ [[Regression|torchcell.trainers.regression]]
- [x] Add wandb log. â†’ [[Dmf_costanzo_deepset|experiments.dmf_costanzo_deepset]]
- [x] Add fitness prediction plot to wandb log. Just plot on validation.
- [x] Setup `.env` for local for data dir
- [x] Globus transfer data
- [x] Setup `.env` for remote for data dir
- [x] Setup remote workspace
- [x] Transfer compute to `Delta`.

## 2023.08.16

- [x] Update CI â†’ Separated out formatting, tests, and mypy. Could separate out more.
- [x] `DmfCostanzoDataset` takes too long to load, consider making it regular dataset, not `InMemory`. â†’ Tried making a [[DmfCostanzoDataset Out Memory Dataset|torchcell.datasets.scerevisiae.costanzo2016#dmfcostanzodataset-out-memory-dataset]] and this takes forever to save all files.
- [x] Recover repo after accidentally deleting instead of deleting cache â†’ missed most recent notes.

## 2023.08.15

- [x] Create an analogous sequence embedding dataset for the nucleotide transformer.
- [x] Unify the embedding datasets with a nucleotide embedding datasets â†’ [[torchcell.datasets.embedding]]
- [x] Test goings on the nucleotide transformer and the utr transformer. â†’ not yet tested properly.
- [x] Run the nucleotide transformer overnight. â†’ Still took half of the next day on local. should consider batching on GPU.
- ðŸ”² DMFCostanzo takes too long to load, consider making it regular dataset, not `InMemory`.
- ðŸ”² Build a training loop for dmf
- ðŸ”² Add tiling window functions for nucleotide transformer â†’ [[user.mjvolk3.torchcell.tasks.future#future]]

## 2023.08.14

- [x] Create datasets for embeddings â†’ `GeneEmbeddingDataset` [[torchcell.datasets.fungal_up_down_transformer]]
- [x] Aggregate embedding datasets â†’ `join_datasets`[[torchcell.datasets.fungal_up_down_transformer]]

## 2023.08.13

- Might have an issue with pydantic â†’ [[Pytorch Lightning Compatibility with Pydantic|dendron://torchcell/torchcell.datamodels.pydantic#pytorch-lightning-compatibility-with-pydantic]]

## 2023.08.10

- [x] Clean up git
- [x] Dataset
- [x] One big issue is that the embeddings take time to compute. It would be better if we made a `GenomeDataset` that takes the convenience functions from the `Genome` class that is more of a raw sequence manipulation and sequence feature selection class, and we use thsese in conjunction with

## 2023.08.09

- [x] Yeastmine can query all of the availbe locusID's with [YeastMine Lists](https://yeastmine.yeastgenome.org/yeastmine/bag.do?subtab=view)
- [x] We can also retrieve all phenotypes for all genes [YeastMine Phenotypes](https://yeastmine.yeastgenome.org/yeastmine/results.do?trail=%257Cquery)

## 2023.08.08

- [x] Review `TorchGeo` data joins. â†’ Looked over enough to get the gist. Ran debugger on this [[../torchgeo/torchgeo_tutorial.py]]. The thing I am most concerned about is joins. Joins really need to be done based on some hybrid `genotype-phenotype-environment` ontology. â†’ [[Genotype-Phenotype-Environment Ontology For Data Merge|dendron://torchcell/torchcell.datasets.cell#genotype-phenotype-environment-ontology-for-data-merge]]
- ðŸ”² Clean up the `pretrain_LLM` interface. We should just be able to import models, not have to run a series of commands on them.
- ðŸ”² Do a join between a cell dataset and costanzo dataset.
- ðŸ”² I am thinking that `CellDataset` is going to be so complex that we will need some sort of configuration. â†’ moved to [[user.mjvolk3.torchcell.tasks.future]]
- ðŸ”² Check the genotype intersection on the `DMF` data
- [x] Look into the environmental ontology, and the systems biology or sequence ontology for genotype. â†’ I did some of this and the ontologies seem incomplete for my purposes.
- ðŸ”² When I do joins of data I want to know what types of data were excluded and which were included. I think that there operations need to be part of something like `Cell.join` â†’ moved to [[user.mjvolk3.torchcell.tasks.future]]
- ðŸ”² Implement Lightning Trainers
- ðŸ”² Add deep set model
- ðŸ”² Minimal Wandb Log
- ðŸ”² Log fitness plot same as `Dcell`
- ðŸ”² Train model

## 2023.08.07

- [x] Updated [[src Drawio|torchcell#src-drawio]]. I think this view is simpler in light over the considerations over the library.
- [x] `window_5utr` and `window_3utr` selection. â†’ Looks like they work, need to add test functions to these.
- [x] costanzo2016 data.
- [x] Yeast LLM
- ðŸ”² Look into the environmental ontology, and the systems biology or sequence ontology for genotype.
- ðŸ”² Review `TorchGeo` data joins.
- ðŸ”² When I do joins of data I want to know what types of data were excluded and which were included. I think that there operations need to be part of something like `Cell.join`
- ðŸ”² Implement Lightning Trainers
- ðŸ”² Add deep set model
- ðŸ”² Minimal Wandb Log
- ðŸ”² Log fitness plot same as `Dcell`
- ðŸ”² Train model

## 2023.08.05

- [x] Make it so genome can call on methods with `__getitem__` like so `genome["YDR210W].seq`, `genome["YDR210W].window(6e4)`, `genome["YDR210W].window(6e4, is_max_size=False)`, `genome["YDR210W].window_5utr(1000)`, `genome["YDR210W].window_3utr(300)`, etc. Think we can do this by having a wrapper object around the db. â†’ Still need to do `window_5utr` and `window_3utr`.

## 2023.08.04

- ðŸ”² Build out a media note. This would really be best if it is linked to the the specific publication notes that contain the necessary information on any given publication. Note that YPD a YEPD are the same. Yeast Extract Peptone Dextrose with their corresponding concentrations. YEPD + G418 for DMA (Deletion Mutant Array) Growth. Need to pay careful attention to this, may not matter if it has already been proven within reason that the addition of G418 creates a small enough deviation. â†’ Moved to [[Tracking Media in the Ontology|dendron://torchcell/torchcell.ontology.tc_ontology#tracking-media-in-the-ontology]]
- ðŸ”² Does the yeastmined data have the pvalues and the sga interaction scores? Looks like for trigenic they are contained in a "note" field... you've got to be kidding me... populated in a "note" field... and for they don't look populated for digenic.... they are populated for Costanzo 2016 in an "alleles" field, but they are not populated for 2010... This data for networks is probably better pulled from the original data, but then there is potential confliction  `MultiDiGraph` and experiments. â†’ moved note [[Yeastmine Data Comparison to SGD Backend API|dendron://torchcell/torchcell.multidigraph.sgd#yeastmine-data-comparison-to-sgd-backend-api]]
- [x] Look into why `src/package` not typically used. Looks like `.egg` is generally not comitted to project. â†’ it is fine to keep src.
- ðŸ”² Make it so genome can call on methods with `__getitem__` like so `genome["YDR210W].seq`, `genome["YDR210W].window(6e4)`, `genome["YDR210W].window(6e4, is_max_size=False)`, `genome["YDR210W].window_5utr(1000)`, `genome["YDR210W].window_3utr(300)`, etc. Think we can do this by having a wrapper object around the db.
- ðŸ”² Allow for indexing on gene name in torch datasets. `dataset[0]`, `dataset["YDR210W"]`
- ðŸ”² Around 32 genes are under 6kb... need to find a way around this. Also made mistake thinking the nucleotide transformer could handle 60kb... whoops. We can still use the Enformer for these large windows. Could also use 3 embeddings to capture the gene with nt transformer. Looks like this is the largest gene in yeast `YKR054C, length: 12278` â†’ [[S288C DNA length for DNA LLMs|dendron://torchcell/torchcell.sequence.genome.scerevisiae.s288c#s288c-dna-length-for-dna-llms]]

## 2023.08.03

- [x] Test [[torchcell/sequence/sequence.py]] window functions â†’ [[tests/torchcell/sequence/test_sequence.py]] #ChatGPT is very useful to get quick tests off the ground that can be tweaked for proper behavior.
- [x] Switch to the [Mypy - Matan Grover](https://marketplace.visualstudio.com/items?itemName=matangover.mypy#review-details) since this uses `.ini` has cross file integration. â†’ Switched but I think we are better off using cmd line. I added some bash scripts so I can common `tasks`.
- ðŸ”² Implement `cell.py` [[Src|torchcell]]
- ðŸ”² Implement `datasets`. â†’ [[Scerevisiae|torchcell.datasets.scerevisiae]] Need to split up by organism...
- ðŸ”² Discuss different data
- ðŸ”² Implement `datasets.py` [[Src|torchcell]]
- ðŸ”² Change to something like from tochcell.genome import SCerevisiaeGenome.
- ðŸ”² Undo the import magic and drop some of the `if __name__`s
