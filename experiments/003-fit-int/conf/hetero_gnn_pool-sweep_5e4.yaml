program: experiments/003-fit-int/scripts/hetero_gnn_pool.py
project: torchcell_003-fit-int_hetero_gnn_pool_5e4
method: random

parameters:
  cell_dataset:
    parameters:
      graphs:
        values: [[physical, regulatory]]
      node_embeddings:
        values:
          # - [codon_frequency]
          # - [one_hot_gene]
          # - [calm]
          # - [fudt_downstream]
          # - [fudt_upstream]
          # - [prot_T5_all]
          # - [prot_T5_no_dubious]
          # - [nt_window_5979]
          # - [nt_window_three_prime_300]
          # - [nt_window_five_prime_1003]
          # - [esm2_t33_650M_UR50D_all]
          # - [esm2_t33_650M_UR50D_no_dubious]
          # - [normalized_chrom_pathways]
          # - [random_1000]
          # - [random_100]
          # - [random_10]
          # - [random_1]
          - [learnable_embedding]
      learnable_embedding_input_channels:
        values: [64]

      graph_processor:
        values: [subgraph_representation] # [subgraph_representation, node_feature, node_augmentation]

  transforms:
    parameters:
      num_bins:
        values: [32]
      strategy:
        values: ["equal_frequency"] # or "equal_width"
      store_continuous:
        values: [true]
      label_type:
        values: [ordinal]

  profiler:
    parameters:
      is_pytorch:
        values: [true]

  data_module:
    parameters:
      is_perturbation_subset:
        values: [true]
      perturbation_subset_size:
        values: [5e4]
      batch_size:
        values: [64]
      num_workers:
        values: [16]
      pin_memory:
        values: [true]
      prefetch:
        values: [false]

  trainer:
    parameters:
      max_epochs:
        values: [40]
      strategy:
        values: [auto]
      accelerator:
        values: [gpu]
      devices:
        values: [auto]

  model:
    parameters:
      # Common parameters for all conv types
      hidden_channels:
        values: [64]
      num_layers:
        values: [2]
      conv_type:
        values: ["GCN", "GAT", "Transformer", "GIN"]
      pooling:
        values: ["sum", "mean"]
      activation:
        values: ["gelu"]
      norm:
        values: ["batch", "layer"]
      num_tasks:
        values: [2]
      dropout:
        values: [0.2]

      # Prediction head parameters
      head_num_layers:
        values: [10]
      head_hidden_channels:
        values: [32]
      head_dropout:
        values: [0.2]
      head_activation:
        values: ["gelu"]
      head_residual:
        values: [true]
      head_norm:
        values: ["batch"]

      # GCN specific parameters
      gcn_bias:
        values: [true]
      gcn_add_self_loops:
        values: [false]
      gcn_normalize:
        values: [false]
      gcn_is_skip_connection:
        values: [true]

      # GAT specific parameters
      gat_heads:
        values: [4]
      gat_concat:
        values: [true]
      gat_bias:
        values: [true]
      gat_add_self_loops:
        values: [false]
      gat_share_weights:
        values: [false]
      gat_is_skip_connection:
        values: [true]

      # Transformer specific parameters
      transformer_heads:
        values: [4]
      transformer_concat:
        values: [true]
      transformer_beta:
        values: [true]
      transformer_bias:
        values: [true]
      transformer_root_weight:
        values: [true]
      transformer_add_self_loops:
        values: [true]
      transformer_edge_dim:
        values: [null]

      # GIN specific parameters
      gin_train_eps:
        values: [true]
      gin_hidden_multiplier:
        values: [2.0]
      gin_add_self_loops:
        values: [false]
      gin_is_skip_connection:
        values: [true]
      gin_num_mlp_layers:
        values: [3]
      gin_is_mlp_skip_connection:
        values: [true]

  regression_task:
    parameters:
      is_weighted_phenotype_loss:
        values: [true]
      loss_type:
        values: ["ce_ordinal"]
      optimizer:
        parameters:
          type:
            values: ["AdamW"]
          lr:
            values: [1e-3, 1e-4, 1e-5, 1e-6]
          weight_decay:
            values: [1e-3, 1e-5, 1e-7]
      lr_scheduler:
        parameters:
          type:
            values: ["ReduceLROnPlateau"]
          mode:
            values: ["min"]
          factor:
            values: [0.2]
          patience:
            values: [3]
          threshold:
            values: [1e-4]
          threshold_mode:
            values: ["rel"]
          cooldown:
            values: [2]
          min_lr:
            values: [1e-9]
          eps:
            values: [1e-10]
      boxplot_every_n_epochs:
        values: [1]
      clip_grad_norm:
        values: [true]
      clip_grad_norm_max_norm:
        values: [10.0]
      grad_accumulation_schedule:
        values: [null]

command:
  - ${env}
  - python
  - ${program}
