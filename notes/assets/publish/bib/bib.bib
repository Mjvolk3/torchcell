@article{corsoGraphNeuralNetworks2024,
  title = {Graph Neural Networks},
  author = {Corso, Gabriele and Stark, Hannes and Jegelka, Stefanie and Jaakkola, Tommi and Barzilay, Regina},
  date = {2024-03-07},
  journaltitle = {Nature Reviews Methods Primers},
  shortjournal = {Nat Rev Methods Primers},
  volume = {4},
  number = {1},
  pages = {1--13},
  publisher = {Nature Publishing Group},
  issn = {2662-8449},
  doi = {10.1038/s43586-024-00294-7},
  url = {https://www.nature.com/articles/s43586-024-00294-7},
  urldate = {2024-03-12},
  abstract = {iGraphs are flexible mathematical objects that can represent many entities and knowledge from different domains, including in the life sciences. Graph neural networks (GNNs) are mathematical models that can learn functions over graphs and are a leading approach for building predictive models on graph-structured data. This combination has enabled GNNs to advance the state of the art in many disciplines, from discovering new antibiotics and identifying drug-repurposing candidates to modelling physical systems and generating new molecules. This Primer provides a practical and accessible introduction to GNNs, describing their properties and applications to the life and physical sciences. Emphasis is placed on the practical implications of key theoretical limitations, new ideas to solve these challenges and important considerations when using GNNs on a new task.},
  langid = {english},
  keywords = {✅🦌,Computational biology and bioinformatics,Statistics},
  file = {/Users/michaelvolk/Zotero/storage/6AKYLCMG/Corso et al. - 2024 - Graph neural networks.pdf}
}

@online{dalla-torreNucleotideTransformerBuilding2023,
  title = {The {{Nucleotide Transformer}}: {{Building}} and {{Evaluating Robust Foundation Models}} for {{Human Genomics}}},
  shorttitle = {The {{Nucleotide Transformer}}},
  author = {Dalla-Torre, Hugo and Gonzalez, Liam and Revilla, Javier Mendoza and Carranza, Nicolas Lopez and Grzywaczewski, Adam Henryk and Oteri, Francesco and Dallago, Christian and Trop, Evan and Sirelkhatim, Hassan and Richard, Guillaume and Skwark, Marcin and Beguir, Karim and Lopez, Marie and Pierrot, Thomas},
  date = {2023-01-15},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2023.01.11.523679},
  doi = {10.1101/2023.01.11.523679},
  url = {https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1},
  urldate = {2023-05-27},
  abstract = {Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learnings between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, integrating information from 3,202 diverse human genomes, as well as 850 genomes from a wide range of species, including model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the representations alone match or outperform specialized methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning. Despite no supervision, the transformer models learnt to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations alone can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence alone.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {🦌✅},
  annotation = {1 citations (Semantic Scholar/DOI) [2023-05-27]},
  file = {/Users/michaelvolk/Zotero/storage/M2XTQGDZ/Dalla-Torre et al. - 2023 - The Nucleotide Transformer Building and Evaluatin.pdf}
}

@article{engelNewDataCollaborations2022,
  title = {New Data and Collaborations at the {{Saccharomyces Genome Database}}: Updated Reference Genome, Alleles, and the {{Alliance}} of {{Genome Resources}}},
  shorttitle = {New Data and Collaborations at the {{Saccharomyces Genome Database}}},
  author = {Engel, Stacia R and Wong, Edith D and Nash, Robert S and Aleksander, Suzi and Alexander, Micheal and Douglass, Eric and Karra, Kalpana and Miyasato, Stuart R and Simison, Matt and Skrzypek, Marek S and Weng, Shuai and Cherry, J Michael},
  date = {2022-04-01},
  journaltitle = {Genetics},
  shortjournal = {Genetics},
  volume = {220},
  number = {4},
  pages = {iyab224},
  issn = {1943-2631},
  doi = {10.1093/genetics/iyab224},
  url = {https://doi.org/10.1093/genetics/iyab224},
  urldate = {2023-08-01},
  abstract = {Saccharomyces cerevisiae is used to provide fundamental understanding of eukaryotic genetics, gene product function, and cellular biological processes. Saccharomyces Genome Database (SGD) has been supporting the yeast research community since 1993, serving as its de facto hub. Over the years, SGD has maintained the genetic nomenclature, chromosome maps, and functional annotation, and developed various tools and methods for analysis and curation of a variety of emerging data types. More recently, SGD and six other model organism focused knowledgebases have come together to create the Alliance of Genome Resources to develop sustainable genome information resources that promote and support the use of various model organisms to understand the genetic and genomic bases of human biology and disease. Here we describe recent activities at SGD, including the latest reference genome annotation update, the development of a curation system for mutant alleles, and new pages addressing homology across model organisms as well as the use of yeast to study human disease.},
  keywords = {🦌✅,SGD,SGD.data},
  annotation = {18 citations (Semantic Scholar/DOI) [2023-08-01]},
  file = {/Users/michaelvolk/Zotero/storage/GF7RFNWL/Engel et al. - 2022 - New data and collaborations at the Saccharomyces G.pdf;/Users/michaelvolk/Zotero/storage/5EUQC34Q/6460340.html}
}

@online{forsterBIONICBiologicalNetwork2021,
  title = {{{BIONIC}}: {{Biological Network Integration}} Using {{Convolutions}}},
  shorttitle = {{{BIONIC}}},
  author = {Forster, Duncan T. and Boone, Charles and Bader, Gary D. and Wang, Bo},
  date = {2021-03-16},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2021.03.15.435515},
  doi = {10.1101/2021.03.15.435515},
  url = {https://www.biorxiv.org/content/10.1101/2021.03.15.435515v1},
  urldate = {2024-02-19},
  abstract = {Biological networks constructed from varied data, including protein-protein interactions, gene expression data, and genetic interactions can be used to map cellular function, but each data type has individual limitations such as bias and incompleteness. Unsupervised network integration promises to address these limitations by combining and automatically weighting input information to obtain a more accurate and comprehensive result. However, existing unsupervised network integration methods fail to adequately scale to the number of nodes and networks present in genome-scale data and do not handle partial network overlap. To address these issues, we developed an unsupervised deep learning-based network integration algorithm that incorporates recent advances in reasoning over unstructured data – namely the graph convolutional network (GCN) – and can effectively learn dependencies between any input network, such as those composed of protein-protein interactions, gene co-expression, or genetic interactions. Our method, BIONIC (Biological Network Integration using Convolutions), learns features which contain substantially more functional information compared to existing approaches, linking genes that share diverse functional relationships, including co-complex and shared bioprocess annotation. BIONIC is scalable in both size and quantity of the input networks, making it feasible to integrate numerous networks on the scale of the human genome.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {🦌✅},
  file = {/Users/michaelvolk/Zotero/storage/VF476WAJ/Forster et al. - 2021 - BIONIC Biological Network Integration using Convo.pdf}
}

@article{forsterBIONICBiologicalNetwork2022,
  title = {{{BIONIC}}: Biological Network Integration Using Convolutions},
  shorttitle = {{{BIONIC}}},
  author = {Forster, Duncan T. and Li, Sheena C. and Yashiroda, Yoko and Yoshimura, Mami and Li, Zhijian and Isuhuaylas, Luis Alberto Vega and Itto-Nakama, Kaori and Yamanaka, Daisuke and Ohya, Yoshikazu and Osada, Hiroyuki and Wang, Bo and Bader, Gary D. and Boone, Charles},
  date = {2022-10-03},
  journaltitle = {Nature Methods},
  shortjournal = {Nat Methods},
  pages = {1--12},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-022-01616-x},
  url = {https://www.nature.com/articles/s41592-022-01616-x},
  urldate = {2022-10-06},
  abstract = {Biological networks constructed from varied data can be used to map cellular function, but each data type has limitations. Network integration promises to address these limitations by combining and automatically weighting input information to obtain a more accurate and comprehensive representation of the underlying biology. We developed a deep learning-based network integration algorithm that incorporates a graph convolutional network framework. Our method, BIONIC (Biological Network Integration using Convolutions), learns features that contain substantially more functional information compared to existing approaches. BIONIC has unsupervised and semisupervised learning modes, making use of available gene function annotations. BIONIC is scalable in both size and quantity of the input networks, making it feasible to integrate numerous networks on the scale of the human genome. To demonstrate the use of BIONIC in identifying new biology, we predicted and experimentally validated essential gene chemical–genetic interactions from nonessential gene profiles in yeast.},
  langid = {english},
  keywords = {🦌✅,data-integration,functional-clustering,functional-genomics,machine-learning,network-topology},
  annotation = {1 citations (Semantic Scholar/DOI) [2022-11-26]},
  file = {/Users/michaelvolk/Zotero/storage/72JJSI8P/Forster et al. - 2022 - BIONIC biological network integration using convo.pdf;/Users/michaelvolk/Zotero/storage/965G3C3Z/s41592-022-01616-x.html}
}

@inproceedings{jetteArchitectureSlurmWorkload2023,
  title = {Architecture of~the~{{Slurm Workload Manager}}},
  booktitle = {Job {{Scheduling Strategies}} for {{Parallel Processing}}},
  author = {Jette, Morris A. and Wickberg, Tim},
  editor = {Klusáček, Dalibor and Corbalán, Julita and Rodrigo, Gonzalo P.},
  date = {2023},
  pages = {3--23},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-43943-8_1},
  abstract = {Slurm is an open source, fault-tolerant, and highly scalable workload manager used on many of the world’s supercomputers and computer clusters. As a cluster workload manager, Slurm has three key functions. First, it allocates exclusive and/or non-exclusive access to resources for some duration of time. Second, it provides a framework for starting, executing, and monitoring work on the allocated resources. Finally, it arbitrates contention for resources by managing queues of pending work and enforcing administrative policies. This paper describes the current design and capabilities of Slurm.},
  isbn = {978-3-031-43943-8},
  langid = {english},
  keywords = {✅🦌,hpc,scheduling,slurm},
  annotation = {2 citations (Semantic Scholar/DOI) [2024-06-06]},
  file = {/Users/michaelvolk/Zotero/storage/PHNLMUHT/Jette_Wickberg_2023_Architecture of the Slurm Workload Manager.pdf}
}

@online{liLeveragingLargeLanguage2024,
  title = {Leveraging Large Language Models for Metabolic Engineering Design},
  author = {Li, Xiongwen and Liang, Zhu and Guo, Zhetao and Liu, Ziyi and Wu, Ke and Luo, Jiahao and Zhang, Yuesheng and Liu, Lizheng and Sun, Manda and Huang, Yuanyuan and Tang, Hongting and Chen, Yu and Yu, Tao and Nielsen, Jens and Li, Feiran},
  date = {2024-09-13},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2024.09.09.612023},
  doi = {10.1101/2024.09.09.612023},
  url = {https://www.biorxiv.org/content/10.1101/2024.09.09.612023v1},
  urldate = {2024-12-02},
  abstract = {Establishing efficient cell factories involves a continuous process of trial and error due to the intricate nature of metabolism. This complexity makes predicting effective engineering targets a challenging task. Therefore, it is vital to learn from the accumulated successes of previous designs for advancing future cell factory development. In this study, we developed a method based on large language models (LLMs) to extract metabolic engineering strategies from research articles on a large scale. We created a database containing over 29006 metabolic engineering entries, 1210 products and 751 organisms. Using this extracted data, we trained a hybrid model combining deep learning and mechanistic approaches to predict engineering targets. Our model outperformed traditional metabolic engineering target prediction algorithms, excelled in predicting the effects of gene modifications, and generalized well to out-of-distribution products and multiple gene combinations. Our study provides a valuable dataset, a chatbot, and an engineering target prediction model for the metabolic engineering field and exemplifies an efficient method for leveraging existing knowledge for future predictions.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {🦌✅},
  annotation = {0 citations (Semantic Scholar/DOI) [2024-12-02]},
  file = {/Users/michaelvolk/Zotero/storage/CGTXN2I2/20B090C5-FEC9-4D43-B030-B05F6D7D03F4.jpeg;/Users/michaelvolk/Zotero/storage/HWSMRLJG/Li et al_2024_Leveraging large language models for metabolic engineering design.pdf;/Users/michaelvolk/Zotero/storage/LMT63YE9/SI - Li et al. - 2024 - Leveraging large language models for metabolic eng.pdf}
}

@article{roohaniPredictingTranscriptionalOutcomes2023,
  title = {Predicting Transcriptional Outcomes of Novel Multigene Perturbations with {{GEARS}} | {{Nature Biotechnology}}},
  author = {Roohani, Yusuf and Huang, Kexin and Leskovec, Jure},
  date = {2023-08-17},
  journaltitle = {Nature Biotechnology},
  issn = {1546-1696},
  doi = {10.1038/s41587-023-01905-6},
  url = {https://www.nature.com/articles/s41587-023-01905-6},
  urldate = {2023-08-25},
  abstract = {Understanding cellular responses to genetic perturbation is central to numerous biomedical applications, from identifying genetic interactions involved in cancer to developing methods for regenerative medicine. However, the combinatorial explosion in the number of possible multigene perturbations severely limits experimental interrogation. Here, we present graph-enhanced gene activation and repression simulator (GEARS), a method that integrates deep learning with a knowledge graph of gene–gene relationships to predict transcriptional responses to both single and multigene perturbations using single-cell RNA-sequencing data from perturbational screens. GEARS is able to predict outcomes of perturbing combinations consisting of genes that were never experimentally perturbed. GEARS exhibited 40\% higher precision than existing approaches in predicting four distinct genetic interaction subtypes in a combinatorial perturbation screen and identified the strongest interactions twice as well as prior approaches. Overall, GEARS can predict phenotypically distinct effects of multigene perturbations and thus guide the design of perturbational experiments.},
  keywords = {🦌✅},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-08-25]},
  file = {/Users/michaelvolk/Zotero/storage/9ZXD4ZIK/Roohani et al. - 2023 - Predicting transcriptional outcomes of novel multigene perturbations with GEARS  Nature Biotechnology.pdf;/Users/michaelvolk/Zotero/storage/CK8M4Z4P/SI - Roohani et al. - 2023 - Predicting transcriptional outcomes of novel multi.pdf}
}

@online{rosenUniversalCellEmbeddings2023,
  title = {Universal {{Cell Embeddings}}: {{A Foundation Model}} for {{Cell Biology}}},
  shorttitle = {Universal {{Cell Embeddings}}},
  author = {Rosen, Yanay and Roohani, Yusuf and Agarwal, Ayush and Samotorčan, Leon and Consortium, Tabula Sapiens and Quake, Stephen R. and Leskovec, Jure},
  date = {2023-11-29},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2023.11.28.568918},
  doi = {10.1101/2023.11.28.568918},
  url = {https://www.biorxiv.org/content/10.1101/2023.11.28.568918v1},
  urldate = {2023-12-13},
  abstract = {Developing a universal representation of cells which encompasses the tremendous molecular diversity of cell types within the human body and more generally, across species, would be transformative for cell biology. Recent work using single-cell transcriptomic approaches to create molecular definitions of cell types in the form of cell atlases has provided the necessary data for such an endeavor. Here, we present the Universal Cell Embedding (UCE) foundation model. UCE was trained on a corpus of cell atlas data from human and other species in a completely self-supervised way without any data annotations. UCE offers a unified biological latent space that can represent any cell, regardless of tissue or species. This universal cell embedding captures important biological variation despite the presence of experimental noise across diverse datasets. An important aspect of UCE’s universality is that any new cell from any organism can be mapped to this embedding space with no additional data labeling, model training or fine-tuning. We applied UCE to create the Integrated Mega-scale Atlas, embedding 36 million cells, with more than 1,000 uniquely named cell types, from hundreds of experiments, dozens of tissues and eight species. We uncovered new insights about the organization of cell types and tissues within this universal cell embedding space, and leveraged it to infer function of newly discovered cell types. UCE’s embedding space exhibits emergent behavior, uncovering new biology that it was never explicitly trained for, such as identifying developmental lineages and embedding data from novel species not included in the training set. Overall, by enabling a universal representation for every cell state and type, UCE provides a valuable tool for analysis, annotation and hypothesis generation as the scale and diversity of single cell datasets continues to grow.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {🦌✅},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-12-13]},
  file = {/Users/michaelvolk/Zotero/storage/PUBV7GDL/Rosen et al. - 2023 - Universal Cell Embeddings A Foundation Model for .pdf}
}

@inproceedings{stewartTorchGeoDeepLearning2022a,
  title = {{{TorchGeo}}: Deep Learning with Geospatial Data},
  shorttitle = {{{TorchGeo}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Advances}} in {{Geographic Information Systems}}},
  author = {Stewart, Adam J. and Robinson, Caleb and Corley, Isaac A. and Ortiz, Anthony and Ferres, Juan M. Lavista and Banerjee, Arindam},
  date = {2022-11-22},
  series = {{{SIGSPATIAL}} '22},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3557915.3560953},
  url = {https://dl.acm.org/doi/10.1145/3557915.3560953},
  urldate = {2023-08-18},
  abstract = {Remotely sensed geospatial data are critical for applications including precision agriculture, urban planning, disaster monitoring and response, and climate change research, among others. Deep learning methods are particularly promising for modeling many remote sensing tasks given the success of deep neural networks in similar computer vision tasks and the sheer volume of remotely sensed imagery available. However, the variance in data collection methods and handling of geospatial metadata make the application of deep learning methodology to remotely sensed data nontrivial. For example, satellite imagery often includes additional spectral bands beyond red, green, and blue and must be joined to other geospatial data sources that can have differing coordinate systems, bounds, and resolutions. To help realize the potential of deep learning for remote sensing applications, we introduce TorchGeo, a Python library for integrating geospatial data into the PyTorch deep learning ecosystem. TorchGeo provides data loaders for a variety of benchmark datasets, composable datasets for generic geospatial data sources, samplers for geospatial data, and transforms that work with multispectral imagery. TorchGeo is also the first library to provide pre-trained models for multispectral satellite imagery (e.g., models that use all bands from the Sentinel-2 satellites), allowing for advances in transfer learning on downstream remote sensing tasks with limited labeled data. We use TorchGeo to create reproducible benchmark results on existing datasets and benchmark our proposed method for preprocessing geospatial imagery on the fly. TorchGeo is open source and available on GitHub: https://github.com/microsoft/torchgeo.},
  isbn = {978-1-4503-9529-8},
  keywords = {🦌✅,datasets,deep-learning,earth-observation,geospatial,models,remote-sensing,samplers,transforms},
  annotation = {0 citations (Semantic Scholar/DOI) [2023-08-18]},
  file = {/Users/michaelvolk/Zotero/storage/JN8FQMKR/Stewart et al. - 2022 - TorchGeo deep learning with geospatial data.pdf}
}

@online{zaheerDeepSets2018,
  title = {Deep {{Sets}}},
  author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
  date = {2018-04-14},
  eprint = {1703.06114},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1703.06114},
  url = {http://arxiv.org/abs/1703.06114},
  urldate = {2023-08-04},
  abstract = {We study the problem of designing models for machine learning tasks defined on \textbackslash emph\{sets\}. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \textbackslash cite\{poczos13aistats\}, to anomaly detection in piezometer data of embankment dams \textbackslash cite\{Jung15Exploration\}, to cosmology \textbackslash cite\{Ntampaka16Dynamical,Ravanbakhsh16ICML1\}. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
  pubstate = {prepublished},
  keywords = {🦌½✅,Computer-Science-Machine-Learning,Deep-Sets,equivariance,invariance,Statistics-Machine-Learning},
  annotation = {1530 citations (Semantic Scholar/arXiv) [2023-08-04]},
  file = {/Users/michaelvolk/Zotero/storage/8RP6SM59/Zaheer et al_2018_Deep Sets.pdf;/Users/michaelvolk/Zotero/storage/X33W7H3S/1703.html}
}
