defaults:
  - default
  - _self_

wandb:
  project: torchcell_006-kuzmin-tmi_hetero_cell_nsa_retry_test
  tags: []

cell_dataset:
  # graphs: [physical] # runs
  # graphs: [physical, regulatory] # runs
  # graphs: [physical, regulatory, tflink] # runs
  # graphs: [physical, regulatory, tflink, string12_0_neighborhood] # runs
  # graphs: [physical, regulatory, tflink, string12_0_neighborhood, string12_0_fusion] # runs
  # graphs: [physical, regulatory, tflink, string12_0_neighborhood, string12_0_fusion, string12_0_cooccurence]
  graphs:
    [
      physical,
      regulatory,
      tflink,
      string12_0_neighborhood,
      string12_0_fusion,
      string12_0_cooccurence,
      string12_0_coexpression,
      string12_0_experimental,
      string12_0_database,
    ]
  node_embeddings:
    - learnable
  learnable_embedding_input_channels: 64
  incidence_graphs: [metabolism_bipartite]

profiler:
  is_pytorch: true

# Transform configuration
transforms:
  use_transforms: true
  forward_transform:
    normalization:
      gene_interaction:
        strategy: "standard" # Options: standard, minmax, robust

data_module:
  is_perturbation_subset: false
  perturbation_subset_size: 1000 #2.5e4
  batch_size: 2 #32 #4 #16
  num_workers: 1 #2
  pin_memory: true
  prefetch: false
  prefetch_factor: 2

trainer:
  max_epochs: 300
  strategy: ddp #ddp #ddp_find_unused_parameters_true #auto # ddp
  num_nodes: 1
  accelerator: gpu
  devices: 4
  overfit_batches: 0
  precision: "bf16-mixed" # "32-true", "16-mixed", "bf16-mixed"

# Compilation settings for PyTorch 2.0+
compile_mode: null # null, "default", "reduce-overhead", "max-autotune"

# ------ Placeholders for hydra optuna
heads: 8 # placeholder
norms: "layer" # placeholder
# ------

model:
  gene_num: 6607
  reaction_num: 7122
  metabolite_num: 2806
  hidden_channels: 64
  out_channels: 1 # Only gene interaction
  attention_pattern: ["M", "S"] # NSA attention pattern
  norm: ${norms}
  activation: "relu"
  dropout: 0.0 # 0.1
  heads: ${heads}
  prediction_head_config:
    hidden_channels: 64
    head_num_layers: 4
    dropout: 0.0
    activation: "relu"
    head_norm: ${norms}

regression_task:
  loss: mle_dist_supcr # Options: logcosh, icloss, mle_dist_supcr
  is_weighted_phenotype_loss: false
  # Lambda weights for loss components
  lambda_mse: 1.0
  lambda_dist: 100.0 # 0.1 # 0.1 #0.1 #0.1
  lambda_supcr: 0.0 #0.001 #0.001 #0.001 #0.001

  # MleDistSupCR specific configuration
  loss_config:
    # Component-specific parameters
    dist_bandwidth: 2.0
    supcr_temperature: 0.1

    # Buffer configuration
    use_buffer: true
    buffer_size: 256
    min_samples_for_dist: 64
    min_samples_for_supcr: 64

    # DDP configuration
    use_ddp_gather: true
    gather_interval: 1

    # Adaptive weighting
    use_adaptive_weighting: true

    # Temperature scheduling
    use_temp_scheduling: true
    init_temperature: 1.0
    final_temperature: 0.1
    temp_schedule: "exponential"

    # Training duration (for temperature scheduling)
    max_epochs: 300
  optimizer:
    type: "AdamW"
    lr: 5e-4
    weight_decay: 1e-12
  lr_scheduler: null
  clip_grad_norm: true
  clip_grad_norm_max_norm: 10.0
  grad_accumulation_schedule: null #{0: 16}  # Accumulate for 16 steps
  plot_sample_ceiling: 10000
  plot_every_n_epochs: 2
