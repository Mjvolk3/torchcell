# wandb_sweep.yaml
program: experiments/003-fit-int/scripts/hetero_cell_bipartite.py
project: torchcell_003-fit-int_hetero_cell_bipartite_4e5
method: grid

parameters:
  # Cell dataset parameters
  cell_dataset:
    parameters:
      graphs:
        value: [physical, regulatory]
      node_embeddings:
        value: [learnable]
      learnable_embedding_input_channels:
        value: 64
      incidence_graphs:
        value: [metabolism_bipartite]

  # Profiler parameters
  profiler:
    parameters:
      is_pytorch:
        value: true

  # Data module parameters
  data_module:
    parameters:
      is_perturbation_subset:
        value: true
      perturbation_subset_size:
        value: 4e5
      batch_size:
        value: 32
      num_workers:
        value: 4
      pin_memory:
        value: false
      prefetch:
        value: false

  # Trainer parameters
  trainer:
    parameters:
      max_epochs:
        value: 200
      strategy:
        value: auto
      accelerator:
        value: gpu
      devices:
        value: 1
      overfit_batches:
        value: 0
  
  # Model parameters
  model:
    parameters:
      gene_num:
        value: 6607
      reaction_num:
        value: 7122
      metabolite_num:
        value: 2806
      hidden_channels:
        value: 64
      out_channels:
        value: 2
      num_layers:
        values: [2, 3, 4, 5]
      norm:
        value: "layer"
      activation:
        value: "relu"
      dropout:
        value: 0.0
      gene_encoder_config:
        parameters:
          heads:
            value: 10
          concat:
            value: true
          bias:
            value: true
          add_self_loops:
            value: false
          share_weights:
            value: false
      gpr_conv_config:
        parameters:
          heads:
            value: 10
          concat:
            value: true
          add_self_loops:
            value: false
      metabolism_config:
        parameters:
          is_stoich_gated:
            value: true
          use_attention:
            value: true
          heads:
            value: 10
          concat:
            value: true
      prediction_head_config:
        parameters:
          hidden_channels:
            value: 64
          head_num_layers:
            value: 2
          dropout:
            value: 0.0
          activation:
            value: "relu"
          residual:
            value: true
          head_norm:
            value: "layer"

  # Regression task parameters that we're varying
  regression_task:
    parameters:
      is_weighted_phenotype_loss:
        value: true
      lambda_dist:
        value: 0.1 #[0.01, 0.1] #[0, 1e-3, 1e-2, 1e-1, 1, 1.5, 2]
      lambda_supcr:
        value: 0.001 #[0.001, 0.01] #[0, 1e-4, 1e-3, 1e-2, 1e-1, 1]
      optimizer:
        parameters:
          type:
            value: "AdamW"
          lr:
            value: 1e-3 #[1e-5, 1e-4, 1e-3]
          weight_decay:
            value: 1e-9
      lr_scheduler:
        parameters:
          type:
            value: "ReduceLROnPlateau"
          mode:
            value: "min"
          factor:
            value: 0.2
          patience:
            value: 3
          threshold:
            value: 1e-4
          threshold_mode:
            value: "rel"
          cooldown:
            value: 2
          min_lr:
            value: 1e-9
          eps:
            value: 1e-10
      clip_grad_norm:
        value: true
      clip_grad_norm_max_norm:
        value: 10.0
      grad_accumulation_schedule:
        value: null
      plot_sample_ceiling:
        value: 5000

command:
  - ${env}
  - python
  - ${program}
