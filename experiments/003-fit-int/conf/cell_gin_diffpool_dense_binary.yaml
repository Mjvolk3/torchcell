defaults:
  - default
  - _self_

wandb:
  # mode: offline # disabled, offline, online
  project: torchcell_test_cell_gin_diffpool_dense_binary
  tags: []

cell_dataset:
  graphs: [physical, regulatory] # [physical], [regulatory], [physical, regulatory], []
  node_embeddings: [codon_frequency]
  # [one_hot_gene], [codon_frequency], [calm], [fudt_downstream], [fudt_upstream], [prot_T5_all], [prot_T5_no_dubious], [nt_window_5979], [nt_window_three_prime_300], [nt_window_five_prime_1003], [esm2_t33_650M_UR50D_all], [esm2_t33_650M_UR50D_no_dubious], [normalized_chrom_pathways], [random_1000], [random_100], [random_10], [random_1]

data_module:
  is_perturbation_subset: true
  perturbation_subset_size: 5e4 #7e3
  num_workers: 12
  batch_size: 12 # ok: 4; oom: 8, 12... ok gh: 6
  pin_memory: true
  prefetch: false

trainer:
  max_epochs: 40
  strategy: ddp # ddp, auto, ddp_spawn(for troubleshooting)
  accelerator: gpu
  devices: 4 # auto or int

model:
  hidden_channels: 8
  num_layers: 2
  num_pooling_layers: 5 
  cluster_size_decay_factor: 6.0
  activation: gelu
  conv_norm: instance
  mlp_norm: instance
  target_dim: 2
  cluster_aggregation: mean
  add_skip_connections: true
  gin_self_loop: true
  train_eps: true
  eps: 0.0

regression_task:
  # loss_type: "mse"
  optimizer:
    type: "AdamW"
    lr: 1e-5
    weight_decay: 1e-6
  lr_scheduler:
    type: "ReduceLROnPlateau"
    mode: "min"
    factor: 0.2
    patience: 3
    threshold: 1e-4
    threshold_mode: "rel"
    cooldown: 2
    min_lr: 1e-9
    eps: 1e-10
  boxplot_every_n_epochs: 1
  clip_grad_norm: true
  clip_grad_norm_max_norm: 1.0
  cluster_loss_weight: 1.0 #1.0
  link_pred_loss_weight: 1.0 # 1.0
  entropy_loss_weight: 1.0 # 1.0
  grad_accumulation_schedule:
    0: 12

  # grad_accumulation_schedule: null
